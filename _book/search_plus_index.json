{"./":{"url":"./","title":"Introduction","keywords":"","body":"Hi there 👋 I'm Bota5ky Here is my Github: Bota5ky 本站为日常学习记录笔记使用：主要包括 Java 后端相关的技术栈 刚入门学习的一些技术栈可能会比较浅显、偏八股，后续会逐渐深入并整理 Leetcode 刷题 record： Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"recommended.html":{"url":"recommended.html","title":"推荐阅读","keywords":"","body":"博客网站 技术博客：适合想学硬核技术的同学，比如美团技术团队、阿里技术团队 科技资讯类：量子位、差评、新智元、无敌信息差 经验分享、编程趋势、技术干货：程序员鱼皮、小林 coding、java guide、程序喵、神光的编程笔记、小白 debug、古时的风筝、苏三、阿秀等 技术文章 OpenTelemetry 系列四｜如何使用 Java Agent 来实现无侵入的调用链 高性能队列——Disruptor Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"java/lock.html":{"url":"java/lock.html","title":"Java Lock","keywords":"","body":"1. synchronized 和 Lock 的区别 类别 synchronized ReentrantLock 存在层次 Java 中的一个关键字，JVM 层面 JDK 提供的一个类，API 层面 锁的释放 自动加锁与释放锁，线程执行发生异常时，会释放锁 需要手动加锁与释放锁 锁的获取 会阻塞其他线程 可以尝试获取锁 锁的类型 可重入、不可中断、非公平锁 可重入、可判断、公平锁或非公平锁 锁的状态 无法判断，锁的是对象，锁信息保存在对象头中 可以判断，int 类型的 state 标识来标识锁的状态 性能 少量同步 大量同步 用法区别： synchronized：在需要同步的对象中加入此控制，synchronized 可以加在方法上，也可以加在特定代码块中，括号中表示需要锁的对象。使用操作系统 Mutex Lock 实现，记录持有线程 ID 和 status（计数），抛异常会释放锁。 lock：需要显示指定起始位置和终止位置。一般使用 ReentrantLock 类做为锁，多个线程中必须要使用一个 ReentrantLock 类做为对象才能保证锁的生效。且在加锁和解锁处需要通过 lock() 和 unlock() 显示指出。所以要在 finally 块中写 unlock() 以防死锁。 性能区别： synchronized： 是托管给 JVM 执行的，在 Java 1.5 中是性能低效的。因为这是一个重量级操作，需要调用操作接口，导致有可能加锁消耗的系统时间比加锁以外的操作还多。但是到了 1.6，进行很多优化：适应自旋，锁消除，锁粗化，轻量级锁，偏向锁等等，使其性能并不比 Lock 差。 synchronized 原始采用的是 CPU 悲观锁机制，即线程获得的是排他锁。排他锁意味着其他线程只能依靠阻塞来等待线程释放锁。而在 CPU 转换线程阻塞时会引起线程上下文切换，当有很多线程竞争锁的时候，会引起 CPU 频繁的上下文切换导致效率很低。 lock：是 Java 写的控制锁的代码，用的是乐观锁机制。所谓乐观锁就是，每次不加锁而是假设没有冲突而去完成某项操作，如果因为冲突失败就重试，直到成功为止。乐观锁实现的机制就是 CAS 操作(Compare and Swap)。进一步研究 ReentrantLock 的源码，会发现其中比较重要的获得锁的一个方法是 compareAndSetState。这里其实就是调用的 CPU 提供的特殊指令。 用途区别： synchronized 和 ReentrantLock 在一般情况下没有什么区别，但是在非常复杂的同步应用中，请考虑使用 ReentrantLock ，特别是遇到下面几种需求的时候： 某个线程在等待一个锁的控制权的这段时间需要中断 分开处理一些 wait-notify，ReentrantLock 里面的 Condition 应用，能够控制 notify 哪个线程 公平锁功能，每个到来的线程都将排队等候 先说第一种情况，ReentrantLock 的 lock 机制有 2 种，忽略中断锁和响应中断锁，这带来了很大的灵活性。比如：如果 A、B 两个线程去竞争锁，A 线程得到了锁，B 线程等待，但是 A 线程这个时候实在有太多事情要处理，就是一直不返回，B 线程可能就会等不及了，想中断自己，不再等待这个锁了，转而处理其他事情。这个时候 ReentrantLock 就提供了两种机制：可中断/可不中断。 B 线程中断自己（或者别的线程中断它），但是 ReentrantLock 不去响应，继续让 B 线程等待，你再怎么中断，我全当耳边风（synchronized 原语就是如此） B 线程中断自己（或者别的线程中断它），ReentrantLock 处理了这个中断，并且不再等待这个锁的到来，完全放弃 2. synchronized 锁的升级过程 无锁（No lock）：初始状态下，共享资源没有被任何线程锁定，可以被任意线程访问。 偏向/匿名偏向锁（Biased lock）：当前锁资源只有一个线程来获取时，就是偏向锁状态。这个线程反复的获取这把锁，就不需要竞争。如果获取锁的是偏向的那个线程，拿锁走人。如果偏向的线程不是当前要获取锁的线程，如果偏向锁被持有，导致获取锁的线程拿锁失败，做锁升级的操作，需要做偏向锁撤销；如果偏向锁没有被持有，直接将偏向的线程更改为要获取锁的线程。 轻量级锁（Lightweight lock）：如果多个线程尝试获取偏向锁失败，JVM 会将锁升级为轻量级锁。在轻量级锁状态下，JVM 会将对象头中的一部分空间用于存储锁记录（Lock Record）的指针。每个线程在进入临界区之前，会通过自旋锁 CAS 的方式尝试几次将锁记录指针替换为自己的线程 ID。如果替换成功，表示获取到锁；否则，表示锁被其他线程占用，需要进行锁膨胀/粗化。 重量级锁（Heavyweight lock）：如果轻量级锁获取失败，JVM 会进行锁膨胀，将锁升级为重量级锁。在重量级锁状态下，JVM 会使用操作系统的互斥量来实现锁。这样，如果一个线程持有重量级锁，其他线程需要等待，直到持有锁的线程释放。 需要注意的是，锁的升级过程是自动逐步发生的。当线程竞争激烈或临界区执行时间较长时，锁可能会直接升级到重量级锁，跳过偏向锁和轻量级锁的阶段。只有偏向锁降级为无锁状态，其他的降级是不可行的。 锁消除：当加的锁不会造成任何线程安全问题时，编译器可能会通过静态分析源代码来确定是否可以将同步锁操作去掉。 HotSpot 实现： Mark Word在不同的锁状态下存储的内容不同，在32位JVM中是这么存的 在64位JVM中是这么存的 无锁态：内部正常存储对象的信息，hashCode，分代年龄等，锁标记位001 偏向锁：内部没地方存储 HashCode了，大部分空间都存储偏向哪个线程，存储了线程的标识，锁标记位101 轻量级锁：内部直接存储了 Lock Record 的地址，Lock Record中存储了对象的信息，锁标记位00 重量级锁：内部直接存储了 ObjectMonitor 的地址，ObjeMonitor 中存储了对象的信息，锁标记为10 查看 MarkWord 信息需要导入包org.openjdk.jol:jol-core:0.9，ClassLayout.parseInstance(obj).toPrintable() 匿名偏向锁：当前线程指针为空，再次打印当前 obj 的 MarkWord 信息，先休眠 5s 之后，再构建的对象，锁标记未是匿名偏向锁如果你是先构建的对象，他默认就是无锁状态，即便休眠了 5s 后，JVM 也不会监控对象动态修改 Markword，因为 JVM 默认有一个偏向锁延迟的设置，会延迟 4s 再次开启偏向锁机制。 为什么要开启偏向锁延迟的机制？首先，因为 JVM 的启动时，需要基于 ClassLoader 将大量的 Class 对象加载到堆内存。在这个加载的过程中，会涉及到 synchronized 的使用。 在这种锁竞争时，偏向锁状态基本不会停留。那偏向锁正常的升级到轻量级锁不就ok了么？因为偏向锁升级到轻量级锁涉及到了一个操作，偏向锁撤销。偏向锁撤销需要找到一个安全点才可以做撤销操作。安全点可以是，比如方法调用，循环的跳转，异常跳转之类的操作，这种操作一般都是安全的，不会对整体程序造成什么问题。 如果业务线程一致在处理 sleep 或者是这种休眠的状态，如果出现了业务线程池一直休眠，很长时间，这时候还会有一个概念叫做安全区域 ，线程执行到了某个位置，标识自己在安全区域，离开的时候，会修改标识，告知 JVM 我已经离开了安全区域。如果恰巧程序在安全区域内离开时，执行偏向锁撤销或者是 GC 时，这个刚刚离开安全区域的线程会等待。偏向锁撤销时，会在安全点或者是安全区域内执行，需要等到这个时间点才可以执行。 3. ReentrantLock lock.lockInterruptibly() // 允许被中断的拿锁方式 基于 java.util.concurrent 包中的抽象类 AbstractQueuedSynchronizer 实现，AQS 本质是一个并发包下的基础类，并没有什么具体的业务实现，但是很多 JUC 包下的工具类，都是基于 AQS 实现的，比如 ReentrantLock、ReentrantReadWriteLock、CountDownLatch、线程池、CyclicBarrier 等等。 AQS 核心内容： state属性：其实就是一个 int 属性，对应 synchronized 中的 _recursions，表示持有当前资源的线程数量 // The synchronization state. private volatile int state; 双向链表 // Head of the wait queue, lazily initialized.Except for ... private transient volatile Node head; // Tail of the wait queue, lazily initialized. Modified only via ... private transient volatile Node tail; 单向链表：存在于 AQS 的内部类 ConditionObject 中，单向链表是持有锁的线程，在执行了 await（相当于 synchronized 的 wait）方法，线程会释放掉锁资源，然后添加到这个单向链表中，然后挂起线程。在 wait 状态的线程被 signal（相当于synchronized 的 notify）方法唤醒后，会从单向链表扔到双向链表中等待获取锁资源 // First node of condition queue private transient Node firstWaiter; // Last node of condition queue private transient Node lastWaiter; 非公平锁流程： 场景：有 A、B 两个线程尝试获取锁资源 线程 A 先执行了加锁方法，执行执行 CAS，尝试将 ReentrantLock 中的 state 从0改为1。成功拿到锁资源，将exclusiveOwnerThread 属性设置为线程 A 线程 B 执行加锁方法，进行 CAS 尝试将 ReentrantLock 中的 state 从0改为1，失败 接着线程 B 或再次尝试拿锁，但是这次不会执行尝试修改 state，先判断 state 是否为0 如果为0，再次尝试将 ReentrantLock 中的 state 从0改为1 成功，拿锁走人 失败，拿锁失败，准备排队 不为0，查看一下，持有锁的线程是否是线程 B 如果是自己持有，走锁重入逻辑 如果不是自己持有，拿锁失败，准备排队 将线程 B 封装为 Node，添加到 AQS 的双向链表（head 节点不存在就进行初始化） 线程 B 进入到队列之后，会再次判断自己的位置位置是否是 head.next 如果是，可以再次尝试走抢锁逻辑（如果失败，走挂起线程的逻辑） 如果不是，走挂起线程的逻辑 因为线程要挂起，必须得有其他线程唤醒。必须让 prev 节点知道线程 B 要挂起了。如果 prev 节点知道了当前线程要挂起了，那就直接挂起线程 线程 A 释放锁资源之后，会让 head 节点看有没有需要唤醒的 Node 如果有，head 节点会唤醒 head.next 如果没有，啥事不做 公平锁流程： 场景：有 A、B 两个线程尝试获取锁资源 线程 A 优先执行 lock 方法拿锁，因为是公平锁，不会直接 CAS 抢 线程 A 会先判断 state 是否为0 如果是0，查看当前 AQS 的双向链表中是否有排队的节点 如果没有排队的，直接抢锁（CAS 将 state 从0改为1），线程 A 拿到锁 线程 B 再来执行拿锁的方法 线程 B 判断 state 是否为0 -> 不是，查看持有锁的线程是否是 B -> 不是 B，拿锁失败，准备排队 将线程 B 封装为 Node，添加到 AQS 的双向链表中 线程 B 进入到队列之后，会再次判断自己的位置，位置是否是 head.next 如果是，再次走抢锁逻辑 ...后续和上面非公平锁流程一样 公平锁和非公平锁的核心区别，其实就是两个方法的实现不同： lock 方法实现不同 非公平锁会在 lock 方法中直接执行 CAS，尝试将 state 从0改为1，如果失败才走 acquire 方法 公平锁在 lock 方法中，直接 acquire tryAcquire 方法实现不同 非公平锁，在 state 为0时，直接 CAS 抢 公平锁，在 state 为0时，查看是否有排的节点，才会根据情况尝试 CAS 抢锁 4. 公平锁和非公平锁 公平锁： 概念：公平锁是一种锁获取机制，它按照请求锁的顺序来分配锁，即先来先服务。在公平锁中，等待锁的线程会按照它们请求锁的顺序逐个获得锁，确保每个线程有公平的机会获得锁。 特点：公平锁可以防止饥饿（starvation），即某个线程永远无法获得锁的情况。它适用于需要确保所有线程都有机会获得锁的场景，即使需要等待一段时间，会导致线程频繁地从用户态和内核态之间切换，增加了上下文切换的开销，降低了系统的吞吐量。 实现：在 Java 中，ReentrantLock 类可以创建公平锁，通过传递 true 给构造函数来创建公平锁。 非公平锁： 概念：非公平锁是一种锁获取机制，它不按照请求锁的顺序来分配锁。在非公平锁中，新请求锁的线程有机会在等待队列中插队，即使有其他线程在等待锁，新线程仍有机会获得锁。 特点：非公平锁可能会导致某些线程饥饿，因为它不考虑等待时间，新线程有可能在老线程之前获得锁。 实现：在 Java 中，默认情况下，synchronized 关键字创建的锁是非公平的。同时，ReentrantLock 类也可以创建非公平锁，通过传递 false 给构造函数来创建非公平锁。 不管是公平锁和非公平锁，它们的底层实现都会使用 AQS （Abstract Queued Synchronizer 队列同步器）来进行排队，它们的区别在于线程在使用lock()方法加锁时，一旦没竞争到锁，都会进行排队，当锁释放时，都是唤醒排在最前面的线程，所以非公平锁只是体现在了线程加锁阶段，而没有体现在线程被唤醒阶段，ReentrantLock是可重入锁，不管是公平锁还是非公平锁都是可重入的。 比较公平锁和非公平锁的选择通常取决于应用的需求和性能要求： 如果需要确保锁的获取顺序与请求顺序一致，以防止某些线程长时间等待（避免饥饿），则可以选择公平锁。 如果对锁获取的顺序不敏感，且更关注性能，允许一些线程插队来提高并发性能，那么可以选择非公平锁。 5. 悲观锁和乐观锁 线程挂起不是 JVM 层面能解决的问题。需要操作系统来完成这个事情。那就需要涉及到用户态和内核态之间的切换，这种切换，会影响一定效率。 悲观锁：synchronized 关键字、ReentrantLock、ReentrantReadWriteLock 概念：悲观锁假设并发访问会导致冲突，因此在访问共享资源之前，它会假定其他线程会修改资源，因此会阻塞或限制其他线程的访问，以确保数据的一致性。 实现：悲观锁常常使用互斥锁或数据库事务（如 SQL 中的锁机制）来实现。当一个线程获得锁时，其他线程会被阻塞，直到获得锁的线程释放它。 适用场景：适用于写操作较多的情况，因为它阻塞其他线程的读写操作，以避免并发冲突。 乐观锁：CAS 概念：乐观锁假定并发访问不会导致冲突，因此它不会阻塞其他线程的访问。它允许多个线程同时访问共享资源，但在提交更新时，会检查是否有其他线程已经修改了资源，如果有冲突，则会回滚更新。 实现：乐观锁通常使用版本控制或时间戳来实现。每个数据项都包含一个版本号或时间戳，当一个线程读取数据后，它在写回时会检查版本号或时间戳是否发生了变化。 适用场景：适用于读操作较多、写操作较少的情况，因为它允许多个线程同时读取数据，只有在写操作时才会检查冲突。 比较悲观锁和乐观锁的选择通常取决于应用的需求和特点： 悲观锁适用于需要强制数据一致性和事务性的场景，例如银行系统中的转账操作，其中不允许并发的读写操作。 乐观锁适用于读操作频繁，且写操作较少的情况，例如社交媒体网站中的帖子浏览，多个用户可以同时读取数据，但只有在更新帖子时才会检查冲突。 在实际应用中，有时会将悲观锁和乐观锁结合使用，以根据不同情况综合考虑并发控制的策略。 6. 可重入锁和不可重入锁 可重入锁： 概念：可重入锁允许同一个线程多次获得同一把锁，而不会被自己所持有的锁所阻塞。这个特性通常称为锁的嵌套，获取了几次就需要释放几次。 特点：可重入锁使得线程可以在执行临界区代码时，再次进入其他依赖相同锁的临界区，而不会被自己所持有的锁所阻塞。这提高了编程的灵活性，允许设计更复杂的数据结构和算法。 实现：在 Java 中，ReentrantLock 类、synchronized 关键字就是可重入锁的实现。 不可重入锁： 概念：不可重入锁不允许同一个线程多次获得同一把锁。如果一个线程已经持有了这把锁，再次尝试获取它时会被阻塞。 特点：不可重入锁通常更简单，但在某些情况下可能会导致死锁，因为线程无法嵌套地访问需要这把锁的不同临界区。 实现：Java 中也有不可重入锁，不过并不是来实现原子性的，比如线程池中的 Worker 对象，它没有实现重入锁机制。 比较可重入锁和不可重入锁的选择通常取决于应用的需求和设计： 如果你的设计需要线程能够多次进入同一把锁的临界区，或者需要锁的嵌套来保护数据结构的一致性，那么可重入锁是一个更好的选择。 如果你的设计要求简单，并且不需要嵌套锁，或者为了避免潜在的死锁情况，那么不可重入锁可能更合适。 7. CAS (Compare And Swap) 内存位置，预期值，新值 原子操作，CPU 硬件指令集提供，硬件保证一个语义看起来需要多次操作才能完成的行为，通过一条处理器指令 CAS 就能完成 JDK 1.5 之后提供了 CAS 操作 Unsafe 类，里面有很多 native 的 compareAndSwapXXX 方法 还有 Atomic 类 存在的问题： ABA 问题：可以通过版本号解决，如 AtomicStampedReference 类。 一直自旋可能会浪费 CPU 资源 synchronized 的处理方案：synchronized 提供了轻量级锁的概念，这个概念下，会基于 CAS 尝试获取锁资源，但是他指定了 CAS 的次数，如果超过了次数没获取到锁，挂起线程。 LongAdder 的处理方案：如果使用 LongAdder 对一个元素做自增时，如果失败就会在其他的位置做自增。当需要元素的 sum 时，会将 LongAdder 多处存储值的内容做总和，然后返回，类似分段锁的方案。 只针对一个属性保证原子性：本身就是 CAS 的特性，不过，类似 synchronized 和 ReentrantLock 虽然都可以锁住一段代码，但是他们底层都用到了CAS。 public final int getAndAddInt(Object varl, long var2, int var4) { int var5; do { var5 = this.getIntVolatile(varl, var2); } while(!this.compareAndSwapInt(varl, var2, var5, var5 + var4) return var5; } 8. 传统的生产者消费者问题，防止虚假唤醒 线程也可以唤醒，而不会被通知，中断或超时，即所谓的虚促唤醒 。 虽然这在实践中很少会发生，但应用程序必须通过测试应该使线程被唤醒的条件来防范，并目如果条件不满足则继续等待。 换句话说，等待应该总是出现在循环中，就像这样： synchronized (obj) { while () obi.wait(timeout); ...// Perform action appropriate to condition } 如果当前线程interrupted任何线程之前或在等待时，那么InterruptedException被抛出。 如上所述，在该对象的锁定状态已恢复之前，不会抛出此异常。 9. Lock 版的生产者消费者问题 Condition 可以实现精准通知唤醒 class BoundedBuffer { final Lock lock = new ReentrantLock(); final Condition notFull = lock.newCondition(); final Condition notEmpty = lock.newCondition(); final Object[] items = new Object[100]; int putptr, takeptr, count; public void put(E x) throws InterruptedException{ lock.lock(); try { while (count == items.length) notFull.await(); items[putptr] = x; if (++putptr == items.length) putptr = 0; ++count; notEmpty.signal(); } finally { lock.unlock(); } } public E take() throws InterruptedException { lock.lock(); try { while (count == 0) notEmpty.await(); E x = (E) items[takeptr]; if (++takeptr == items.length) takeptr = 0; --count; notFull.signal(); return x; } finally { lock.unlock(); } } } 10. 八锁现象 JUC 学习之8锁现象 syncMethod 方法使用 synchronized 关键字修饰，锁定对象为当前实例（this），nonSyncMethod 方法没有使用同步关键字，不会进行锁定 static 修饰的方法属于 Class 模版，只有一个 11. 如何避免死锁？ 造成死锁的几个原因: 一个资源每次只能被一个线程使用 一个线程在阻塞等待某个资源时，不释放已占有资源 一个线程已经获得的资源，在未使用完之前，不能被强行剥夺 若干线程形成头尾相接的循环等待资源关系 这是造成死锁必须要达到的 4 个条件，如果要避免死锁，只需要不满足其中某一个条件即可。而其中前 3 个条件是作为锁要符合的条件，所以要避免死就需要打破第 4 个条件，不出现循环等待锁的关系。 在开发过程中: 要注意加锁顺序，保证每个线程按同样的顺序进行加锁 要注意加锁时限，可以针对所设置一个超时时间 要注意死锁检查，这是一种预防机制，确保在第一时间发现死锁并进行解决 12. volatile volatile关键字用于在多线程编程中确保共享变量的可见性和禁止指令重排序 线程的私有堆栈：Java 内存模型告诉我们，各个线程会将共享变量从主内存中拷贝到工作内容，然后执行引擎会基于工作内存中的数据进行操作处理。这个时机对普通变量是没有规定的，而针对volatile修饰的变量给 Java 虚拟机特殊的约定，线程对volatile变量的修改会立刻被其他线程所感知，即不会出现数据脏读的现象，从而保证数据的“可见性”。 只保证可见性，并不保证原子性 13. happens-before 规则 程序次序规则（Program Order Rule）：在一个线程内，按照程序代码顺序，书写在前面的操作先行发生于书写在后面的操作。准确地说，应该是控制流顺序而不是程序代码顺序，因为要考虑分支、循环等结构。 管程锁定规则（Monitor Lock Rule）：一个 unlock 操作先行发生于后面对同一个锁的 lock 操作。这里必须强调的是同一个锁，而 “后面” 是指时间上的先后顺序。 volatile 变量规则（Volatile Variable Rule）：对一个 volatile 变量的写操作先行发生于后面对这个变量的读操作，这里的 “后面” 同样是指时间上的先后顺序。 线程启动规则（Thread Start Rule）：Thread 对象的 start () 方法先行发生于此线程的每一个动作。 线程终止规则（Thread Termination Rule）：线程中的所有操作都先行发生于对此线程的终止检测，我们可以通过 Thread.join () 方法结束、Thread.isAlive () 的返回值等手段检测到线程已经终止执行。 线程中断规则（Thread Interruption Rule）：对线程 interrupt () 方法的调用先行发生于被中断线程的代码检测到中断事件的发生，可以通过 Thread.interrupted () 方法检测到是否有中断发生。 对象终结规则（Finalizer Rule）：一个对象的初始化完成（构造函数执行结束）先行发生于它的 finalize () 方法的开始。 传递性（Transitivity）：如果操作 A 先行发生于操作 B，操作 B 先行发生于操作 C，那就可以得出操作 A 先行发生于操作 C 的结论。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 10:54:03 "},"java/thread.html":{"url":"java/thread.html","title":"Java 多线程","keywords":"","body":"1. 多线程实现方式 继承 Thread 类，重写 run 方法 缺点：Java 是单继承的，MyThread 无法再继承其他类 class MyThread extends Thread { private int ticket = 5; @Override public void run() { for (int i = 0; i 0) { System.out.println(\"车票第\" + ticket-- + \"张\"); } } } } public void Test() { new MyThread().start(); new MyThread().start(); new MyThread().start(); } 实现 Runnable 接口，重写 run 方法，实现 Runnable 接口的实现类的实例对象作为 Thread 构造函数的 target 如果要实现多继承就得要用implements，Java 提供了接口java.lang.Runnable来解决上边的问题。 Runnable是可以共享数据的，多个 Thread 可以同时加载一个Runnable，当各自 Thread 获得 CPU时间片的时候开始运行Runnable，Runnable里面的资源是被共享的，所以使用Runnable更加的灵活。 class MyRunnable implements Runnable { private int ticket = 5; @Override public void run() { for (int i = 0; i 0) { System.out.println(\"车票第\" + ticket-- + \"张\"); } } } } public void Test() { Runnable myRunnable = new MyRunnable(); // 将myRunnable作为Thread target创建新的线程 new Thread(myRunnable).start(); new Thread(myRunnable).start(); } 在第二种方法（Runnable）中，ticket 输出的顺序并不是54321，这是因为线程执行的时机难以预测，ticket--并不是原子操作。 在第一种方法中，我们 new 了3个 Thread 对象，即三个线程分别执行三个对象中的代码，因此便是三个线程去独立地完成卖票的任务；而在第二种方法中，我们同样也 new 了3个 Thread 对象，但只有一个Runnable对象，3个 Thread 对象共享这个Runnable对象中的代码，因此，便会出现3个线程共同完成卖票任务的结果。如果我们 new 出3个Runnable对象，作为参数分别传入3个 Thread 对象中，那么3个线程便会独立执行各自Runnable对象中的代码，即3个线程各自卖5张票。 在第二种方法中，由于3个 Thread 对象共同执行一个Runnable对象中的代码，因此可能会造成线程的不安全，比如可能 ticket 会输出-1（如果我们System.out....语句前加上线程休眠操作，该情况将很有可能出现），这种情况的出现是由于，一个线程在判断 ticket 为1>0后，还没有来得及减1，另一个线程已经将 ticket 减1，变为了0，那么接下来之前的线程再将 ticket 减1，便得到了-1。这就需要加入同步操作（即互斥锁），确保同一时刻只有一个线程在执行每次for循环中的操作。而在第一种方法中，并不需要加入同步操作，因为每个线程执行自己 Thread 对象中的代码，不存在多个线程共同执行同一个方法的情况。 通过 Callable 和 FutureTask 创建线程 Runnable是执行工作的独立任务，但是它不返回任何值。如果你希望任务在完成的能返回一个值，那么可以实现Callable接口而不是Runnable接口。在 Java SE5 中引入的Callable是一种具有类型参数的泛型，它的参数类型表示的是从方法call()(不是run())中返回的值。 FutureTask具有仅执行1次run()方法的特性(即使有多次调用也只执行1次)，避免了重复查询的可能。 class MyThread implements Callable { public Integer call() throws Exception { System.out.println(\"当前线程名——\" + Thread.currentThread().getName()); int i = 0; for (; i futureTask = new FutureTask<>(myThread); new Thread(futureTask, \"线程名：有返回值的线程2\").start(); try { System.out.println(\"子线程的返回值：\" + futureTask.get()); } catch (Exception e) { e.printStackTrace(); } } 通过线程池创建线程 ExecutorService executorService = Executors.newFixedThreadPool(5); MyRunnable myRunable = new MyRunnable(); executorService.execute(myRunable); executorService.shutdown(); ExecutorService、Callable、Future三个接口实际上都是属于Executor框架。返回结果的线程是在 JDK 1.5 中引入的新特征，有了这种特征就不需要再为了得到返回值而大费周折了 有返回值的任务必须实现Callable接口。类似的，无返回值的任务必须实现Runnable接口 执行Callable任务后，可以获取一个Future的对象，在该对象上调用get就可以获取到Callable任务返回的Object了 注意：get方法是阻塞的，即：线程无返回结果，get方法会一直等待 再结合线程池接口ExecutorService就可以实现传说中有返回结果的多线程了 下面提供了一个完整的有返回结果的多线程测试例子，在 JDK 1.5 下验证过没问题可以直接使用 2. 实现 Runnable 接口相比继承 Thread 类的优势 可以避免由于 Java 的单继承特性而带来的局限 增强程序的健壮性，代码能够被多个线程共享，代码与数据是独立的 适合多个相同程序代码的线程区处理同一资源的情况 3. 实现 Runnable 接口和实现 Callable 接口的区别 Runnable是自从 Java 1.1 就有了，而Callable是 1.5 之后才加上去的 Callable规定的方法是call()，Runnable规定的方法是run() Callable的任务执行后可返回值，而Runnable的任务是不能返回值（是 void） call方法可以抛出异常，run方法不可以 运行Callable任务可以拿到一个Future对象，表示异步计算的结果。它提供了检查计算是否完成的方法，以等待计算的完成，并检索计算的结果。通过Future对象可以了解任务执行情况，可取消任务的执行，还可获取执行结果 加入线程池运行，Runnable使用ExecutorService的execute方法，Callable使用submit方法 4. 线程的六种状态 NEW：初始化状态，未调用start方法 RUNNABLE：运行态 Ready：等待CPU发时间片 Running：执行态 BLOCKED：阻塞态，锁 WAITING：等待态，拿到锁之后，发现当前执行条件不太满足，需要暂时停止执行，以便让出CPU资源供其他线程执行，wait()，继续执行需要notify() TIMED_WAITING：超时等待 TERMINATED：结束态 5. sleep 和 wait 的区别 sleep 暂停当前线程执行，但不释放锁 Thread.sleep() 可以在任何场景使用 只能等待 sleep 结束 wait 暂停当前线程执行，并释放锁 属于对象 不能写在synchronized同步块之外 notify()唤醒 6. 停止线程 但是stop()方法是不建议使用，并且是有可能在未来版本中删除掉的： @Deprecated(since=\"1.2\", forRemoval=true) public final void stop() {} 因为stop()方法太粗暴了，一旦调用了stop()，就会直接停掉线程，这样就可能造成严重的问题，比如任务执行到哪一步了？该释放的锁释放了没有？都存在疑问。 这里强调一点，stop()会释放线程占用的synchronized锁，而不会自动释放ReentrantLock锁 建议通过中断来停止线程t1.interrupt()，执行结果根据线程内部的逻辑决定 if (Thread.currentThread().isInterrupted() && otherOptions) { //中断执行的操作break } try { Thread.sleep(1000); } catch (InterruptedException e) { //如果没有对中断做处理，中断信号会被重置为false，在循环内判断isInterrupted就会失效 } 线程池也是通过 interrupt 方法来停止线程的，比如 shutdownNow() 方法中会调用： void interruptIfStarted() { Thread t; if (getState() >= 0 && (t = thread) != null && !t.isInterrupted()) { try { t.interrupt(); } catch (SecurityException ignore) { } } } 7. 有序性 Java JIT（Just In Time）和 CPU 都有可能对指令做重排序。 对于懒汉式的单例模式，在多线程的情况下可能会存在问题，因为 B 线程初始化单例实际分为 3 个指令：申请空间、初始化、赋值，后面 2 个指令有可能会被调换顺序。当 B 线程先执行了赋值，还未进行初始化，此时暂停 B 线程转而继续执行 A 线程，就会导致 A 线程直接返回未初始化完成的 instance。解决方法就是给 instance 变量加上关键字 volatile。 public static Singleton getInstance() { //A // 先判断实例是否存在，若不存在再对类对象进行加锁处理 if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); //B } } } return instance; } 8. ThreadLocal ThreadLocal是 Java 中所提供的线程本地存储机制，可以利用该机制将数据缓存在某个线程内部，该线程可以在任意时刻、任意方法中获取缓存的数据。 ThreadLocal底层是通过ThreadLocalMap来实现的，每个Thread对象（注意不是ThreadLocal对象）中都存在一个ThreadLocalMap，Map 的 key 为ThreadLocal对象，Map 的 value 为需要缓存的值。 // Thread.java ThreadLocal.ThreadLocalMap threadLocals = null; 如果在线程池中使用ThreadLocal会造成内存泄漏，因为当ThreadLocal对象使用完之后，应该要把设置的 key、value，也就是Entry对象进行回收，但线程池中的线程不会回收，而线程对象是通过强引用指向ThreadLocalMap，ThreadLocalMap也是通过强引用指向Entry对象线程不被回收，Entry对象也就不会被回收，从而出现内存泄漏，解决办法是，在使用了ThreadLocal对象之后，手动调用ThreadLocal的remove方法，手动清除Entry对象。 threadLocal.remove(); ThreadLocal经典的应用场景就是连接管理（一个线程持有一个连接，该连接对象可以在不同的方法之间进行传递，线程之间不共享同一个连接）。 9. fail-fast 和 fail-safe fail-fast是 Java 中的⼀种快速失败机制， java.util 包下所有的集合都是快速失败的，快速失败会抛出 ConcurrentModificationException异常， fail-fast 你可以把它理解为⼀种快速检测机制，它只能⽤来检测错误，不会对错误进⾏恢复， fail-fast 不⼀定只在多线程环境下存在， ArrayList 也会抛出这个异常，主要原因是由于 modCount 不等于 expectedModCount。 fail-safe是 Java 中的⼀种安全失败机制，它表示的是在遍历时不是直接在原集合上进⾏访问，而是先复制原有集合内容，在拷⻉的集合上进⾏遍历。 由于迭代时是对原集合的拷⻉进⾏遍历，所以在遍历过程中对原集合所作的修改并不能被迭代器检测到，所以不会触发ConcurrentModificationException。java.util.concurrent包下的容器都是安全失败的，可以在多线程条件下使⽤，并发修改。 10. CTL (control) 在 Java 中，线程池（Thread Pool）是一种用于管理和复用线程的机制。在 Java 的线程池实现中，ctl是一个表示线程池状态和线程数量的变量。 具体来说，ctl是一个 32 位的整数，其中高 3 位表示线程池的状态，低 29 位表示线程池中的线程数量。这样的设计可以同时表示线程池的状态和线程数量，提供了一种紧凑的表示方式。 通过对ctl的操作，可以实现对线程池状态和线程数量的管理，包括增加或减少线程数量、判断线程池是否在运行等功能。这样的设计可以更好地控制线程池的行为和资源利用，提高多线程编程的效率和可靠性。 11. 线程池的创建 //三大方法 Executors.newSingleThreadExecutor(); // 单个线程 Executors.newFixedThreadPool(5); // 固定的线程池大小 Executors.newCachedThreadPool(); // 可伸缩的 //七大参数 public ThreadPoolExecutor( int corePoolSize, int maximumPoolSize, long keepAliveTime, TimeUnit unit, // TimeUnit.SECONDS BlockingQueue workQueue, // new LinkedBlockingDeque<>(3) ThreadFactory threadFactory, // Executors.defaultThreadFactory() RejectedExecutionHandler handler // new ThreadPoolExecutor.AbortPolicy() ) 12. 线程池的状态 状态 行为 RUNNING 会接收新任务并且会处理队列中的任务 SHUTDOWN 不会接收新任务并且会处理队列中的任务，任务处理完后会中断所有线程 STOP 不会接收新任务并且不会处理队列中的任务，并且会直接中断 TIDYING 所有线程所有线程都停止了之后，线程池的状态就会转为TIDYING，一旦达到此状态，就会调用线程池的terminated() TERMINATED terminated()执行完之后就会转变为TERMINATED SHUTDOWN对应 executor.shutdown() 方法，STOP对应 executor.shutdownNow() 方法 线程池状态的转换情况 转变前 转变后 转变条件 RUNNING SHUTDOWN 手动调用shutdown()触发，或者线程池对象GC时会调用finalize()从而调用shutdown() RUNNING STOP 手动调用shutdownNow()触发 SHUTDOWN STOP 手动先调用shutdown()紧着调用shutdownNow()触发 SHUTDOWN TIDYING 线程池所有线程都停止后自动触发 STOP TIDYING 线程池所有线程都停止后自动触发 TIDYING TERMINATED 线程池自动调用terminated()后触发 13. 线程池中提交一个任务的流程 14. 四种拒绝策略 AbortPolicy 如果线程池拒绝了任务，直接报错 CallerRunsPolicy 线程池让调用者去执行 public static class CallerRunsPolicy implements RejectedExecutionHandler { public CallerRunsPolicy() { } public void rejectedExecution(Runnable r, ThreadPoolExecutor e) { if (!e.isShutdown()) { r.run(); // 可以看见源码逻辑为，先判断线程池还未关闭，然后直接r.run运行了任务。 } } } DiscardPolicy 如果线程池拒绝了任务，直接丢弃 DiscardOldestPolicy 如果线程池拒绝了任务，直接将线程池中最旧的，未运行的任务丢弃，将新任务入队 15. 为什么不建议使用 Executors 来创建线程池？ 当我们使用 Executors 创建 FixedThreadPool 时，对应的构造方法为： public static ExecutorService newFixedThreadPool(int nThreads) { return new ThreadPoolExecutor(nThreads, nThreads, 0L, TimeUnit.MILLISECONDS, new LinkedBlockingQueue()); } 发现创建的队列为 LinkedBlockingQueue，是一个无界阻塞队列，如果使用该线程池执行任务如果任务过多就会不断的添加到队列中，任务越多占用的内存就越多，最终可能耗尽内存，导致OOM。 阿里开发手册：线程池不允许使用 Executors 去创建， 而是通过 ThreadPoolExecutor 的方式， 这样的处理方式让写的同学更加明确线程池的运行规则， 规避资源耗尽的风险。 Executors 返回的线程池对象的弊端如下： FixedThreadPool 和 SingleThreadPool：允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 CachedThreadPool：允许的创建线程数量为 Integer.MAX_VALUE，可能会创建大量的线程，从而导致 OOM。 ScheduledThreadPool：允许的请求队列长度为 Integer.MAX_VALUE，可能会堆积大量的请求，从而导致 OOM。 除开有可能造成 OOM 之外，我们使用 Executors 来创建线程池也不能自定义线程的名字，不利于排查问题，所以建议直接使用 ThreadPoolExecutor来定义线程池，这样可以灵活控制。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 15:55:19 "},"java/jvm.html":{"url":"java/jvm.html","title":"JVM","keywords":"","body":"1. CMS (Concurrent-Mark-Sweep) 在 JDK 1.5 时期，HotSpot 推出了一款在强交互应用中几乎可认为有划时代意义的垃圾收集器：CMS 收集器，这款收集器是 HotSpot 虚拟机中第一款真正意义上的并发收集器，它第一次实现了让垃圾收集线程与用户线程同时工作。 CMS 收集器的关注点是尽可能缩短垃圾收集时用户线程的停顿时间。停顿时间越短（低延迟）就越适合与用户交互的程序，良好的响应速度能提开用户体验。 目前很大一部分的 Java 应用集中在互联网站或者 B/S 系统的服务端上，这类应用尤其重视服务的响应速度，希望系统停顿时间最短，以给用户带来较好的体验。CMS 收集器就非常符合这类应用的需求。 CMS 的垃圾收集算法采用标记-清除算法，并且也会“stop-the-world” 不幸的是，CMS 作为老年代的收集器，却无法与 JDK 1.4. 中已经存在的新生代收集器 Parallel Scavenge 配合工作，所以在 JDK 1.5 中使用 CMS 来收集老年代的时候，新生代只能选择 ParNew 或者 Serial 收集器中的一个。 在 G1 出现之前，CMS 使用还是非常广泛的。一直到今天，仍然有很多系统使用 CMS GC。 CMS 垃圾清理过程：当堆内存使用率达到某一阈值时，便开始进行回收。 初始标记（STW）：暂时时间非常短，标记与 GC Roots 直接关联的对象。 并发标记（最耗时）：从 GC Roots 开始历整个对象图的过程，不会停顿用户线程。 重新标记（STW）：修复并发标记环节，因为用户线程的执行，导致数据的不一致性问题 并发清理(最耗时) 有人会觉得既然 Mark Sweep 会造成内存碎片，那么为什么不把算法换成 Mark Compact 呢？ 因为当并发清除的时候，用 Compact 整理内存的话，原来的用户线程使用的内存还怎么用呢？要保证用户线程能继续执行，前提的它运行的资源不受影响嘛。Mark-Compact 更适合“stop the World”这种场景下使用。 参数： -XX:+UseConcMarkSweepGC手动指定使用 CMS 收集器执行内存回收任务 开启该参数后会自动将-XX:+UseParNewGC打开。即：ParNew（Young区用）+CMS（old区用）+Serial old 的组合。 -XX:CMSInitiatingOccupanyFraction设置堆内存使用率的阙值，一旦达到该阙值，便开始进行回收。 JDK5 及以前版本的默认值为 68,即当老年代的空间使用率达到 68% 时，会执行一次 CMS 回收。JDK6 及以上版本默认值为 92% 如果内存增长缓慢，则可以设置一个稍大的值，大的阙值可以有效降低CMS的触发频率，减少老年代回收的次数可以较为明显地改善应用程序性能。反之，如果应用程序内存使用率增长很快，则应该降低这个阙值，以避免频繁触发老年代串行收集器。因此通过该选项便可以有效降低Full GC 的执行次数。 -XX:+UseCMSCompactAtFullCollection用于指定在执行完 Full GC 后对内存空间进行压缩整理，以此避免内存碎片的产生。不过由于内存压缩整理过程无法并发执行，所带来的问题就是停顿时间变得更长了。 -XX:CMSFulGCsBeforeCompaction设置在执行多少次 Full GC 后对内存空间进行压缩整理。 2. G1 (Garbage First) G1 将内存划分为一个个的 region。内存的回收是以 region 作为基本单位的。Region 之间是复制算法，但整体上实际可看作是标记-压缩（Mark-Compact）算法，两种算法都可以避免内存碎片。这种特性有利于程序长时间运行，分配大对象时不会因为无法找到连续内存空间而提前触发下一次 GC。尤其是当 Java 堆非常大的时候，G1 的优势更加明显。 参数： -XX: +UseG1GC手动指定使用G1收集器执行内存回收任务。 -XX:G1HeapRegionSize设置每个 Region 的大小。值是 2 的幂，范围是 1MB 到 32MB 之间，目标是根据最小的 Java 堆大小划分出约 2048 个区域。默认是堆内存的 1/2000。 -XX:MaxGCPauseMillis 设置期望达到的最大 GC 停顿时间指标（JVM会尽力实现，但不保证达到）。默认值是200ms。（主要优化参数） -XX:ParallelGCThread设置 STW 时 GC 线程数的值。最多设置为 8。 -XX:ConcGCThreads设置并发标记的线程数。将n设置为并行垃圾回收线程数（ParallelGCThreads）的1/4左右。 -XX:InitiatingHeapOccupancyPercent设置触发并发 GC 周期的 Java 堆占用率阙值。超过此值，就触发GC。默认值是45。 G1 GC 的垃圾回收过程主要包括如下三个环节： 年轻代 GC（Young GC） 老年代并发标记过程（Concurrent Marking） 混合回收 (Mixed GC)：整个 Young Region 加一部分 Old Region （如果需要，单线程、独占式、高强度的 Full GC 还是继续存在的。它针对 GC 的评估失败提供了一种失败保护机制，即强力回收。） 3. GC 评估指标 吞吐量：程序的运行时间/(程序的运行时间+内存回收的时间) 垃圾收集开销：吞吐量的补数，垃圾收集器所占时间与总时间的比例。 暂停时间： 执行垃圾收集时，程序的工作线程被暂停的时间。 收集频率：相对于应用程序的执行，收集操作发生的频率。 内存占用：Java 堆区所占的内存大小。 快速：一个对象从诞生到被回收所经历的时间。 吞吐量优先：单位时间内，STW 的时间最短 响应时间优先：尽可能让单次 STW 的时间最短 现在 JVM 调优标准：在最大吞吐量优先的情况下，降低停顿时间。 4. 各 GC 使用场景 垃圾收集器 分类 作用位置 使用算法 特点 适用场景 Serial 串行 新生代 复制算法 响应速度优先 适用于单CPU环境下的client模式 ParNew 并行 新生代 复制算法 响应速度优先 多CPU环境Server模式下与CMS配合使用 Parallel 并行 新生代 复制算法 吞吐量优先 适用于后台运算而不需要太多交互的场景 Serial Old 串行 老年代 标记压缩 响应速度优先 适用于单CPU环境下的client模式 Parallel Old 并行 老年代 标记压缩 吞吐量优先 适用于后台运算而不需要太多交互的场景 CMS 并发 老年代 标记清除 响应速度优先 适用于互联网或B/S业务 G1 并发并行 新生代老年代 标记压缩复制 响应速度优先 面向服务端应用 5. JVM 常用参数 options 参数含义 -verbose:gc 输出GC日志信息，默认输出到标准输出 -XX:+PrintGC 输出GC日志。类似：-verbose:gc -XX:+PrintGCDetails 在发生垃圾回收时打印内存回收详细的日志并在进程退出时输出当前内存各区域分配情况 -XX:+PrintGCTimeStamps 输出GC发生时的时间截 -XX:+PrintGCDateStamps 输出GC发生时的时间戳（以日期的形式，如2013-05-04T21:53:59.234+0800) -XX:+PrintHeapAtGC 每一次GC前和GC后，都打印堆信息 -Xloggc:-XX:+UseGCLogFileRotation-XX:NumberOfGCLogFiles=5 表示把GC日志写入到一个文件中去，而不是打印到标准输出中 -Xss512K 设置每个线程的栈大小 -XX:MetaspaceSize=64m-XX:MaxMetaspaceSize=60m 设置元空间大小 -XX:+HeapDumpOnOutOfMemoryError-XX:heapDumpPath=heap/heapdump.hprof 生成dump文件 -XX:-DoEscapeAnalysis 不使用逃逸分析，jdk6u23之后默认是开启的 -XX:+PrintEscapeAnalysis 查看逃逸分析的筛选结果 -XX:-EliminateAllocations 关闭标量替换，默认开启，需要开启逃逸分析才能关闭 通常会将-Xms和-Xmx两个参数配置相同的值，其目的是为了能够在 java 垃圾回收机制清理完堆区后不需要重新分隔计算堆区的大小，从而提高性能。 手动生成 dump 文件： jmap -dump:format=b,file= jmap -dump:live,format=b,file= 6. OOM 案例 栈空间溢出 StackOverFlow，一般也不称为 OOM。 堆溢出：不断创建对象 元空间溢出：不断创建代理类 while (true) { Enhancer enhancer = new Enhancer(); enhancer.setSuperclass(SecurityProperties.User.class); enhancer.setUseCache(false); enhancer.setCallback(new MethodInterceptor() { @Override public Object intercept(Object obj, Method method, Object[] args, MethodProxy proxy) throws Throwable { return proxy.invoke(obj, args); } }); Object o = enhancer.create(); } GC overhead limit exceeded：垃圾回收效率不足，提前报异常 线程溢出：创建了大量线程 7. JVM 调优 JVM 监控及诊断工具 命令行： jps -l jstat -gc jinfo -flag jmap jstack > GUI：Visual VM、eclipse MAT、Arthas 为什么要调优？ 防止出现 OOM，进行 JVM 规划和预调优 解决程序运行中各种 OOM 减少 Full GC 出现的频率，解决运行慢、卡顿问题 调优监控的依据：运行日志、异常堆栈、GC 日志、线程快照、堆转储快照 性能优化的步骤： 第1步：熟悉业务场景 第2步（发现问题）：性能监控：GC 频繁、cpu load 过高、OOM、内存泄漏、死锁、程序响应时间较长 第3步（排查问题）：性能分析： 打印GC日志，通过GCviewer或者http://gceasy.io来分析日志信息 灵活运用命令行工具，jstack，jmap，jinfo等 dump 出堆文件，使用内存分析工具分析文件 使用阿里 Arthas，或 jconsole，JVisualVM 来实时查看 JVM jstack 查看堆栈信息 第4步（解决问题）：性能调优 适当增加内存，根据业务背景选择垃圾回收器 优化代码，控制内存使用 增加机器，分散节点压力 合理设置线程池线程数量 使用中间件提高程序效率，比如缓存，消息队列等 其他...... JIT 在解释运行的时候才会进行优化，所以编译生成的字节码文件不会做同步消除（消除无效锁）。 public void function(){ object o = new Object(); synchronized(o){...} } 标量替换：在 JIT 阶段，如果经过逃逸分析，发现一个对象（那些还可以分解的数据叫做聚合量 Aggregate）不会被外界访问的话，那么经过 JIT 优化，就会把这个对象拆解成若干个其中包含的若干个成员变量来代替。 官方推荐设置： Java 整个堆大小设置，Xmx 和 Xms 设置为老年代存活对象的 3-4 倍，即 FullGC 之后的老年代内存占用的 3-4 倍。 方法区（永久代 PermSize 和 MaxPermSize 或 元空间 MetaspaceSize 和 MaxMetaspaceSize）设置为老年代存活对象的 1.2-1.5 倍。 年轻代Xmn的设置为老年代存活对象的 1-1.5 倍 CPU 占用很高排查方案： ps aux l grep java 查看到当前 java 进程使用 cpu、内存、磁盘的情况获取使用量异常的进程 top -Hp 检查当前使用异常线程的 pid 把线程 pid 变为 16 进制如 31695 -> 0x7c8f，jstack | grep -A20 0x7c8f 得到相关进程的代码 8. 垃圾回收 可作为 gc roots 的对象： 虚拟机栈（栈帧中的本地变量表）中引用的对象 本地方法栈中 JNI（即一般说的 Native 方法）引用的对象 方法区中类静态属性引用的对象 方法区中常量引用的对象 9. HotSpot 垃圾分代回收算法 默认情况下新生代占 1/3，老年代占 2/3 绝大多数对象在新生代中被创建，这里的垃圾回收非常频繁且速度很快 新生代通常采用复制算法，由于存活对象少，复制成本很低 新生代分为 Eden、Surivivor from、Surivivor to 区，占比 8:1:1 Eden 填满后触发一次新生代的垃圾回收，称为 minor gc，存活对象复制到任一 Surivivor 区，然后将 Eden 区清空即可完成这次 gc，Surivivor from 区的存活对象会复制到另一个 Surivivor to 区，这里需要保证 to 区为空 存活超过复制次数阈值（默认 15）会被复制到老年代 Surivivor 空间不够容纳存活对象时，也会直接进入老年代 大数组或者特别大的字符串 老年代通常使用标记整理算法进行回收，将存活对象向一端进行移动，称为 major gc 10. G1/CMS 并发标记原理 三色标记： 白色：没有被访问过 -> 垃圾对象 黑色：包括其引用都被访问过 灰色：被访问过，但还存在一些引用没有被访问 对象消失问题：扫描过程中插入了一条或多条从黑色对象到白色对象的新引用，并且同时去掉了灰色对象到该白色对象的直接引用或者间接引用。 解决方法，破坏上述两个条件之一即可： 增量更新：记录引用关系，并发扫描结束后根据记录重新扫描一次 -> CMS 原始快照（SATB）：记录 -> G1 JVM_垃圾收集之三色标记算法详解 CMS 缺点：占用 CPU 资源，不超过 25%；浮动垃圾；内存碎片 11. 其他垃圾回收算法 Serial：JDK 1.3 版本之前唯一的串行垃圾回收器，Stop The World ParNew：多线程垃圾回收，只负责新生代的垃圾回收，可以配合 Serial Old 和 Concurrent Mark Swap 处理老年代 Parallel Scavenge：也是新生代的收集器，可控制吞吐量，gc 自适应，配合 Parallel old 处理老年代 12. 类加载机制 类的生命周期：（class 文件 -> Java虚拟机内存 -> 卸载） 加载 -> 验证 -> 准备 -> 解析 -> 初始化 -> 使用 -> 卸载 类的加载过程： 加载：查找并加载类的二进制数据（Class文件） 方法区：类的类信息 堆：Class 文件对应的类实例 验证：确保加载的类信息是正确的 准备：为类的静态变量进行初始化，分配空间并赋予初始值。例如：public static int a = 1; 在准备阶段对静态变量 a 赋默认值 0 解析：是将符号应用转换为直接引用 初始化：JVM 对类进行初始化，对静态变量赋予正确值。例如：public static int a = 1;这个时候才对静态变量 a 赋初始值 1 静态代码块 13. String String 代表的是 Java 中的字符串 ， String 类比较特殊，它整个类都是被 final 修饰的，也就是说，String 不能被任何类继承，任何修改 String 字符串的⽅法都是创建了⼀个新的字符串（保证了线程安全性）。 不可变对象不是真的不可变，可以通过反射来对其内部的属性和值进⾏修改，不过⼀般我们不这样做。 方法String.intern()：在 jdk1.7 及以后调⽤intern()⽅法是判断运⾏时常量池中是否有指定的字符串，如果没有的话，就把字符串添加到常量池（jdk1.8 之后，字符串常量池在堆中）中，并返回常量池中的对象。 String a = new String(\"ab\"); String b = new String(\"ab\"); String c = \"ab\"; String d = \"a\"; String e = new String(\"b\"); String f = d + e; // + 号相当于是执行 new StringBuilder.append(), 但每次都会new StringBuilder()，所以多次拼接建议自建 StringBuilder String g = \"a\" + \"b\"; // 编译器会优化，会直接被优化为bbbccc，也就是直接创建了一个 bbbccc 对象 System.out.println(a.intern() == b); //false System.out.println(a.intern() == b.intern()); //true System.out.println(a.intern() == c); //true System.out.println(a.intern() == f); //false //equals()方法作对比都是true 14. Java 对象的内存布局 对象头（Object Header）：包括了关于堆对象的布局、类型、GC状态、同步状态和标识哈希码的基本信息。Java对象和vm内部对象都有一个共同的对象头格式。 Markword：用于存储对象自身的运行时数据，如哈希码（HashCode）、GC分代年龄、锁状态标志、线程持有的锁、偏向线程ID、偏向时间戳等等。 类型指针：是对象指向它的类元数据的指针，虚拟机通过这个指针来确定这个对象是哪个类的实例。 实例数据（Instance Data）：主要是存放类的数据信息，父类的信息，对象字段属性信息。 对齐填充（Padding）：为了字节对齐，填充的数据，不是必须的。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 11:06:53 "},"framework/spring.html":{"url":"framework/spring.html","title":"Spring","keywords":"","body":"1. ApplicationContext 和 BeanFactory 的区别 BeanFactory 是 Spring 中非常核心的组件，表示 Bean 工厂可以生成 Bean，维护 Bean，而 ApplicationContext 继承了 BeanFactory，所以 ApplicationContext 拥有 BeanFactory 所有的特点，也是一个 Bean工厂，但是 ApplicationContext 除开继承了 BeanFactoy 之外，还继承了诸如 EnvironmentCapable（获取环境变量、properties等）、MesageSoure（国际化）、ApplicationEventPublisher 等接口，从而 ApplicationContext 还有获取系统环境变量、国际化、事件发布等功能，这是 BeanFactory 所不具备的。 2. Spring Boot 是如何启动 Tomcat 的？ 首先，SpringBoot 在启动时会先创建一个 Spring 容器 在创建 Spring 容器过程中，会利用@Conditional0nClass技术来判断当前 classpath 中是否存在 Tomct 依赖，如果存在则会生成一个启动 Tomcat 的 Bean Spring 容器创建完之后，就会获取启动 Tomcat 的 Bean，并创建 Tomcat 对象，并绑定端口等，然后启动 Tomcat 3. @SpringBootApplication 注解 @SpringBootApplication 注解：这个注解标识了一个 SpringBoot 工程，它实际上是另外三个注解的组合，这三个注解是 @SpringBootConfiguration：这个注解实际就是一个 @Configuration，表示启动类也是一个配置类 @EnableAutoConiquration：向Spring容器中导入了一个 Seletor，用来加载 ClassPath 下Springfactories 中所定的自动配置类，将这些白动加载为配置Bean @ComponentScan：标识扫描路径，因为默认是没有配置实际扫描路径，所以SpringBoot扫描的路径是启动类所在的当前目录 4. Spring 容器启动流程 在创建 Spring 容器，也就是启动 Spring 时： 首先会进行扫描，扫描得到所有的 BeanDefinition 对象，并存在一个 Map 中 然后筛选出非懒载的单例 BeanDefinition 进行创建 Bean，对于多例 Bean 不需要在启动过程中去进行创建，对于多例 Bean 会在每次获取 Bean 时利用 BeanDefinition 去创建 利用 BeanDefinition 创建 Bean 就是 Bean 的创建生命周期，这期间包括了合并 BeanDefinition、推断构造方法、实例化、属性填充、初始化前、初始化、初始化后等步骤，其中 AOP 就是发生在初始化后这一步骤中 单例 Bean 创建完了之后，Spring 会发布一个容器启动事件 Spring 启动结束 在源码中会更复杂，比次如源码中会提供一些模板方法，让子类来实现，比如源码中还涉及到一些 BeanfactoryPostProcesser 和 BeanPostProcessor 的注册，Spring 的扫描就是通过 BeanFactoryPostProcessor 来实现的，依赖注入就是通过 BeanPostProcessor 来实现的 在 Spring 启动过程中还会去处理 @Import 等注解 5. Spring 事务传播机制 多个事务方法相互调用时，事务如何在这些方法间传播，方法 A 是一个事务的方法，方法 A 执行过程中调用了方法 B，那么方法 B 有无事务以及方法 B 对事务的要求不同都会对方法 A 的事务具体执行造成影响，同时方法 A 的事务对方法 B 的事务执行也有影响，这种影响具体是什么就由两个方法所定义的事务传播类型所决定。 REQUIRED（Spring 默认的事务传播类）：如果当前没有事务，则自己新建一个事务，如果当前存在事务，则加入这个事务 SUPPORTS：当前存在事务，则加入当前事务，如果当前没有事务，就以非事务方法执行 MANDATORY：当前存在事务，则加入当前事务，如果当前事务不存在，则抛出异常 REQUIRES_NEW：创建一个新事务，如果存在当前事务，则挂起该事务 NOT_SUPPORTED：以非事务方式执行如果当前存在事务，则挂起当前事务 NEVER：不使用事务，如果当前事务存在，则抛出异常 NESTED：如果当前事务存在，则在嵌套事务中执行，否则 REQUIRED 的操作一样（开启一个事务） 6. Spring 事务什么时候会失效？ Spring@Transactional 注解的 12 种失效场景 7. Spring 中的设计模式 工厂模式：BeanFactory、FactoryBean、ProxyFactory 原型模式：原型 Bean、PrototypeTargetSource、PrototypeAspectInstanceFactory 单例模式：单例 Bean、SingletonTargetSource、DefaultBeanNameGenerator、SimpleAutowireCandidateResolver、AnnotationAwareOrderComparator 构造器模式：BeanDefinitionBuilder——BeanDefinition 构造器、BeanFactoryAspectJAdvisorsBuilder - 解析并构造 @AspectJ 注解的 Bean 中所定义的 Advisor、StringBuilder 适配器模式：ApplicationListenerMethodAdapter——将@EventListener 注解的方法适配成 ApplicationListener、AdvisorAdapter 把 Advisor 适配成 MethodInterceptor 访问者模式：PropertyAccessor——属性访问器，用来访问和设置某个对象的某个属性、MessageSourceAccessor——国际化资源访问器 装饰器模式：BeanWrapper——比单纯的Bean对象功能更加强大、HttpRequestWrapper 代理模式：方式生成了代理对象的地方就用到了代理模式、AOP、@Configuration、@Lazy 观察者模式：ApplicationListener——事件监听机制、AdvisedSupportListener——ProxyFactory 可以提交此监听器，用来监听 ProxyFactory 创建代理对象完成事件、添加 Advisor 事件等 策略模式：InstantiationStrategy——Spring需要根据BeanDefinition来实例化Bean，但是员体可以选择不同的策略来进行实例化、BeanNameGenerator——beanName 生成器 模板方法模式：AbstractApplicationContext：postProcessBeanFactory()——子类可以继续处理 BeanFactory、onRefresh()——子类可以做一些额外的初始化 责任链模式：DefaultAdvisorChainFactory——负责构造一条 AdvisorChain，代理对象执行某个方法时会依次经过 AdvisorChain 中的每个 Advisor、QualifierAnnotationAutowireCandidateResolver——判断某个Bean能不能用来进行依赖注入勉强可认为也是责任链 8. Spring 中 Bean 是线程安全的吗？ Spring 本身并没有针对 Bean 做线程安全的处理，所以 如果 Bean 是无状态的，那么 Bean 则是线程安全的 如果 Bean 是有状态的，那么 Bean 则不是线程安全的 另外，Bean 是不是线程安全，跟 Bean 的作用域没有关系，Bean 的作用域只是表示 Bean 的生命周期范围，对于任何生命周期的 Bean 都是一个对象，这个对象是不是线程安全的，还是得看这个 Bean 对象本身。 9. Spring中的Bean创建的生命周期有哪些步骤 https://juejin.cn/post/6844904065457979405 Spring 中一个 Bean 的创建大概分为以下几个步骤： 推断构造方法 实例化 填充属性，也就是依赖注入 处理 Aware 回调 初始化前，处理 @PostConstruct 注解 初始化，处理 InitializingBean 接口 初始化后，进行 AOP 10. Spring 中的事务是如何实现的 Spring事务底层是基于数据库事务和 AOP 机制的 首先对于使用了 @Transactional 注解的 Bean，Spring 会创建一个代理对象作为 Bean 当调用代理对象的方法时，会先判断该方法上是否加了 @Transactional 注解 如果加了，那么则利用事务管理器创建一个数据库连接 并且修改数据库连接的 autocommit 属性为 false，禁止此连接的自动提交，这是实现 Spring 事务非常重要的一步 然后执行当前方法，方法中会执行 sql 执行完当前方法后，如果没有出现异常就直接提交事务 如果出现了异常，并且这个异常是需要回滚的就会回滚事务，否则仍然提交事务 Spring 事务的隔离级别对应的就是数据库的隔离级别 Spring 事务的传播机制是 Spring 事务自己实现的，也是 Spring 事务中最复杂的 Spring 事务的传播机制是基于数据库连接来做的，一个数据车连接一个事务，如果传播机制图置为需要新开一个事务，那么实际上就是先建立一个数据车连接，在此新数据库连接上执行 sql 11. SpringBoot 启动原理 参考自：https://www.cnblogs.com/zyly/p/13194186.html，https://www.bilibili.com/video/BV1hv4y1z7PQ/ 1. 服务构建 new SpringApplication() 传入资源加载器 resourceLoader、主方法类 primarySources 逐一判断对应的服务类是否存在，来确定 Web 服务类型：Servlet、Reactive、None 加载初始化类，读取所有META-INF/spring.factories文件中的“注册初始化”、“上下文初始化”、“监听器”这三类配置，spring-boot和 spring-boot-autoconfigure 这两个工程中配置了 7 个“上下文初始化”和 8 个“监听器” 注册初始化 BootstrapRegistryInitializer，默认为空 设置上下文初始化 ApplicationContextInitializer 设置监听器 ApplicationListener 通过“运行栈” stackTrace 推断出 main 方法所在的类 2. 环境准备 SpringApplication.run() 新建 BootstrapContext “启动上下文”，逐一调用上面加载的“启动注册初始化器” BootstrapRegistryInitializer 中的 initialize 方法 将“java.awt.headless”这个设置改为 true，表示缺少显示器、键盘等输入设备也可以正常启动 通过 prepareEnvironment 方法“组装启动参数” 构造一个“可配置环境” ConfigurableEnvironment，根据不同的 Web 服务器类型会构造不同的环境 加载系统环境变量、JVM 系统属性等到 propertySources 中 通过 ConfigurableEnvironment 将传入的环境参数 args 进行设置 在 propertySources 集合首位添加 configurationProperties 空配置 发布“环境准备完成”事件 刚加载的 8 个 Listener 会监听到这个事件，其中的部分监听器会进行相应（串行）处理，例如EnvironmentPostProcessorApplicationListener 会去加载 spring.factories 配置文件中“环境配置后处理器 “ EnvironmentPostProcessor 考虑到刚创建的“可配置环境”在一系列过程中可能会有变化做的补偿，通过二次更新保证匹配 将”spring.beaninfo.ignore“设为 true，表示不加载 Bean 的元数据信息，打印 banner 3. 容器创建 ApplicationContext createApplicationContext()，根据服务类型创建”容器“ ConfigurableApplicationContext，对应 SERVLET 服务在过程会构造以下内容： Bean 工厂 用来解析@Component、@ComponentScan 等注解的“配置类后处理器 ConfigurationClassPostProcessor 用来解析@Autowired、@Value、@Inject 等注解的“自动注解 Bean 后处理器 AutowiredAnnotationBeanPostProcessor 等在内的属性对象 通过 prepareContext 方法对容器中的部分属性进行初始化 用 postProcessApplicationContext 方法设置”Bean名称生成器 beanNameGenerator“、”资源加载器 resourceLoader\"、”类型转换器 ConversionService“等 执行之前加载的 ApplicationContextInitializer，容器 ID、警告日志处理、日志监听都是在这里实现的 发布“容器准备完成”事件 为容器注册“启动参数”、“Banner”、“Bean 引用策略”和“懒加载策略”等等 通过 Bean 定义加载器将“启动类”在内的资源加载到 BeanDefinitionMap 中 发布“资源加载完成”事件 4. 填充容器 这个过程也被称为“自动装配”，包含“Bean 生命周期”管理和 Tomcat 启动： prepareRefresh：准备 servlet 相关的环境 obtainFreshBeanFactory：Springboot 无实际逻辑，Spring 会重新构造 beanfactory，加载 BeanDefinition prepareBeanFactory： 准备“类加载器” BeanClassLoader、“表达式解析器” BeanExpressionResolver、“配置文件处理器” PropertyEditorRegistrar 等系统级处理器 以及两个“Bean 后置处理器”：用来解析 Aware 接口的 ApplicationContextAwareProcessor、用来处理自定义监听器注册和销毁的 ApplicationListenerDetector 同时会注册一些”特殊 Bean“和“系统级 Bean“：比如容器本身 BeanFactory 和 ApplicationContext、系统环境 environment（特殊对象池）、系统属性 systemProperties（单例池） 等 postProcessBeanFactory：对 BeanFactory 进行额外设置或修改 主要定义了包括 request、session 在内的 Servlet 相关作用域 Scopes 注册与 Servlet 相关的特殊 Bean：包括 ServletRequest、ServletResponse、HttpSession 等 invokeBeanFactoryPostProcessors 执行各种 BeanFactory 后置处理器，其中最主要的就是用来加载所有”Bean定义“的”配置处理器“ ConfigurationClassPostProcessor，通过它加载所有 @Configuration 配置类 通过 Bean 扫描器将所有扫描出来的”Bean 定义“（包括加了 @Bean、@Import 等注解的类和方法）都放到 BeanDefinitionMap 中 registerBeanPostProcessors：检索所有 Bean 后置处理器，根据指定 order 进行排序，放入后置处理器池 beanPostProcessors 中，每一个后置处理器都会在 bean 初始化之前和之后分别执行对应的逻辑 会通过 initMessageSource 和 initApplicationEventMulticaster 方法，从单例池中获取两个非常实用的 Bean放在 ApplicationContext 中，一个是用于国际化，名为“messageSource”的 Bean，另一个是用于自定义广播事件，名为“applicationEventMulticaster”的 Bean，有了它就可以通过 publishEvent 方法进行事件的发布了 通过 onRefresh 构造并后动 Web 服务器，先查找实现了 ServletWebServerFactory 这个接口的应用服务器Bean，接下来通过 getWebServer 方法构造一个 Tomcat 对象并启动 通过 registerListeners 方法，在 bean 中查找所有的“监听器 Bean”，将它们注册到第八步构造的“消息广播器” applicationEventMulticaster 中 通过 finishBeanFactorylnitialization 来生产我们所有的 Bean，整体分为“构造对象、填充属性、机始化实例、注册销毁”四个步骤 通过 finishRefresh 方法构造并注册”生命周期管理器 lifecycleProcessor，同时会调用所有实现了“生命周期接口”Lifecycle 的 Bean 中的 start 方法，当然在容器关闭时也会自动调用对应的 stop 方法，发布“容器刷新完成”事件 12. 如何理解 SpringBoot 的 Starter 机制 提供预配置的依赖：Spring Boot Starters 是一组预配置的依赖项集合，用于启用特定类型的功能。例如，你可以使用 spring-boot-starter-web 启用 Web 应用程序相关的功能，包括内嵌的 Web 服务器、Spring MVC、Jackson 等。这样，你只需引入这个Starter，而无单独处理每个依赖项的版本和配置。 简化依赖管理：Spring Boot Starters 简化了项目的依赖管理。通过引入适当的 Starter 你不需要手动指定每个相关的依赖项，Spring Boot 会自动处理它们的版本兼容性和配置。这有助于避免版本冲突和配置错误。 自动配置：Spring Boot Starters 还包含了与功能相关的自动配置类。这些自动配置类根据应用程序的依赖和配置，自动配置了必要的组件和设置。这使得你可以快速地启用和使用功能，而无需手动配置每个组件。 遵循约定：Spring Boot Starters 遵循约定大于配置的原则，它们约定了如何将依敕和配置组合在一起，使得应用程序开发更加一致和高效。 自定义扩展：尽管 Spring Boot Starters 提供了默认的依赖和配置，你仍然可以根需要进行自定义扩展。你可以在自己的项目中覆盖默认的配置，添加额外的依赖项，或者通过属性配置文件来修改 Starter 的行为。 总之，Spring Boot Starter 机制是 Spring Boot 框架的一项重要功能，它通过提供预配置的依赖和自动配置，大大简化了项目的依赖管理和配置过程，使开发人员能够更专注于业务逻辑的实现。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"framework/netty.html":{"url":"framework/netty.html","title":"Netty","keywords":"","body":"1. Netty 如何实现自己的 I/O 模型 基于非阻塞 I/O 实现，底层依赖的是 JDK NIO 框架的多路复用器 Selector，一个多路复用器 Selector 可以同时轮询多个 Channel。 事件分发器 (Event Dispather)：负责将读写事件分发给对应的读写事件处理器(Event Handler)，Reactor和 Proactor。 2. Netty 整体架构 v4.1.42 Core 核心层：提供底层网络通信的通用抽象和实现包括可扩展的事件模型、通用的通信 API、支持零拷贝的 ByteBuf 等。 Protocol Support 协议支持层：覆盖了主流协议的编解码实现如 HTTP、SSL、Protobuf、压缩、大文件传输WebSocket、文本、二进制等，支持自定义应用层协议。 Transport Service 传输服务层：传输服务层提供了网络传输能力的定义和实现方法支持 Socket、HTTP 隧道、虚拟机管道等传输方式，Netty 的模块设计具备较高的通用性和可扩展性。 3. Netty 逻辑架构 网络通信层的职责是执行网络 I/O 的操作，支持多种网络协议和 I/O 模型的连接操作，包含 BootStrap、ServerBootStrap、Channel 三个组件。 Bootstrap 主要负责整个 Netty 程序的启动、初始化、服务器连接等过程 Bootstrap 可用于连接远端服务器，只绑定一个 EventLoopGroup（Boss） ServerBootStrap 用于服务端启动绑定本地端口，绑定两个 EventLoopGroup（Worker） 每个服务器中都会有一个 Boss，会有一群做事情的 WorkerBoss 会不停地接收新的连接，将连接分配给一个个 Worker 处理连接。 Channel 是网络通信的载体，提供了基本的 API 用于网络 I/O 操作，如 register、bind、connect、read、write、flush 等，Netty 自己实现的 Channel 是以 JDK NIO Channel 为基础的。 NioServerSocketChannel 异步 TCP 服务端、NioSocketChannel 异步 TCP 客户端 OioServerSocketChannel 同步 TCP 服务端、OioSocketChannel同步 TCP 客户端 NioDatagramChannel 异步 UDP 连接、OioDatagramChannel 同步 UDP 连接 Channel 会有多种状态，如连接建立、连接注册、数据读写、连接销毁等。 事件 说明 channelRegistered Channel 创建后被注册到 EventLoop 上 channelUnregistered Channel 创建后未注册或者从 EventLoop 取消注册 channelActive Channel 处于就绪状态，可以被读写 channellnactive Channel 处于非就绪状态 channelRead Channel 可以从远端读取到数据 channelReadComplete Channel 读取数据完成 事件调度层的职责：通过 Reactor 线程模型对各类事件进行聚合处理，通过 Selector 主循环线程集成多种事件，核心组件包括 EventLoopGroup、EventLoop。 一个 EventLoopGroup 往往包含一个或者多个 EventLoop，EventLoop 用于处理 Channel 声明周期内的所有 IO 事件，如 accept、connect、read、write 等 IO 事件 EventLoop 同一时间会有一个线程绑定，每个 EventLoop 负责处理多个 Channel 每新建一个 Channel，EventLoopGroup 会选择一个 EventLoop 与其绑定，该 Channel 在生命周期内都可以对 EventLoop 进行多次绑定和解绑 EventLoopGroup 是 Netty Reactor 线程模型的具体实现方式，Netty 通过不同的 EventLoopGroup 参数配置，就可以支持 Reactor 的三种线程模型。 单线程模型：EventLoopGroup 只包含一个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup 多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 和 Worker 使用同一个EventLoopGroup 主从多线程模型：EventLoopGroup 包含多个 EventLoop，Boss 是主 Reactor，Worker 是从 Reactor，主 Reactor 负责新的网络连接 Channel 创建，然后把 Channel 注册到从 Reactor 服务编排层：负责组装各类服务，用以实现网络事件的动态编排和有序传播，包含 ChannelPipeline、ChannelHandler、ChannelHandlerContext 组件。 ChannelPipeline：负责组装各种 ChannelHandler 实际数据的编解码以及加工处理操作由 ChannelHandler 完成，可以理解为 ChannelHandler 的实例列表，内部通过双向链表将不同的 ChannelHandler 链接在一起。当 I/O 读写事件触发时，ChannelPipeline 会依次调用 ChannelHandler 列表对 Channel 的数据进行拦截和处理。每一个新的 Channel 会对应绑定一个新的 ChannelPipeline（线程安全），一个 ChannelPipeline 关联一个 EventLoop 一个 EventLoop 仅会绑定一个线程。 每个 ChannelHandler 绑定 ChannelHandlerContext 的作用是什么呢？ 保存 ChannelHandler 上下文 实现 ChannelHandler 之间的交互 包含 ChannelHandler 生命周期的所有事件如 connect、bind、read、flush、write、close 等 如果每个 ChannelHandler 都有一些通用的逻辑需要实现没有 ChannelHandlerContext 这层模型抽象，你是不是需要写很多相同的代码呢？ 组件关系梳理： 服务端启动初始化时，有 Boss EventLoopGroup 和 Worker EventLoopGroup 两个组件 其中 Boss 负责监听网络连接事件，当有新的网络连接事件到达时，则将 Channel 注册到 Worker EventLoopGroup Worker EventLoopGroup 会被分配一个 EventLoop，负责处理该 Channel 的读写事件 每个 EventLoop 都是单线程的，通过 selector 进行事件循环，当客户端发起 io 读写事件时，服务端 EventLoop 会进行数据的读取 ，然后通过 pipeline 触发各种监听器，进行数据的加工处理 客户端数据会被传递到 ChannelPipeline 的第一个 ChannelInboundHandler 当中，数据处理完成后将加工完成的数据传递给下一个 ChannelInboundHandler 当数据写回客户端时，会将处理结果在 ChannelPipeline 的 ChannelOutboundHandler 中传播，最后到达客户端 Netty 源码结构： netty-common 模块——Netty 的核心基础包，提供了丰富的工具类 通用工具类：定时器工具 TimerTask、时间轮 HashedWheelTimer 等 自定义并发包：异步模型 Future & Promise、相比 JDK 增强的 FastThreadLocal 等 ByteBuf 工具类——网络通信中的数据载体 解决了 ByteBuffer 长度固定造成的内存浪费问题，更安全地更改了 Buffer 的容量 Netty 对 BvteBuf 做了优化——缓存池化、减少数据拷贝的 CompositeByteBuf 等 netty-resover 模块提供有关基础设施的解析工具——IP Address、Hostname、DNS 等 Protocol Support 协议支持层模块...（未完待续 https://www.bilibili.com/video/BV1e24y1z7eJ?p=3） Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"architecture/design_pattern.html":{"url":"architecture/design_pattern.html","title":"Design Pattern","keywords":"","body":"1. 设计原则 SOLID 原则：是面向对象设计和编程中的一组基本原则，由以下五个原则的首字母缩写组成： 单一职责原则（Single Responsibility Principle, SRP）：一个类或者模块只应该有一个单一的责任。这个原则告诉我们，一个类应该只负责一项功能，不要试图把太多的职责塞到一个类里面。 开闭原则（Open Closed Principle, OCP）：软件应该对扩展开放，对修改关闭。这个原则告诉我们，我们应该尽量通过扩展来实现新的功能，而不是去修改已经存在的代码。 里氏替换原则（Liskov Substitution Principle, LSP）：子类可以被看作是父类的一种类型，即父类能出现的地方子类也能够出现。这个原则告诉我们，在使用继承时，子类不能改变父类原有的行为，否则会导致程序出现意想不到的问题。 接口隔离原则（Interface Segregation Principle, ISP）：客户端不应该依赖于它不需要的接口。这个原则告诉我们，在设计接口时，应该尽量将接口拆分成更小粒度的接口，避免接口的臃肿和复杂度的增加。 依赖倒置原则（Dependency Inversion Principle, DIP）：高层模块不应该依赖低层模块，二者都应该依赖其抽象。这个原则告诉我们，在设计类和模块之间的关系时，应该通过抽象来实现低耦合、高内聚的设计。 2. 设计模式的类型 创建型模式：这些设计模式提供了一种在创建对象的同时隐藏创建逻辑的方式，而不是使用 new 运算符直接实例化对象。这使得程序在判断针对某个给定实例需要创建哪些对象时更加灵活。包括：工厂模式（Factory Pattern）、抽象工厂模式（Abstract Factory Pattern）、单例模式（Singleton Pattern）、建造者模式（Builder Pattern）、原型模式（Prototype Pattern） 结构型模式：这些模式关注对象之间的组合和关系，旨在解决如何构建灵活且可复用的类和对象结构。包括：适配器模式（Adapter Pattern）、桥接模式（Bridge Pattern）、过滤器模式（Filter、Criteria Pattern）、组合模式（Composite Pattern）、装饰器模式（Decorator Pattern）、外观模式（Facade Pattern）、享元模式（Flyweight Pattern）、代理模式（Proxy Pattern） 行为型模式： 这些模式关注对象之间的通信和交互，旨在解决对象之间的责任分配和算法的封装。包括：责任链模式（Chain of Responsibility Pattern）、命令模式（Command Pattern）、解释器模式（Interpreter Pattern）、迭代器模式（Iterator Pattern）、中介者模式（Mediator Pattern）、备忘录模式（Memento Pattern）、观察者模式（Observer Pattern）、状态模式（State Pattern）、空对象模式（Null Object Pattern）、策略模式（Strategy Pattern）、模板模式（Template Pattern）、访问者模式（Visitor Pattern） J2EE 模式：这些设计模式特别关注表示层。这些模式是由 Sun Java Center 鉴定的。包括：MVC 模式（MVC Pattern）、业务代表模式（Business Delegate Pattern）、组合实体模式（Composite Entity Pattern）、数据访问对象模式（Data Access Object Pattern）、前端控制器模式（Front Controller Pattern）、拦截过滤器模式（Intercepting Filter Pattern）、服务定位器模式（Service Locator Pattern）、传输对象模式（Transfer Object Pattern） 并发型模式（Concurrency Patterns）：这些模式涉及到多线程和并发编程方面的问题，用于解决并发环境下的一些常见问题。常见的并发型模式包括：生产者-消费者模式（Producer-Consumer Pattern）、读者-写者模式（Reader-Writer Pattern）、同步模式（Synchronization Pattern） 3. 单例模式 懒汉式单例模式在第一次调用的时候进行实例化。 适用于单线程环境（不推荐）： 此方式在单线程的时候工作正常，但在多线程的情况下就有问题了。如果两个线程同时运行到判断instance是否为null的if语句，并且instance的确没有被创建时，那么两个线程都会创建一个实例，此时类型Singleton就不再满足单例模式的要求了。 public class Singleton { private static Singleton instance = null; private Singleton() {} public static Singleton getInstance() { if (null == instance) { instance = new Singleton(); } return instance; } } 适用于多线程环境，但效率不高（不推荐）: 为了保证在多线程环境下我们还是只能得到该类的一个实例，只需要在getInstance()方法加上同步关键字sychronized，就可以了。但每次调用getInstance()方法时都被synchronized关键字锁住了，会引起线程阻塞，影响程序的性能。 public static synchronized Singleton getInstance() { if (instance == null) { instance = new Singleton(); } return instance; } 双重检验锁（DCL，即 double-checked locking）： 为了在多线程环境下，不影响程序的性能，不让线程每次调用getInstance()方法时都加锁，而只是在实例未被创建时再加锁，在加锁处理里面还需要判断一次实例是否已存在。 private static volatile Singleton instance = null; public static Singleton getInstance() { // 先判断实例是否存在，若不存在再对类对象进行加锁处理 if (instance == null) { synchronized (Singleton.class) { if (instance == null) { instance = new Singleton(); } } } return instance; } 如果 instance 属性不加 volatile，可能会返回未初始化完成的对象。 静态内部类方式（推荐）： 加载一个类时，其内部类不会同时被加载。一个类被加载，当且仅当其某个静态成员（静态域、构造器、静态方法等）被调用时发生。 由于在调用StaticSingleton.getInstance()的时候，才会对单例进行初始化，而且通过反射，是不能从外部类获取内部类的属性的；由于静态内部类的特性，只有在其被第一次引用的时候才会被加载，所以可以保证其线程安全性。 总结： 优势：兼顾了懒汉模式的内存优化（使用时才初始化）以及饿汉模式的安全性（不会被反射入侵） 劣势：需要两个类去做到这一点，虽然不会创建静态内部类的对象，但是其 Class 对象还是会被创建，而且是属于永久带的对象 public class StaticSingleton { private StaticSingleton() {} // 这里可以去掉synchronized，因为JVM会保证静态内部类的加载是线程安全 public static StaticSingleton getInstance() { return StaticSingletonHolder.instance; } // 一个私有的静态内部类，用于初始化一个静态final实例 private static class StaticSingletonHolder { private static final StaticSingleton instance = new StaticSingleton(); } } 饿汉式单例类：在类初始化时，已经自行实例化。 饿汉式： public class Singleton { private static final Singleton instance = new Singleton(); private Singleton() {} public static Singleton getInstance() { return instance; } } 枚举方式（推荐）： 创建枚举默认就是线程安全的，所以不需要担心 DCL，而且还自动支持序列化机制，防止反序列化重新创建新的对象，绝对防止多次实例化。不能通过 reflection attack 来调用私有构造方法。 public class Singleton { enum Single { SINGLE; private Single() {} public void print() {System.out.println(\"hello world\");} } } Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/shortcuts.html":{"url":"linux/shortcuts.html","title":"Terminal快捷键","keywords":"","body":"简明 VIM 练级攻略 常用 Ctrl+L ：清屏 Ctrl+M ：等效于回车 Ctrl+C : 中断正在当前正在执行的程序 历史命令 Ctrl+P : 上一条命令，可以一直按表示一直往前翻 Ctrl+N : 下一条命令 Ctrl+R : 再按历史命令中出现过的字符串：按字符串寻找历史命令（重度推荐） Ctrl+G : 从执行 Ctrl+R 的搜索历史命令模式退出 命令行编辑 Tab : 自动补齐（重度推荐） Ctrl+A ： 移动光标到命令行首 Ctrl+E : 移动光标到命令行尾 Ctrl+F : 光标前进 Ctrl+B : 光标后退 Alt+F : 光标前进一个单词 Alt+B : 光标后退一格单词 Ctrl+] : 从当前光标往后搜索字符串，用于快速移动到该字符串 Ctrl+Alt+] : 从当前光标往前搜索字符串，用于快速移动到该字符串 Ctrl+H : 删除光标的前一个字符（相当于退格键） Ctrl+D : 删除当前光标所在字符 Ctrl+K ：删除光标之后所有字符 Ctrl+U : 清空当前键入的命令 Ctrl+W : 删除光标前的单词(Word, 不包含空格的字符串) Ctrl+Y : 粘贴Ctrl+W或Ctrl+K删除的内容 Ctrl+\\ : 删除光标前的所有空白字符 Alt+. : 粘贴上一条命令的最后一个参数（很有用） Alt+[0-9],Alt+. 粘贴上一条命令的第[0-9]个参数 Alt+[0-9],Alt+.,Alt+. 粘贴上上一条命令的第[0-9]个参数 Ctrl+X,Ctrl+E : 调出系统默认编辑器编辑当前输入的命令，退出编辑器时，命令执行 控制快捷键 Ctrl+Z : 把当前进程放到后台（之后可用fg命令回到前台） Ctrl+S : 锁定终端，使之无法输入内容 Ctrl+Q : 解锁执行Ctrl+S的锁定状态 Ctrl+Insert : 复制 Shift+Insert : 粘贴（相当于 Windows 的 Ctrl+V） 在命令行窗口选中即复制 在命令行窗口中键即粘贴，可用 Shift+Insert 代替 Ctrl+PageUp : 屏幕输出向上翻页 Ctrl+PageDown : 屏幕输出向下翻页 !号开头的快捷命令 !! : 执行上一条命令 !pw : 执行最近以pw开头的命令 !pw:p : 仅打印最近pw开头的命令，但不执行 !num : 执行历史命令列表的第num(数字)条命令 !$ : 上一条命令的最后一个参数，相当于Esc+. Esc相关 Esc+. : 获取上一条命令的最后的部分（空格分隔） Esc+B : 移动到当前单词的开头 Esc+F : 移动到当前单词的结尾 Esc+T : 颠倒光标所在处及其相邻单词的位置 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/command.html":{"url":"linux/command.html","title":"Linux命令","keywords":"","body":"1. 常用命令 # 1.使用ssh连接远程主机 # 最简单的用法只需要指定用户名和主机名参数即可，主机名可以是IP地址或者域名。 ssh user@hostname # 2.ssh连接到其他端口 # SSH 默认连接到目标主机的22端口上，可以使用-p选项指定端口号 ssh -p 10022 user@hostname # 3.使用ssh在远程主机执行一条命令并显示到本地, 然后继续本地工作 # 直接连接并在后面加上要执行的命令就可以了 ssh pi@10.42.0.47 ls -l # 4.在远程主机运行一个图形界面的程序 # 使用ssh的-X选项，然后主机就会开启X11转发功能 ssh -X feiyu@222.24.51.147 # 5.构建ssh密钥对 # 使用ssh-keygen -t ，现在大多数都使用rsa或者dsa算法。 ssh-keygen -t rsa # 6.使用-F选项查看是否已经添加了对应主机的密钥 ssh-keygen -F 222.24.51.147 # 7，使用-R选项删除主机密钥，也可以在~/.ssh/known_hosts文件中手动删除 ssh-keygen -R 222.24.51.147 # 8.绑定源地址 # 如果你的客户端有多于两个以上的IP地址，你就不可能分得清楚在使用哪一个IP连接到ssh服务器。为了解决这种情况，我们可以使用-b选项来指定一个IP地址。这个IP将会被使用做建立连接的源地址。 ssh -b 192.168.0.200 root@192.168.0.103 # 9.对所有数据请求压缩 # 使用 -C 选项，所有通过ssh发送或接收的数据将会被压缩，并且仍然是加密的。 ssh -C root@192.168.0.103 # 10.打开调试模式 # 因为某些原因，我们想要追踪调试我们建立的ssh连接情况。ssh提供的-v选项参数正是为此而设的。其可以看到在哪个环节出了问题。 ssh -v root@192.168.0.103 # 11.上传公钥到服务器，之后就可以免密登录 ssh-copy-id -i ~/.ssh/id_rsa.pub root@192.168.235.22 2. /etc/ssh/sshd_config 配置文件详细说明 # 设置sshd监听的端口号 Port 2 # 设置sshd服务器绑定的IP地址 ListenAddress 192.168.1.1 # 设置包含计算机私人密匙的文件 HostKey /etc/ssh/ssh_host_key # 定义服务器密匙的位数 ServerKeyBits 1024 # 设置如果用户不能成功登录，在切断连接之前服务器需要等待的时间（以秒为单位） LoginGraceTime 600 # 这个参数的是意思是每5分钟，服务器向客户端发一个消息，用于保持连接 ClientAliveInterval 300（默认为0） # 设置在多少秒之后自动重新生成服务器的密匙（如果使用密匙）。重新生成密匙是为了防止用盗用的密匙解密被截获的信息 KeyRegenerationInterval 3600 # 设置root能不能用ssh登录。这个选项一定不要设成\"yes\" PermitRootLogin no # 设置验证的时候是否使用\"rhosts\"和\"shosts\"文件 IgnoreRhosts yes # 设置ssh daemon是否在进行RhostsRSAAuthentication安全验证的时候忽略用户的\"$HOME/.ssh/known_hosts IgnoreUserKnownHosts yes # 设置ssh在接收登录请求之前是否检查用户家目录和rhosts文件的权限和所有权。这通常是必要的，因为新手经常会把自己的目录和文件设成任何人都有写权限 StrictModes yes # 设置是否允许X11转发 X11Forwarding no # 设置sshd是否在用户登录的时候显示\"/etc/motd\"中的信息 PrintMotd yes # 设置在记录来自sshd的消息的时候，是否给出\"facility pre\" SyslogFacility AUTH # 设置记录sshd日志消息的层次 LogLevel INFO # 设置只用rhosts或\"/etc/hosts.equiv\"进行安全验证是否已经足够了 RhostsAuthentication no # 设置是否允许用rhosts或\"/etc/hosts.equiv\"加上RSA进行安全验证 RhostsRSAAuthentication no # 设置是否允许只有RSA安全验证 RSAAuthentication yes # 设置是否允许口令验证 PasswordAuthentication yes # 设置是否允许用口令为空的帐号登录 PermitEmptyPasswords no 3. ssh隧道 机器IP: 10.211.55.3 10.211.55.7 装有 nginx 本地转发，将远程机器的指定端口，通过本地的一个端口转发，适用于本地仅能连接远程机器的 ssh 端口： -L：表示使用本地端口转发创建 ssh 隧道 -R：表示使用远程端口转发创建 ssh 隧道 -N：表示创建隧道以后不连接到 sshServer 端，通常与-f选项连用 -f：表示在后台运行 ssh 隧道，通常与-N选项连用 ssh -fNL 12345:10.211.55.7:80 root@10.211.55.7 访问地址：http://localhost:12345/ 同上，区别：适用于远程机器仅能通过中间跳板机连接，本地无法直接访问： ssh -fNL 12345:10.211.55.7:80 root@10.211.55.7 -J root@10.211.55.3 访问地址：http://localhost:12345/ 远程转发，适用于自己有公网IP的虚机，且开放了对应端口的防火墙，并且远程虚机修改过对应配置： /etc/ssh/sshd_config GatewayPorts yes ssh -fNTR 71.132.36.61:10035:0.0.0.0:80 ec2-user@71.132.36.61 -i ./.ssh/temp.pem 访问地址：http://71.132.36.61:12345/ 4. Git 命令备忘 git add * # 会忽略.gitignore过滤 git commit --amend -m 'xxx' # 合并上一次提交 git commit -am 'xxx' # 添加同时提交，不包含untrack的文件 git rm xxx # 删除index中的文件 git rm -r * # 递归删除 git log -n # 显示n行日志 git log --stat # 显示提交日志及相关变动文件 git log -p -m git show # 默认显示HEAD日志的相关信息 git show dfb02e6e4f2f7b573337763e5c0013802e392818 # 显示某个提交的详细内容 git show dfb02 # 可只用commitid的前几位 git show HEAD # 显示HEAD提交日志 git show HEAD^ # 显示HEAD上一个版本的提交日志^^为上两个版本^5为上5个版本 git tag # 显示已存在的tag git tag -a v2.0 -m 'xxx' # 增加v2.0的tag git show v2.0 # 显示v2.0的日志及详细内容 git log v2.0 # 显示v2.0的日志 git log --oneline # 一行显示日志 git diff HEAD^ # 比较与上一个版本的差异 git diff HEAD -- ./lib # 比较与HEAD版本lib目录的差异 git diff origin/master..master # 比较远程分支master上有本地分支master上没有的 git diff origin/master..master --stat # 只显示差异的文件，不显示具体内容 git remote # 列出它存储的远端仓库别名 git remote -v # 还可以看到每个别名的实际链接地址 git remote add origin git+ssh://git@192.168.53.168/VT.git # 增加远程定义（用于push/pull/fetch） git branch --contains 50089 # 显示包含提交50089的分支 git branch -a # 显示所有分支 git branch -r # 显示所有原创分支 git branch --merged # 显示所有已合并到当前分支的分支 git branch --no-merged # 显示所有未合并到当前分支的分支 git branch -m master master_copy # 本地分支改名 git checkout -b master_copy # 从当前分支创建新分支master_copy并检出 git checkout -b master master_copy # 上面的完整版 git checkout features/performance # 检出已存在的features/performance分支 git checkout --track hotfixes/BJVEP933 # 检出远程分支hotfixes/BJVEP933并创建本地跟踪分支 git checkout v2.0 # 检出版本v2.0 git checkout -b devel origin/develop # 从远程分支develop创建新本地分支devel并检出 git checkout -- README # 检出head版本的README文件（可用于修改错误回退） git merge origin/master # 合并远程master分支至当前分支 git cherry-pick ff44785404a8e # 合并提交ff44785404a8e的修改 git push origin master # 将当前分支push到远程master分支 git push origin :hotfixes/BJVEP933 # 删除远程仓库的hotfixes/BJVEP933分支 git push --tags # 把所有tag推送到远程仓库 git fetch --prune # 获取所有原创分支并清除服务器上已删掉的分支 git pull --rebase git pull origin master # 获取远程分支master并merge到当前分支 git mv README README2 # 重命名文件README为README2 git reset --soft HEAD # 重置位置的同时，保留working Tree工作目录和index暂存区的内容，只让repository中的内容和 reset 目标节点保持一致，因此原节点和reset节点之间的「差异变更集」会放入index暂存区中(Staged files)。所以效果看起来就是工作目录的内容不变，暂存区原有的内容也不变，只是原节点和Reset节点之间的所有差异都会放到暂存区中 git reset (--mixed) HEAD # 重置位置的同时，只保留Working Tree工作目录的內容，但会将 Index暂存区 和 Repository 中的內容更改和reset目标节点一致，因此原节点和Reset节点之间的「差异变更集」会放入Working Tree工作目录中。所以效果看起来就是原节点和Reset节点之间的所有差异都会放到工作目录中 git reset --hard HEAD # 重置位置的同时，直接将 working Tree工作目录、 index 暂存区及 repository 都重置成目标Reset节点的內容,所以效果看起来等同于清空暂存区和工作区 git rebase git rebase -i head~4 # 查看当前后面的4个提交 git branch -d hotfixes/BJVEP933 # 删除分支hotfixes/BJVEP933（本分支修改已合并到其他分支） git branch -D hotfixes/BJVEP933 # 强制删除分支hotfixes/BJVEP933 git ls-files # 列出git index包含的文件 git show-branch # 图示当前分支历史 git show-branch --all # 图示所有分支历史 git whatchanged # 显示提交历史对应的文件修改 git revert dfb02e6e4f2f7b573337763e5c0013802e392818 # 撤销提交dfb02e6e4f2f7b573337763e5c0013802e392818 git ls-tree HEAD # 内部命令：显示某个git对象 git rev-parse v2.0 # 内部命令：显示某个ref对于的SHA1 HASH git reflog # 显示所有提交，包括孤立节点 git show HEAD@{5} git show master@{yesterday} # 显示master分支昨天的状态 git log --pretty=format:'%h %s' --graph # 图示提交日志 git show HEAD~3 git show -s --pretty=raw 2be7fcb476 git stash list # 查看所有暂存 git stash save \"test-cmd-stash\" # 暂存时添加message git stash show -p stash@{0} # 参考第一次暂存 git stash apply stash@{0} # 应用第一次暂存但不删除 git stash drop stash@{0} # 删除暂存 git stash clear # 删除所有缓存的stash git grep \"delete from\" # 文件中搜索文本“delete from” git grep -e '#define' --and -e SORT_DIRENT git gc git fsck 5. PowerShell How do you set PowerShell's default directory? 删除文件 Remove-Item -Path \"*\" -Include \"*.txt\" -Recurse -Force 获取文件个数 Get-ChildItem -Include *.dump -Recurse | select Name | Measure-Object 删除Sample开头的sqlserver数据库 $Databases = Invoke-SQLcmd -ServerInstance $ServerInstance -Query (\"SELECT * FROM sys.databases WHERE NAME LIKE 'Sample%'\") ForEach ($Database in $Databases){Invoke-SQLcmd -ServerInstance $ServerInstance -Query (\"DROP DATABASE [\" + $Database.Name + \"]\") \"$($Database.Name) is deleted.\"} dump文件转postgres psql -U postgres -d -f 'C:\\xxxxxx.dump' 删除Sample开头的postgres数据库 psql -U postgres -t -A -c \"select datname from pg_database where datname ~ 'Sample\\w*'\" | ForEach-Object { dropdb --force --echo --username postgres \"$_\" } Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/jsonpath.html":{"url":"linux/jsonpath.html","title":"JsonPath","keywords":"","body":"参考资料 课程：https://kodekloud.com/courses/json-path-quiz/ 注意点： json query 得到的都是数组形式的 result list list 有序（序号从0开始），而 dictionary 无序 取第0个和第3个：$[0,3] 取第0个到第3（不包含）个：$[0:3] 每隔2个取第0个到第8（不包含）个：$[0:8:2] 取最后1个：在某些实现下不起作用$[-1]，需要写成$[-1:0]，$[-1:] 操作符 操作符 描述 $ root根元素 @ 表示list中的每一个元素 * 匹配所有 ?() if过滤匹配 ['' (, '')] 用方括号和单引号取值 逻辑表达式 逻辑表达式 描述 ==, !=, , >= 等于，不等于... =~ 右边匹配正则表达式 in, nin 匹配是否存在数组中 subsetof, anyof, noneof 子集匹配 size, empty 数组大小匹配 函数 函数 描述 输出类型 min, max, avg 最小最大平均值 Double stddev 标准偏差值 Double sum 求和 Double length 长度 Integer keys 提供属性键（终端波浪号~的替代项） Set concat, append 拼接，加入 like input first, last, index 数组元素 由数组决定 kubectl $非强制需要写上，kubectl会帮助添加 拼接内容：kubectl get nodes -o=jsonpath='{.items[*].metadata.name}{\"\\n\"}{.items[*].status.capacity.cpu}' Loop： '{range .items[*]} {.metadata.name}{\"\\t\"}{.status.capacity.cpu}{\"\\n\"} {end}' custom-columns： kubectl get nodes -o=custom-columns=: Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/regex.html":{"url":"linux/regex.html","title":"正则表达式","keywords":"","body":"匹配模式 讲道理的正则表达式对于 *+? 的匹配模式是贪心匹配 用a*a*b匹配aab时，a*吃掉了两个a，而a*没得a吃 在 *+? 后面加上一个 ? 可以把匹配模式变为惰性匹配 用a*?a*?b 匹配aab时，a*?一个a都不吃，而a*?被迫吃掉了两个a 在 *+? 后面加上一个+可以把匹配模式变为侵占匹配 用a*+a无法匹配aaa，因为a*+抢走了所有的a，导致a匹配失败 分组 \\ 如\\1，引用前面的第1个分组var match = Regex.Match(\"abcabc\", @\"(.+)\\1\"); // match.Groups[1] = \"abc\" var result = Regex.Replace(\" 358 * (126 + 282)\", @\"(\\d+) \\* \\((\\d+) \\+ (\\d+)\\)\", \"$1 * $2 + $1 * $3\"); // result = \"358 * 126 + 358 * 282\" 对于不想要的分组，开头加上 ?:var match = Regex.Match(\"abcabc\", @\"(?:.+)\\1\"); // Reference to undefined group number 1 注意只要有一对单独的圆括号就存在一个组，顺序为开括号出现的顺序var match = Regex.Match(\"abcdef\", @\"(?:a(bc)(d(ef)))\"); // match.Groups[1] = bc // match.Groups[2] = def // match.Groups[3] = ef 分组时用 ? 指定名字，即可用 \\k 的方式引用var match = Regex.Match(\"abcabc\", @\"(?.+)\\k\") // match.Groups[\"x\"] = abc 正则替换的引用方式: var result = Regex.Replace(\" 358 + 126\", @\"(?\\d+) \\+ (?\\d+)\", \"${B} + ${A}\") // result = \"126 + 358\" 断言 (? 匹配前面匹配 x 的 y x(?=y) 匹配后面匹配 y 的 x (? 匹配前面不匹配 x 的 y x(?!y) 匹配后面不匹配 y 的 x ^ 断言当前位置为字符串开头 $ 断言当前位置为字符串末尾 \\b 断言当前位置为单词(包括数字下划线)的边界 C# 特供 C# 对于正则的分组行为强于其它语言 组以堆栈的形式存在而非单个元素 对于同组匹配多个的正则，能同时存储被匹配的内容 对于带名称的分组效果相同 除了放入元素还可以取出元素，遵循堆栈的后入先出原则Pattern p = Pattern.compile(\"(.)+\") Matcher m = p.matcher(\"abc\"); if (m.matches()) System.out.println(m.group(1)); // 输出:c var groups = Regex.Match(\"abc\", \"(.)+\").Groups; var captures = groups[1].Captures.ToArray(); var output = string.Join(\",\"), captures); Console.WriteLine(output); // 输出:a,b,c Multiple outputs from T4 made easy – revisited Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/system.html":{"url":"linux/system.html","title":"操作系统","keywords":"","body":"1. IO 多路复用 select、poll、epoll select 模型，使用的是数组来存储 Socket 连接文件描述符，容量是固定的，需要通过轮询来判断是否发生了 IO 事件 poll 模型，使用的是链表来存储 socket 连接文件描述符，容量是不固定的，同样需要通过轮询来判断是否发生了 IO 事件 epoll 模型，epoll 和 poll 是完全不同的，epoll 是一种事件通知模型，当发生了 IO 事件时，应用程序才进行 IO 操作，不需要像 poll 模型那样主动去轮询 select： sockfd = socket(AF_INET,SOCK_STREAM, 0); memset(&addr, 0, sizeof(addr)); addr.sin_family = AF_INET; addr.sin_port = htons(2000); addrsin_addr.s_addr = INADDR ANY; bind(sockfd, (struct sockaddr*)&addr, sizeof(addr)); listen(sockfd, 5); for (i=0;i max) max = fds[i]; } // 上面用来创建5个文件描述符 while(1){ FD_ZER0(&rset); // 初始化赋空值 for (i = 0; i select 中由内核来判断 fd（rset 是 bitmap 形式保存 fd 的状态） 是否有数据。 fd 没有数据时，会一只阻塞在 select() 有数据时，fd 置位表示有数据（注意这里的 FD 置位中 FD 指的其实是 rset 中对应的那一位，而不是真正的 fds 中的元素），select() 返回 缺点： fd 的大小默认是 1024，可以调整，但是仍然有上限 FDset 不可重用，每次都需要初始化 rset 拷贝到内核态仍然需要一定的开销，虽然比判断单个 fd 效率高 select() 返回后仍需要遍历，不清楚其中哪个 fd 有数据，复杂度 O(n) poll： struct pollfd { int fd; short events; // 表示 读、写、读写 事件 short revents; } for (i=0;i 用户态是数组存储 fd 状态，内核态使用的是链表 epoll： struct epoll_event events[5]; int epfd = epoll_create(10);// 白板 ... ... for (i=0;i 用户态和内核态共享 epfd，没有拷贝的开销。 有数据时通过重排置位，把有数据的 fd 放到最前面，然后返回有数据的个数 nfds。 epoll LT 模式和 ET 模式详解：https://cloud.tencent.com/developer/article/1636224 2. CAP 理论 CAP理论是分布式领域中非常重要的一个指导理论，C（Consistency）表示强一致性，A（Availability）表示可用性，P（Patition Tolerance）表示分区容错性，CAP 理论指出在目前的硬件条件下，一个分布式系统是必须要保证分区容错性的，而在这个前提下，分布式系统要么保证 CP，要么保证 AP，无法同时保证 CAP。 分区容错性表示，，一个系统虽然是分布式的，但是对外看上去应该是一个整体，不能由于分布式系统内部的某个结点挂点，或网络出现了故障，而导致系统对外出现异常所以，对于分布式系统而言是一定要保证分区容错性的。 强一致性表示，一个分布式系统中各个结点之间能及时的同步数据，在数据同步过程中，是不能对外提供服务的，不然就会造成数据不一致，所以强一致性和可用性是不能同时满足的。 可用性表示，一个分布式系统对外要保证可用。 3. BASE 理论 由于不能同时满足 CAP，所以出现了 BASE 理论： BA：Basically Available，表示基本可用，表示可以允许一定程度的不可用，比如由于系统故障，请求时间变长，或者由于系统的障导致部分非核心功能不可用，都是允许的 S：Soft state：表示分布式系统可以处于一种中间状态，比如数据正在同步 E：Eventually consistent，表示最终一致性，不要求分布式系统数据实时达到一致，允许在经过一段时间后再达到一致，在达到一致过程中，系统也是可用的 4. Linux 的五种主要 IO 模式 同步阻塞 I/O（BIO） sequenceDiagram Application->>+Kernel: 系统调用 Kernel->>+Application: 数据从内核缓冲区拷贝到用户态缓冲区 应用进程向内核发起 IO 请求，发起调用的线程一直等待内核返回结果，只能使用多线程模型，一个请求对应一个线程。 同步非阻塞 I/O（NIO） sequenceDiagram Application->>+Kernel: 系统调用 Kernel->>+Application: 无数据 Application->>+Kernel: 系统调用 Kernel->>+Application: 无数据 Application->>+Kernel: 系统调用 Kernel->>+Application: 数据从内核缓冲区拷贝到用户态缓冲区 应用进程向内核发起 IO 请求后，不再会同步等待结果，而是会立即返回，通过轮询的方式获取请求结果。NIO 相比 BIO 虽然大幅提升了性能，但是轮询过程当中大量的系统调用，导致上下文切换开销很大，所以单独使用非阻塞 IO 时效率并不高，并且随着并发量的提升，非阻塞 IO 会存在严重的性能浪费。 I/O 多路复用 sequenceDiagram Application->>+Kernel: 系统调用 Kernel->>+Application: 无数据 Kernel->>+Application: 数据已经准备就绪 Application->>+Kernel: 系统调用 Kernel->>+Application: 数据从内核缓冲区拷贝到用户态缓冲区 多路复用实现了一个线程处理多个 IO 句柄的操作，多路指的是多个数据通道，复用指的是使用一个或多个固定线程来处理每一个 socket、select、poll、epoll，都是 IO 多路复用的具体实现。线程一次 select 调用，可以获取内核态中多个数据通道的数据状态。多路复用解决了同步阻塞 IO 和同步非阻塞 IO 的问题，是一种非常高效的 IO 模型。 信号驱动 I/O sequenceDiagram Application->>+Kernel: 系统调用 Kernel->>+Application: 无数据，已收到信号 Kernel->>+Application: 数据已经准备就绪，递交SIGO信号 Application->>+Kernel: 系统调用 Kernel->>+Application: 数据从内核缓冲区拷贝到用户态缓冲区 信号驱动 IO 并不常用，它是一种半异步的 IO 模型，当数据准备就绪后，内核通过发送一个 SIGO 信号，通知应用进程，应用进程就可以开始读取数据了 异步 I/O sequenceDiagram Application->>+Kernel: 系统调用 Kernel->>+Application: 无数据 Kernel->>+Application: 数据已经准备就绪，数据从内核缓冲区拷贝到用户态缓冲区 异步 IO 最重要的一点是，从内核缓冲区拷贝数据到用户态缓冲区的过程，也是由系统异步完成，应用进程只需要在指定的数组中引用数据即可。异步 IO 和信号驱动 IO 这种半异步模式的主要区别是信号驱动 IO 由内核通知何时可以开始一个 IO 操作，而异步 IO 由内核通知 IO 操作何时已经完成。 5. 零拷贝 方法类型 CPU 拷贝 DMA 拷贝 系统调用 上下文切换 传统方法 2 2 read+write 4 mmap+write 1 2 mmap+write 4 sendfile 1 2 sendfile 2 sendfile + DMA gather copy 0 2 sendfile 2 传统方法： mmap+write： sendfile： sendfile + DMA gather copy： Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/network.html":{"url":"linux/network.html","title":"Network","keywords":"","body":"1. TCP/IP 网络模型有哪几层？ 应用层（message消息或报文）：工作在操作系统中的用户态，传输层及以下则工作在内核态 传输层（segment段）：为应用层提供网络支持，如 TCP（Transmission Control Protocol 传输控制协议）、UDP（用户数据报协议 User Datagram Protocol） 网络层（packet包）：网络层最常使用的是 IP 协议（Internet Protocol），IP 协议会将传输层的报文作为数据部分，再加上 IP 包头组装成 IP 报文，如果 IP 报文大小超过 MTU（以太网中一般为 1500 字节）就会再次进行分片，得到一个即将发送到网络的 IP 报文 网络接口层（frame帧）：网络接口层主要为网络层提供「链路级别」传输的服务，负责在以太网、WiFi 这样的底层网络上发送原始数据包，工作在网卡这个层次，使用 MAC 地址来标识网络上的设备。在 IP 头部的前面加上 MAC 头部，并封装成数据帧（Data frame）发送到网络上 IP 地址分成两种意义： 一个是网络号，负责标识该 IP 地址是属于哪个「子网」的； 一个是主机号，负责标识同一「子网」下的不同主机； IP地址 & 子网掩码 = 网络号 IP地址 & 子网掩码取反 = 主机号 CIDR（Classless Inter-Domain Routing 无类别域间路由）是一种对IP地址进行分配和路由选择的标准方法。 2. OSI（Open System Interconnection Reference Model 开放式系统互联通信参考模型） 物理层（Physical Layer）：负责传输原始的比特流，处理物理介质、电压等物理连接细节。 数据链路层（Data Link Layer）：在直接连接的节点之间传输数据，处理帧（Frame）的发送和接收，提供可靠的数据传输。 网络层（Network Layer）：负责在不同网络之间进行路径选择和数据包转发，处理数据包的路由和寻址。 传输层（Transport Layer）：提供端到端的可靠数据传输，负责分段、流控制、差错检测和恢复等功能。 会话层（Session Layer）：建立、管理和终止应用程序之间的会话，负责进行对话控制和同步。 表示层（Presentation Layer）：处理数据的表示格式，负责数据的加密、压缩、解压缩以及数据格式的转换等。 应用层（Application Layer）：提供直接面向用户的应用服务，包括各种应用程序和协议，如电子邮件、文件传输、远程登录等。 3. ARP（Address Resolution Protocol 地址解析协议） ARP 是用于在局域网（LAN）中将IP地址解析为对应的物理MAC地址的协议。 在计算机网络中，数据包在传输过程中需要知道目标设备的物理地址（MAC地址），而通常应用程序使用的是目标设备的IP地址。ARP 协议（如果目标设备的 IP 地址和 MAC 地址映射在 ARP 缓存中不存在）通过在局域网内广播请求，询问某个IP地址对应的MAC地址，从而实现 IP 地址到 MAC 地址的解析。 在 Linux 系统中，我们可以使用 arp -a 命令来查看 ARP 缓存的内容。 > arp -a ? (192.168.3.20) at f0:76:1c:58:f4:bc [ether] on eth2 # IP地址 MAC地址 网口名称 4. ICMP（Internet Control Message Protocol Internet控制报文协议） ICMP 是一种网络协议，用于在IP网络中传输控制和错误消息。 ICMP 协议的主要功能包括以下几个方面： 错误报告：当在网络通信中出现错误时，ICMP 可以生成和传输错误报文给源设备，以通知发送者发生的问题。例如，当目标主机不可达、数据报超时、端口不可达等情况发生时，ICMP 可以生成相应的错误报文。 回显请求和回显回答：ICMP 提供了回显请求（Ping）和回显回答的功能。通过发送一个回显请求消息到目标设备，可以测试目标设备是否可达和计算往返时间（RTT）。目标设备收到回显请求后，会发送一个回显回答消息作为响应。 传输状况报告：ICMP 可以传输有关网络状况的信息。例如，当路由器转发数据报时，可以使用 ICMP 来发送传输状况报告，如 TTL（生存时间）过期报文等。 网络重定向：当路由器发现数据包正在沿着非最佳路径传输时，它可以使用 ICMP 报文发送网络重定向消息，告知发送者采用更佳的路径。 ICMP 协议通常工作在网络层（第3层），它直接封装在 IP 数据报中传输。它在网络通信中起到控制和诊断的作用，帮助网络管理员监控和排除网络问题，并提供了一些基本的网络工具和命令，如 Ping 和 Traceroute。 5. TCP报文 源端口和目的端口：各16bits，端口是传输层与应用层的服务接口 序号seq：32bits，TCP 连接中传送的数据流中的每一个字节都编上一个序号，序号字段的值则指的是本报文段所发送的数据的第一个字节的序号 确认号ack：32bits，是期望收到对方的下一个报文段的数据的第一个字节的序号 数据偏移：4bits，表示首部长度包含可选部分，最小单位为32bits 保留：6bits，保留为今后使用，但目前应置为 0 URG：当 URG=1 时，表示紧急数据，相当于高优先级的数据，尽快传送 ACK：当 ACK=1 时确认号字段才有效，当 ACK=0 时无效 PSH(PuSH)：当 PSH = 1 时，就尽快地交付接收应用进程，而不再等到整个缓存都填满了后再向上交付 RST (ReSeT)：当 RST=1 时，由于主机崩溃或其他等原因必须释放然后再重新建立连接 SYN：当 SYN = 1 时，表示这是一个连接请求或连接接受报文 FIN：但 FIN=1 时，表示发送方的数据已发送完毕，要求释放连接 窗口大小：16bits，窗口字段用来控制对方发送的数据量，单位为字节。TCP 连接的一端根据设置的缓存空间大小确定自己的接收窗口大小，然后通知对方以确定对方的发送窗口的上限。 检验和：16bits，校验首部前12字节 + 数据部分 紧急指针：16bits，指出在本报文段中紧急数据共有多少个字节（紧急数据放在本报文段数据的最前面） 选项options：长度可变。TCP首部可以有多达40字节的可选信息，用于把附加信息传递给终点，或用来对齐其它选项。 这部分最多包含40字节，因为TCP头部最长是60字节（其中还包含前面讨论的20字节的固定部分） 填充：使整个首部长度是 4 字节的整数倍 6. options 字段 TCP报文中的选项（Options）字段具有以下结构： 选项类型（Option Kind）：1字节，表示选项的类型。常见的选项类型包括： 0：表示选项字段的结束标记（End of Option List）。 1：表示无操作（No-Operation），用于填充字节和对齐选项字段。 2：表示最大报文段长度（Maximum Segment Size，MSS）。 3：表示窗口扩大因子（Window Scale）。 4：表示选择确认（Selective Acknowledgment，SACK）。 其他选项类型还包括时间戳（Timestamp）、窗口探测（Window Probe）等。 选项长度（Option Length）：1字节，表示选项字段的长度，包括选项类型字段和选项数据字段的总长度。 选项数据（Option Data）：可变长度，根据选项类型的不同而具体确定。选项数据字段包含了选项的具体内容和参数。 TCP报文的选项字段可以包含多个选项，按顺序排列。每个选项的结构由选项类型、选项长度和选项数据组成。选项字段的长度是根据选项数据的实际长度计算得出的。 在TCP报文头中，选项字段紧跟在固定长度的报文头字段后面。如果选项字段的长度不是4字节的倍数，将会使用填充字节进行对齐，确保选项字段的起始位置是4字节对齐的。 选项字段的使用是根据需要和协商来确定的。发送方和接收方需要共同支持并理解使用的选项类型和相应的选项数据格式，以确保正确的解析和处理。选项字段的存在为TCP协议提供了一定的灵活性和功能扩展性，允许在TCP报文中传递额外的信息和实现协议的扩展功能。 7. 三次握手 目的是保证双方都有发送和接收的能力 ISN：初始化序列号（Initial Sequence Number） 8. 查看 TCP 连接状态 netstat -napt #-n（或 --numeric）：以数字形式显示 IP 地址和端口号，而不进行反向解析。 #-a（或 --all）：显示所有的连接，包括监听和非监听状态的连接。 #-p（或 --program）：显示与连接相关的程序名称和 PID。 #-t（或 --tcp）：仅显示 TCP 协议相关的连接信息。 9. http报文 TCP 报文中的数据部分就是存放 HTTP 头部 + 数据 （1）请求方法 URI 协议 / 版本 GET /index.html HTTP/1.1 http 1.1协议支持以下几种请求方法： GET：用于从服务器获取指定资源（幂等） POST：用于向服务器提交数据 HEAD：与 GET 方法类似，但只请求获取资源的元数据，而不包含实际的资源内容。主要用于获取资源的元信息，如资源的大小、最后修改时间等 PUT：将请求中包含的实体存储在服务器上的指定位置。PUT 方法用于上传、替换或创建指定位置的资源 DELETE：请求服务器删除指定的资源 OPTIONS：用于请求服务器返回对指定资源支持的通信选项。主要用于获取服务器支持的请求方法、报文头部支持的字段等信息 TRACE：用于对服务器进行环回测试。服务器收到 TRACE 请求后，应该将请求报文的内容作为响应主体返回给客户端，用于检测中间的代理服务器或网关对请求的修改。 CONNECT：用于建立与目标服务器的隧道连接，通常用于进行安全的HTTPS通信。 （2）请求头 (Request Header) 请求头包含许多有关的客户端环境和请求正文的有用信息。例如，请求头可以声明浏览器所用的语言，请求正文的长度等。 Accept:image/gif.image/jpeg.*/* Accept-Language:zh-cn Connection:Keep-Alive Host:localhost User-Agent:Mozila/4.0(compatible:MSIE5.01:Windows NT5.0) Accept-Encoding:gzip,deflate. (3) 请求正文 请求头和请求正文之间是一个空行，这个行非常重要，它表示请求头已经结束，接下来的是请求正文。请求正文中可以包含客户提交的查询字符串信息。 10. IP报文 版本（Version）：4 bits，IP协议的版本号，常用4、6 首部长度（Header Length）：4 bits，不包含Data部分，单位4 bytes，最大 60 bytes 服务类型（Type of Service）：8 bits，用于指定IP报文的优先级、延迟和吞吐量要求等服务质量(QoS)相关信息 总长度（Total Length）：16 bits，指定整个IP报文的总长度（包括IP首部和数据部分），单位 bytes，最大值为65535 标识（Identification）：16 bits，用于标识IP报文的唯一标识符。在分片传输中，这个字段在所有片段中保持不变 标志（Flags）：3 bits，用于指示是否进行分片以及如何处理分片 第一个比特位为保留位 第二个比特位为不分片DF （Don't Fragment）标志位，用于指示该报文是否允许分片 第三个比特位为更多分片MF（More Fragments）标志位，用于指示是否还有更多分片 片偏移（Fragment Offset）：13 bits，用于指示该分片相对于原始IP报文开始位置的偏移量，以8字节为单位 生存时间（Time to Live）：8 bits，表示报文在网络中可以经过的最大跳数（即经过的路由器数量） 协议（Protocol）：8 bits，指定IP报文中携带的上层协议类型，例如TCP(6)、UDP(17)、ICMP(1)等。 首部校验和（Header Checksum）：16 bits，用于检验IP首部的完整性，以便在接收端进行错误检测 源IP地址（Source IP Address）：32 bits，指定报文的发送者的IP地址 目标IP地址（Destination IP Address）：32 bits，指定报文的接收者的IP地址 选项（Options）：可选字段，用于在IP报文中传递一些额外的控制和参数信息，如记录路由路径、时间戳等 数据（Data）：IP报文的数据部分，根据协议类型的不同而具有不同的格式和含义 11. 假设客户端有多个网卡，就会有多个 IP 地址，那 IP 头部的源地址应该选择哪个 IP 呢？ 当存在多个网卡时，在填写源地址 IP 时，就需要判断到底应该填写哪个地址。这个判断相当于在多块网卡中判断应该使用哪个一块网卡来发送包。 这个时候就需要根据路由表规则，来判断哪一个网卡作为源地址 IP。 在 Linux 操作系统，我们可以使用 route -n 命令查看当前系统的路由表。 > route -n Kernel IP routing table Destination Gateway Genmask Flags Metric RefUse Iface 192.168.3.0 0.0.0.0 255.255.255.0 U 0 0 eth0 192.168.10.0 0.0.0.0 255.255.255.0 U 0 0 eth1 0.0.0.0 192.168.3.1 0.0.0.0 UG 0 0 eth0 举个例子，根据上面的路由表，我们假设 Web 服务器的目标地址是 192.168.10.200。 首先先和第一条目的子网掩码（Genmask Generalized netmask）进行 与运算，得到结果为 192.168.10.0，但是第一个条目的 Destination 是 192.168.3.0，两者不一致所以匹配失败。 再与第二条目的子网掩码进行 与运算，得到的结果为 192.168.10.0，与第二条目的 Destination 192.168.10.0 匹配成功，所以将使用 eth1 网卡的 IP 地址作为 IP 包头的源地址。 那么假设 Web 服务器的目标地址是 10.100.20.100，那么依然依照上面的路由表规则判断，判断后的结果是和第三条目匹配。 第三条目比较特殊，它目标地址和子网掩码都是 0.0.0.0，这表示默认网关，如果其他所有条目都无法匹配，就会自动匹配这一行。并且后续就把包发给路由器，Gateway 即是路由器的 IP 地址。 12. MAC 报文 MAC（Media Access Control）报文是在数据链路层上使用的一种格式，用于在本地网络中的主机之间进行通信。下面是MAC报文的基本格式： 目的MAC地址（6字节） 源MAC地址（6字节） 类型/长度字段（2字节） 数据（46-1500字节） 帧校验序列（4字节） 目的MAC地址（Destination MAC Address）：指示报文的目标设备的物理地址，通常是一个唯一的标识符，采用十六进制表示法（如00:11:22:33:44:55） 源MAC地址（Source MAC Address）：指示报文的发送设备的物理地址，也是一个唯一的标识符 类型/长度字段（Type/Length Field）：用于指示数据字段中包含的协议类型或者报文长度。 如果该字段的值大于等于0x600（1536），则表示指示的是协议类型，一般在 TCP/IP 通信里，MAC 包头的协议类型只使用：0800 ： IP 协议，0806 ： ARP 协议 如果该字段的值小于0x600，则表示指示的是报文的长度 数据（Data）：MAC报文携带的有效数据部分，长度通常为46至1500字节。这部分数据是由上层协议（如IP协议）产生的数据。 帧校验序列（Frame Check Sequence，FCS）：用于校验报文是否在传输过程中发生错误。它是通过对报文中的数据部分进行计算所得到的一种校验值，长度为4个字节。 需要注意的是，上述格式是Ethernet（以太网）中MAC报文的基本格式，还有其他类型的局域网技术和数据链路协议也使用不同的报文格式，但是以太网是目前应用最广泛的局域网技术之一。 13. 出口 —— 网卡 网络包只是存放在内存中的一串二进制数字信息，没有办法直接发送给对方。因此，我们需要将数字信息转换为电信号，才能在网线上传输，也就是说，这才是真正的数据发送过程。 负责执行这一操作的是网卡，要控制网卡还需要靠网卡驱动程序。 网卡驱动获取网络包之后，会将其复制到网卡内的缓存区中，接着会在其开头加上报头和起始帧分界符，在末尾加上用于检测错误的帧校验序列。 起始帧分界符是一个用来表示包起始位置的标记 末尾的 FCS（帧校验序列）用来检查包传输过程是否有损坏 最后网卡会将包转为电信号，通过网线发送出去。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"linux/algorithm.html":{"url":"linux/algorithm.html","title":"Algorithm","keywords":"","body":"1. 排序算法 稳定性：用于描述排序算法在处理相等元素时是否保持它们的相对顺序不变。 各种排序算法的时间和空间复杂性 类别 排序方法 时间复杂度 空间复杂度 稳定性 平均情况 最好情况 最坏情况 辅助存储 插入排序 直接插入 O(n2) O(n) O(n2) O(1) 稳定 希尔排序 O(n1.3) O(n) O(n2) O(1) 不稳定 选择排序 直接选择 O(n2) O(n2) O(n2) O(1) 不稳定 堆排序 O(nlogn) O(nlogn) O(nlogn) O(1) 不稳定 交换排序 冒泡排序 O(n2) O(n) O(n2) O(1) 稳定 快速排序 O(nlogn) O(nlogn) O(n2) O(nlogn) 不稳定 归并排序 O(nlogn) O(nlogn) O(nlogn) O(n) 稳定 基数排序 O(d(r+n)) O(d(n+rd)) O(d(r+n)) O(rd+n) 稳定 说明：希尔排序根据不同的增量序列得到的复杂度分析也不同，这里取 N//2。 InsertionSort 插入排序： // InsertionSort 插入排序 稳定 // 时间复杂度：平均 O(n^2) 最好 O(n) 最坏 O(n^2) // 空间复杂度：O(1) func InsertionSort(nums []int) { for i := 1; i 0 && nums[j-1] > temp; j-- { nums[j] = nums[j-1] /*依次与已排序序列中元素比较并右移*/ } nums[j] = temp /* 放进合适的位置 */ } } ShellSort 希尔排序： // ShellSort 希尔排序 不稳定 // 时间复杂度：平均 O(n^1.3) 最好 O(n) 最坏 O(n^2) // 空间复杂度：O(1) func ShellSort(nums []int) { for k := len(nums) / 2; k > 0; k /= 2 { for i := k; i = k && nums[j-k] > temp; j -= k { /* 注意界限 j >= k */ nums[j] = nums[j-k] /*依次与已排序序列中元素比较并右移*/ } nums[j] = temp /* 放进合适的位置 */ } } } HeapSort 堆排序： // HeapSort 堆排序 不稳定 // 时间复杂度：平均 O(nlogn) 最好 O(nlogn) 最坏 O(nlogn) // 空间复杂度：O(1) func HeapSort(nums []int) { for i := len(nums); i > 0; i-- { heapify(nums[:i]) nums[0], nums[i-1] = nums[i-1], nums[0] } } func heapify(nums []int) { last := len(nums)/2 - 1 //最后一个非叶子节点 for i := last; i >= 0; i-- { max := i left := 2*i + 1 right := 2*i + 2 if left nums[max] { max = left } if right nums[max] { max = right } nums[max], nums[i] = nums[i], nums[max] } } QuickSort 快速排序： public void quickSort(int head, int tail) { if (tail - head = pivot) { j--; } nums[i] = nums[j]; while (i 2. LRU (Least Recently Used) 最近最少使用 JDK自带实现，继承LinkedHashMap，重写removeEldestEntry方法 class LRUCache extends LinkedHashMap { //构造器等实现 @Override protected boolean removeEldestEntry(Map.Entry eldest) { return size() > capacity; } } 对应力扣题 146. LRU 缓存，采用双向链表实现 3. LFU (Least Frequently Used) 最不经常使用 对应力扣题 460. LFU 缓存，采用多条双向链表实现 class LFUCache { private final DoubleLinkedList tailList; private final Map cache; private final int capacity; private static class DoubleLinkedList { int frequency; Node head; Node tail; DoubleLinkedList moreFreq; DoubleLinkedList lessFreq; DoubleLinkedList(int frequency) { //只保留了基本的数据结构 } } private static class Node { int key; int val; Node pre; Node next; DoubleLinkedList doubleLinkedList; } } 4. KMP 算法 next[j-1]代表子串可以“跳过匹配”的字符个数，参考教程 BV1AY4y157yL 子串 A B A C A B A B next 0 0 1 0 1 2 3 2 var next = new int[pattern.length()]; for (int i = 1, j = 0; i 0 && pattern.charAt(i) != pattern.charAt(j)) { j = next[j - 1]; } if (pattern.charAt(i) == pattern.charAt(j)) { j++; } next[i] = j; } 遍历时能保证i一直增加 5. 位运算 绝对值 mask=num>>(num.bit_length()-1) # 获取符号位，正数为0，负数为-1 (num+mask)^mask # 使用异或运算去除符号位 -6^-1=5 6^0=6 (num^mask)-mask # 另一种写法 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"database.html":{"url":"database.html","title":"数据库迁移","keywords":"","body":"1. SQL Server Compact Edition SQL CE 中 sp_rename 仅支持表的修改 sp_rename 'oldTableName','newTableName'; 在 SQL Server 2005 Management Studio 中，您必须使用新名称创建一个新列，然后使用旧列中的值更新它，然后删除旧列。如果列是索引的一部分，那么最后一个操作是困难的。 SQL CE 查询表信息 SELECT table_name_, column_name FROM INFORMATION_SCHEMA.COLUMNS WHERE table_name_='stu'; SQL CE 和其他 SQL 的区别 SQL CE 3.5 和其他 SQL 的区别 SQL CE 3.5 SP2和其他 SQL 的区别 SQL CE 4.0和其他 SQL 的区别 SQL CE 使用 EFCore 连接并持久化对象： https://entityframework-extensions.net/efcore-sql-server-compact-provider 确定 Firebird SQL 版本 SELECT rdb$get_context('SYSTEM', 'ENGINE_VERSION') as version from rdb$database; 2. 不同数据库之间的类型映射关系 常用关键字翻译 T-SQL PostgreSQL cast(value AS datetime)convert(datetime, value) cast(value AS timestamp(3)) datepart(day, value) extract(day FROM value) dateadd(day, value1, value2) value2 + value1 * INTERVAL '1 day' dateadd(d,-1,dateadd(mm, datediff(m,0,column_name)+1,0))当月的最后一天 date_trunc('month', column_name) + INTERVAL '1 month- 1 day' len(value) char_length(value) + || IDENTITY (1,1) GENERATED BY DEFAULT AS IDENTITY ROWVERSION bytea CREATE TABLE table_name (column_name varbinary(46) NOT NULL DEFAULT ((0))) CREATE TABLE table_name (column_name bytea NOT NULL DEFAULT E'\\x00000000') DECLARE @tablename TABLE(column_name1 nvarchar(500), column_name2 nvarchar(500)) CREATE TEMPORARY TABLE tablename (column_name1 nvarchar(500), column_name2 nvarchar(500))WITH tablename AS (SELECT …) NOCHECK CONSTRAINT allWITH CHECK CHECK CONSTRAINT all DISABLE TRIGGER ALLENABLE TRIGGER ALL 常用类型映射 T-SQL PostgreSQL smallint smallint, int2 int integer, int, int4 bigint bigint, int8 tinyint 不支持 float(n) 1 , real float(n) 1 , real, float4 float(n) 25 double precision, float(n) 25 , float8 numeric, decimal numeric, decimal money, smallmoney moneyIn SQL Server, money is (19,4) and smallmoney is (10,4) in , but in Postgres money is (19,2) varbinary(n) bytea with checkPostgreSQL uses 'check' to simulate n varbinary(max), image bytea binary(n) 不支持 date date datetime timestamp(3) without time zone datetime2(n) timestamp(n) without time zoneIn SQL Server 0 , but in PostgreSQL 0 datetimeoffset(n) timestamp(n) with time zone, timestamptzIn SQL Server 0 , but in PostgreSQ 0 bit bit, bit(1), boolean, bool char(n) character(n), char(n) nchar(n) character(n), char(n)For UCS-2 encoding, the storage size is two times n bytes varchar(n) character varying(n), varchar(n) nvarchar(n) character varying(n), varchar(n)For UCS-2 encoding, the storage size is two times n bytes text text ntext text Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"redis/基础概念.html":{"url":"redis/基础概念.html","title":"Redis","keywords":"","body":"1. 设置远程 Redis demo demo工具网站：https://app.redislabs.com 远程登录 redis-cli -u redis://:@redis-16985.c14.us-east-1-2.ec2.cloud.redislabs.com:16985 性能测试：redis-benchmark 2. 在 MacOs 上查看 Redis 配置文件 cat /usr/local/etc/redis.conf 默认 16 个数据库，可以使用SELECT 切换数据库 DBSIZE # 查看数据库当前使用的大小 KEYS * # 查看所有的key FLUSHDB # 清空当前库 FLUSHALL # 清空全部库 MOVE # 移动到指定数据库 EXPIRE TTL # 查看剩余时间 -1表示不会过期 -2表示已过期 TYPE # 查看key类型 3. Redis 使用单线程的原因主要有以下几点： 减少锁竞争：Redis 的核心操作是基于内存的，使用单线程可以避免多线程环境下的锁竞争问题，提高性能。由于单线程模型避免了线程切换和上下文切换的开销，可以更高效地利用 CPU。 避免复杂性：多线程编程涉及到线程同步、数据一致性等复杂问题。使用单线程可以避免这些复杂性，使代码更加简单、可靠，减少潜在的错误。 避免资源竞争：Redis 的主要瓶颈是 CPU 和内存带宽，而不是计算能力。使用单线程可以充分利用 CPU 的计算能力，避免了多线程之间因为资源竞争而导致的性能下降。 高效的网络模型：Redis 使用非阻塞的 I/O 多路复用模型，单线程可以处理大量的并发连接。这种模型在处理高并发读写请求时非常高效，避免了线程间切换的开销。 需要注意的是，尽管 Redis 主线程是单线程的，但它通过使用异步 I/O 和多路复用技术实现了非阻塞的网络通信和高并发处理能力。此外，Redis 也提供了一些并发操作的指令，如使用多个 Redis 实例进行数据分片来提高整体性能。 4. String 字符串 字符串：可以用来做最简单的数据，可以缓存某个简单的字符串，也可以存某个 json 格式的字符串，Redis 分布式锁的实现就利用了这种数据结构，还包括可以实现技数器、Session 共享、分布式 ID。 SET GET APPEND 如果不存在，相当于SET EXISTS INCR DECR INCRBY DECRBY GETRANGE 包含start和end，end为-1在倒数第1的位置 SETRANGE 从offset开始替换成value，替换的范围为value的长度 SETEX 设置key-value和过期时间，不存在则创建 SETNX 成功返回 1，否则 0（在分布式锁中常常会使用） MSET MGET MSETNX 有一个存在则全部失败，原子性操作 GETSET 先get再set，不存在返回nil，同样会设置新值 5. List 列表 List of strings，实际上是个链表。Redis的列表通过命令的组合，既可以当做栈，也可以当做队列来使用，可以用来缓存类似微信公众号、微博等消息流数据。 LLEN LINSERT LPUSH 不存在则创建 RPUSH 在尾部添加值 LPOP RPOP LINDEX 找到第 index 个 key 的位置，从 0 开始，末尾从 -1 开始，越界报错 LRANGE LREM 最多移除 count 个数的 value，精确匹配 LTRIM 截取 list 部分元素 RPOPLPUSH LSET 没有 key 或者超出原有 key 的 index 范围都报错 LINSERT BEFORE|AFTER 6. Hash 哈希表 可以用来存储一些 key-value 对，更适合用来存储对象。 HGET HSET [field value ...] HMGET HGETALL HMSET HDEL HLEN HEXISTS HKEYS HVALS 7. Set 集合 和列表类似，也可以存储多个元素，但是不能重复，集合可以进行交集、并集、差集操作，从而可以实现类似，我和某人共同关注的人、朋友圈点赞等功能。 SADD SREM SCARD 获取元素总个数 cardinality 基数 SDIFF [key ...] 差集 SINTER [key ...] 交集 SUNION [key ...] 合集 SISMEMBER SMEMBERS # 显示所有元素 SPOP # 随机移除元素 SRANDMEMBER 随机count个数的获取元素 SMOVE 8. Sorted Set 有序集合 集合是无序的，有序集合可以设置顺序，可以用来实现排行榜功能。 ZADD key [NX | XX] [GT | LT] [CH] [INCR] score member [score member...] ZDIFF ZCOUNT ZRANGEBYSCORE key min max [WITHSCORES] [LIMIT offset count] ZREVRANGEBYSCORE 逆序 9. Bitmap Kind of like a collection of booleans BITOP BITCOUNT BITPOS 10. Hyperloglog Kind of like a collection of booleans PFADD PFCOUNT PFMERGE 11. JSON Nested JSON structure JSON.SET JSON.GET JSON.DEL 12. Index Internal data used for searching FT.SEARCH FT.CREATE FT.PROFILE Documentation for commands: https://redis.io/commands 13. geospatial 地理位置 底层由 Zset 实现 GEOADD key [NX | XX] [CH] longitude latitude member [longitude latitude member ...] 地球两极无法直接添加 经度纬度 GEODIST # 单位m，km，mi，ft GEOHASH GEOPOS GEORADIUS # 可以限制数量 GEORADIUSBYMEMBER 14. Hyperloglog 基数，不重复的元素个数，可以接受误差 PFADD key [element [element ...]] PFMERGE PFCOUNT 15. Bitmap 位存储 SETBIT key offset value 最大32位 BITCOUNT 统计1的位数个数 16. 缓存穿透、缓存击穿、缓存雪崩分别是什么 缓存中存放的大多都是热点数据，目的就是防止请求可以直接从缓存中获取到数据，而不用访问 MySQL。 缓存雪崩：如果缓存中某一时刻大批热点数据同时过期，那么就可能导致大量请求直接访 MySQL 了，解决办法就是在过期时间上增加一点随机值，另外如果搭建一个高可用的 Redis 集群也是防止缓存雪崩的有效手段 缓存击穿：缓存雪崩类似，缓存雪前是大批热点数据失效，而存击穿是指某一个热点 key 突然失效，也导致了大量请求直接访问 MySQL 数据库，这就是缓存击穿，解决方案就是考虑这个热点 key 不设过期时间 缓存穿透：假如某一时刻访问 Redis 的大量 key 都在 Redis 中不存在，比如黑客意造一些乱七八的 key，那么也会给数据库造成压力，这就是缓存穿透，解决方案是使用布隆过滤器，它的作用就是如果它认为一个 key 不存在，那么这个 key 就肯定不存在，所以可以在缓存之前加一层布隆过滤器来拦截不存在的 key 17. 如何降低内存穿透风险？ 布隆过滤器（Bloom Filter）是 1970 年由布隆提出的。 它实际上是一个很长的二进制向量和一系列随机映射函数。 布隆过滤器可以用于检索一个元素是否在一个集合中。 它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-01 14:54:37 "},"redis/事务及Java集成.html":{"url":"redis/事务及Java集成.html","title":"事务及 Java 集成","keywords":"","body":"1. 什么是Redis事务 Redis 事务的本质是一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。 总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。 2. Redis事务相关命令和使用 MULTI ：开启事务，redis会将后续的命令逐个放入队列中，然后使用EXEC命令来原子化执行这个命令系列。 EXEC：执行事务中的所有操作命令。 DISCARD：取消事务，放弃执行事务块中的所有命令。 WATCH：监视一个或多个key,如果事务在执行前，这个key(或多个key)被其他命令修改，则事务被中断，不会执行事务中的任何命令。 UNWATCH：取消WATCH对所有key的监视。 3. 如何理解Redis与事务的ACID？ 原子性atomicity Redis事务在编译错误时回滚，运行时错误跳过。很多文章由此说Redis事务违背原子性的；而官方文档认为是遵从原子性的。 Redis官方文档给的理解是，Redis的事务是原子性的：所有的命令，要么全部执行，要么全部不执行。而不是完全成功。 一致性consistency redis事务可以保证命令失败的情况下得以回滚，数据能恢复到没有执行之前的样子，是保证一致性的，除非redis进程意外终结。 隔离性Isolation redis事务是严格遵守隔离性的，原因是redis是单进程单线程模式(v6.0之前），可以保证命令执行过程中不会被其他客户端命令打断。 但是，Redis不像其它结构化数据库有隔离级别这种设计。 持久性Durability redis事务是不保证持久性的，这是因为redis持久化策略中不管是RDB还是AOF都是异步执行的，不保证持久性是出于对性能的考虑。 4. CAS操作实现乐观锁 check-and-set (CAS)：被 WATCH 的键会被监视，并会发觉这些键是否被改动过了。 如果有至少一个被监视的键在 EXEC 执行之前被修改了， 那么整个事务都会被取消， EXEC 返回nil-reply来表示事务已经失败。 WATCH mykey val = GET mykey val = val + 1 MULTI SET mykey $val EXEC 5. Redis事务执行步骤 通过上文命令执行，很显然Redis事务执行是三个阶段： 开启：以MULTI开始一个事务 入队：将多个命令入队到事务中，接到这些命令并不会立即执行，而是放到等待执行的事务队列里面 执行：由EXEC命令触发事务 当一个客户端切换到事务状态之后， 服务器会根据这个客户端发来的不同命令执行不同的操作： 如果客户端发送的命令为 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令的其中一个， 那么服务器立即执行这个命令。 与此相反， 如果客户端发送的命令是 EXEC 、 DISCARD 、 WATCH 、 MULTI 四个命令以外的其他命令， 那么服务器并不立即执行这个命令， 而是将这个命令放入一个事务队列里面， 然后向客户端返回 QUEUED 回复。 6. Jedis Jedis是Redis官方提供的Java客户端，用于在 Java 应用程序中连接、操作 Redis，它提供了与 Redis 通信的 API，简化了 Java 开发者与 Redis 的交互流程。 Jedis Github Readme：https://github.com/redis/jedis#getting-started 7. SpringBoot 在 SpringBoot2.x 之后，原来使用的 jedis 被替换成了 lettcuce，原因： Jedis：采用直连，多个线程操作不安全，需要使用 Jedis pool 来避免，会有线程阻塞的情况（BIO模式） Lettuce：采用 netty，实例可以在多个线程共享，不存在线程不安全的情况，可以减少线程数量（NIO模式） Lettuce Doc：https://lettuce.io/core/release/reference/#_for_gradle_users 8. Redis.conf 详解 查找 redis.conf 位置 redis-cli config get dir find / -name redis.conf Linux系统可以 systemctl status redis-server unit 单位对大小写不敏感 bind 127.0.0.1 绑定IP protected-mode yes 保护模式，docker启动需要关闭 port 6379 端口设置 daemonize yes 以守护进程的方式运行 pidfile /var/run/redis_6379.pid 如果以后台方式运行，需要指定pid进程文件 loglevel notice logfile \"\" 日志的文件位置名 database 16 默认的数据库数量 always-show-logo yes 是否总是显示logo 快照：持久化，在规定的时间内，执行了多少次操作，则会持久化到文件.rdb,.aof redis是内存数据库，如果没有持久化，那么断电即丢失！ save 900 1 #如果900秒，如果至少有一个key进行了修改，我们即进行持久化操作 save 300 10 save 60 10000 stop-writes-on-bgsave-error yes 如果持久化出错，是否还需要继续工作 rdbcompression yes 是否压缩rdb持久化文件，需要消耗cpu资源 rdbchecksum yes 保存时是否校验rdb文件 dir ./ 持久化rdb文件保存的目录 config get requirepass # 获取requirepass项配置 config set requirepass \"123456\" # 设置requirepass项配置 auth 123456 # 登录才能查看及修改以上配置内容 maxclients 10000 默认可连接的最大客户端数目 maxmemory redis配置最大内存 LRU least recently used LFU least frequently used maxmemory-policy noeviction 内存到达上限之后的处理策略： volatile-lru：只对设置了过期时间的key进行LRU算法进行删除 allkeys-lru：对所有key执行LRU算法进行删除 volatile-lfu：只对设置了过期时间的key进行LFU算法进行删除 allkeys-lfu：对所有key执行LFU算法进行删除 volatile-random：随机删除设置有过期时间的key allkeys-random：随机删除 volatile-ttl：删除即将过期的 noeviction：永不过期，返回错误 appendonly no 默认不开启aof模式，默认是使用rdb方式持久化的，大部分情况下，rdb完全够用 appendfilename \"appendonly.aof\" 持久化的文件名字 appendfsync everysec 每秒执行一次同步，always 每次修改都写入，速度较慢，no 让操作系统决定同步，速度最快 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 10:37:02 "},"redis/RDB_AOF持久化.html":{"url":"redis/RDB_AOF持久化.html","title":"RDB & AOF 持久化","keywords":"","body":"1. RDB (Redis DataBase) 在指定的时间间隔内将内存中的数据集快照写入磁盘，也就是行话讲的 Snapshot 快照，它恢复时是将快照文件直接读到内存里。 Redis 会单独创建（fork）一个进程来进行持久化，会先将数据写入到一个临时文件中，待持久化过程都结束了，再用这个临时文件替换上次持久化好的文件。整个过程中，主进程是不进行任何 IO 操作的。这就确保了极高的性能。如果需要进行大规模数据的恢复，且对于数据恢复的完整性不是非常敏感，那 RDB 方式要比 AOF 方式更加的高效。RDB 的缺点是最后一次持久化后的数据可能丢失。我们默认的就是 RDB，一般情况下不需要修改这个配置！ RDB 保存的文件是dump.rdb（在 conf 中 dbfilename 修改文件名） 在主从复制中，RDB 就是备用，在从机上面。 1.1 触发生成 rdb 文件的机制： save 的规则满足 执行FLUSHALL 退出 Redis 1.2 如何恢复 rdb 文件 ？ 只需要将 rdb 文件放在我们 Redis 启动目录就可以，Redis 启动的时候会自动检查dump.rdb 恢复其中的数据！ 查看需要存在的位置 > config get dir 1)\"dir\" 2)\"/usr/1ocal/bin\" # 如果在这个目录下存在 dump.rdb 文件，启动就会自动恢复其中的数据 1.3 优点 一旦采用该方式，那么你的整个 Redis 数据库将只包含一个文件，这对于文件备份而言是非常完美的。比如，你可能打算每个小时归档一次最近 24 小时的数据，同时还要每天归档一次最近 30 天的数据。通过这样的备份策略，一旦系统出现灾难性故障，我们可以非常容易的进行恢复。 对于灾难恢复而言，RDB 是非常不错的选择。因为我们可以非常轻松的将一个单独的文件压缩后再转移到其它存储介质上。 性能最大化。对于 Redis 的服务进程而言，在开始持久化时，它唯一需要做的只是 fork 出子进程，之后再由子进程完成这些持久化的工作，这样就可以极大的避免服务进程执行 IO 操作了。 相比于 AOF 机制，如果数据集很大，RDB 的启动效率会更高。 1.4 缺点 如果你想保证数据的高可用性，即最大限度的避免数据丢失，那么 RDB 将不是一个很好的选择。因为系统一旦在定时持久化之前出现宕机现象，此前没有来得及写入磁盘的数据都将丢失。 由于 RDB 是通过 fork 子进程来协助完成数据持久化工作的，因此，如果当数据集较大时，可能会导致整个服务器停止服务几百毫秒，甚至是 1 秒钟。 2. AOF (Append Only File) 以日志的形式来记录每个写操作，将 Redis 执行过的所有指令记录下来（读操作不记录），只许加文件但不可以改与文件，Redis 启动之初会读取该文件重新构建数据，换言之，Redis 重启的话就根据日志文件的内容将写指令从前到后执行一次以完成数据的恢复工作。 AOF 保存的是appendonly.aof文件，默认不开启。 同时开启 redis-check-aof --fix # 修复aof文件 重写机制： no-appendfsync-on-rewrite no # 由操作系统决定重写时机 auto-aof-rewrite-percentage 100 auto-aof-rewrite-min-size 64mb 2.1 优点 该机制可以带来更高的数据安全性，即数据持久性。Redis 中提供了3中同步策略，即每秒同步、每修改同步和不同步。事实上，每秒同步也是异步完成的，其效率也是非常高的，所差的是一旦系统出现宕机现象，那么这一秒钟之内修改的数据将会丢失。而每修改同步，我们可以将其视为同步持久化，即每次发生的数据变化都会被立即记录到磁盘中。可以预见，这种方式在效率上是最低的。至于无同步，无需多言，我想大家都能正确的理解它。 由于该机制对日志文件的写入操作采用的是 append 模式，因此在写入过程中即使出现宕机现象，也不会破坏日志文件中已经存在的内容。然而如果我们本次操作只是写入了一半数据就出现了系统崩溃问题，不用担心，在 Redis 下一次启动之前，我们可以通过 redis-check-aof 工具来帮助我们解决数据一致性的问题。 如果日志过大，Redis 可以自动启用 rewrite 机制。即 Redis 以 append 模式不断的将修改数据写入到老的磁盘文件中，同时 Redis 还会创建一个新的文件用于记录此期间有哪些修改命令被执行。因此在进行 rewrite 切换时可以更好的保证数据安全性。 AOF 包含一个格式清晰、易于理解的日志文件用于记录所有的修改操作。事实上，我们也可以通过该文件完成数据的重建。 2.2 缺点 对于相同数量的数据集而言，AOF 文件通常要大于 RDB 文件。RDB 在恢复大数据集时的速度比 AOF 的恢复速度要快。 根据同步策略的不同，AOF 在运行效率上往往会慢于 RDB。总之，每秒同步策略的效率是比较高的，同步禁用策略的效率和 RDB 一样高效。 二者选择的标准，就是看系统是愿意牺牲一些性能，换取更高的缓存一致性（AOF），还是愿意写操作频繁的时候，不启用备份来换取更高的性能，待手动运行 save 的时候，再做备份（RDB）。RDB 这个就更有些 eventually consistent 的意思了。 3. 扩展 RDB 持久化方式能够在指定的时间间隔内对你的数据进行快照存储 AOF 持久化方式记录每次对服务器写的操作，当服务器重启的时候会重新执行这些命令来恢复原始的数据，AOF命令以Redis 协议追加保存每次写的操作到文件末尾，Redis还能对AOF文件进行后台重写，使得AOF文件的体积不至于过大 只做缓存，如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化 同时开启两种持久化方式 在这种情况下，当 Redis 重启的时候会优先载入 AOF 文件来恢复原始的数据，因为在通常情况下 AOF 文件保存的数据集要比 RDB 文件保存的数据集要完整 RDB 的数据不实时，同时使用两者时服务器重启也只会找 AOF 文件，那要不要只使用 AOF 呢？作者建议不要，因为 RDB 更适合用于备份数据库（AOF 在不断变化不好备份），快速重启，而且不会有 AOF 可能潜在的 Bug，留着作为一个万一的手段 性能建议 因为 RDB 文件只用作后备用途，建议只在 Slave 上持久化 RDB 文件，而且只要 15 分钟备份一次就够了，只保留 save 900 1这条规则。 如果 Enable AOF，好处是在最恶劣情况下也只会丢失不超过两秒数据，启动脚本较简单只 load 自己的 AOF 文件就可以了，代价一是带来了持续的 IO，二是 AOF rewrite 的最后将 rewrite 过程中产生的新数据写到新文件造成的阻塞几乎是不可避免的。只要硬盘许可，应该尽量减少 AOF rewrite 的频率，AOF 重写的基础大小默认值 64M 太小了，可以设到 5G 以上，默认超过原大小 100% 大小重写可以改到适当的数值。 如果不 Enable AOF，仅靠 Master-Slave Replication 实现高可用性也可以，能省掉一大第10，也减少了rewrite 时带来的系统波动。代价是如果Master/Slave 同时断电，会丢失十几分钟的数据，启动脚本也要比较两个 Master/Slave 中的 RDB文件，载入较新的那个，微博就是这种架构。 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-08-21 10:11:25 "},"redis/订阅发布及主从复制.html":{"url":"redis/订阅发布及主从复制.html","title":"订阅发布及主从复制","keywords":"","body":"1. Commands SUBSCRIBE channel [channel ...] PUBLISH channel message UNSUBSCRIBE [channel [channel ...]] PSUBSCRIBE pattern [pattern ...] 正则订阅 PUBSUB subcommand [argument [argument ...]] 查看订阅与发布系统状态 PUNSUBSCRIBE [pattern [pattern ...]] 退订所有给定模式的频道 2. 原理 Redis 是使用 C 实现的，通过分析 Redis 源码里的pubsub.c文件，了解发布和订阅机制的底层实现，籍此加深对 Redis 的理解。 Redis 通过 PUBLISH、SUBSCRIBE 和 PSUBSCRIBE 等命令实现发布和订阅功能。 通过 SUBSCRIBE 命令订阅某频道后，redis-server 里维护了一个字典，字典的键就是一个个channel，而字典的值则是一个链表，链表中保存了所有订阅这个 channel 的客户端。SUBSCRIBE 命令的关键，就是将客户端添加到给定 channel 的订阅链表中。 通过 PUBLISH 命令向订阅者发送消息，redis-server 会使用给定的频道作为键，在它所维护的 channel 字典中查找记录了订阅这个频道的所有客户端的链表，遍历这个链表，将消息发布给所有订阅者。 Pub/Sub 从字面上理解就是发布（Publish）与订阅（Subscribe），在Redis中，你可以设定对某一个kev值进行消息发布及消息阅，当一个key值上进行了消息发布后，所有订阅它的客户端都会收到相应的消息。这一功能最明显的用法就是用作实时消息系统，比如普通的即时聊天，群聊等功能。 3. 主从复制的作用 数据冗余 故障恢复 负载均衡 高可用 原因： 单点故障 单台服务器内存有限，一般来说，单台最大不应该超过20G 4. 配置 master服务器不用特殊配置 info replication # 查看集群主从信息 需要配置的项： port 6379 daemonize yes # 打开后台运行 pidfile /var/run/reds_6379.pid logfile \"6379.log\" dbfilename dump6379.rdb 从机配置 从机ReadOnly无法写入，链式主从复制，中间的节点依旧属于Slave。 SLAVEOF host port # 使用命令进行配置 SLAVEOF no one # 从slave更新为master节点 replicaof # 使用配置项进行永久配置 5. 复制原理 slave启动成功连接到 master 后会发送一个sync同步命令 master 接到命令，启动后台的存盘进程，同时收集所有接收到的用于修改数据集命令，在后台进程执行完毕之后，master将传送整个数据文件到slave，并完成一次完全同步。 全量复制：而slave服务在接收到数据库文件数据后，将其存盘并加载到内存中。 增量复制:：master 继续将新的所有收集到的修改命令依次传给slave，完成同步但是只要是重新连接master，一次完全同步（全量复制）将被自动执行。 6. 哨兵模式 哨兵模式是一种特殊的模式，首先Redis提供了哨兵的命令，哨兵是一个独立的进程，作为进程，它会独立运行。其原理是哨兵通过发送命令等待Redis服务器响应从而监控运行的多个Redis实例。 多哨兵模式： 假设主服务器宕机，哨兵1先检测到这个结巢，系统并不会马上进行failover过程，仅仅是哨兵1主观的认为主服务器不可用，这个现象成为主观下线。当后面的哨兵也检测到主服务器不可用，并且数量达到一定值时，那么哨兵之间就会进行一次投票，投票的结果由一个哨兵发起，进行failover[故障转移]操作。切换成功后，就会通过发布订阅模式，让各个哨兵把自己监控的从服务器实现切换主机，这个过程称为客观下线。 conf配置： sentinel monitor # 告诉Sentinel监听指定主节点，并且只有在至少哨兵达成一致的情况下才会判断它 O_DOWN 状态 redis-sentinel kconfig/sentinel.conf # 启动sentinel 故障下线又恢复的master节点会变成slave。 全部配置项： # Example sentinel.conf # *** IMPORTANT *** # 绑定IP地址 # bind 127.0.0.1 192.168.1.1 # 保护模式（是否禁止外部链接，除绑定的ip地址外） # protected-mode no # port # 此Sentinel实例运行的端口 port 26379 # 默认情况下，Redis Sentinel不作为守护程序运行。 如果需要，可以设置为 yes。 daemonize no # 启用守护进程运行后，Redis将在/var/run/redis-sentinel.pid中写入一个pid文件 pidfile /var/run/redis-sentinel.pid # 指定日志文件名。 如果值为空，将强制Sentinel日志标准输出。守护进程下，如果使用标准输出进行日志记录，则日志将发送到/dev/null logfile \"\" # sentinel announce-ip # sentinel announce-port # # 上述两个配置指令在环境中非常有用，因为NAT可以通过非本地地址从外部访问Sentinel。 # # 当提供announce-ip时，Sentinel将在通信中声明指定的IP地址，而不是像通常那样自动检测本地地址。 # # 类似地，当提供announce-port 有效且非零时，Sentinel将宣布指定的TCP端口。 # # 这两个选项不需要一起使用，如果只提供announce-ip，Sentinel将宣告指定的IP和“port”选项指定的服务器端口。 # 如果仅提供announce-port，Sentinel将通告自动检测到的本地IP和指定端口。 # # Example: # # sentinel announce-ip 1.2.3.4 # dir # 每个长时间运行的进程都应该有一个明确定义的工作目录。对于Redis Sentinel来说，/tmp就是自己的工作目录。 dir /tmp # sentinel monitor # # 告诉Sentinel监听指定主节点，并且只有在至少哨兵达成一致的情况下才会判断它 O_DOWN 状态。 # # # 副本是自动发现的，因此您无需指定副本。 # Sentinel本身将重写此配置文件，使用其他配置选项添加副本。另请注意，当副本升级为主副本时，将重写配置文件。 # # 注意：主节点（master）名称不能包含特殊字符或空格。 # 有效字符可以是 A-z 0-9 和这三个字符 \".-_\". sentinel monitor mymaster 127.0.0.1 6379 2 # 如果redis配置了密码，那这里必须配置认证，否则不能自动切换 # Example: # # sentinel auth-pass mymaster MySUPER--secret-0123passw0rd # sentinel down-after-milliseconds # # 主节点或副本在指定时间内没有回复PING，便认为该节点为主观下线 S_DOWN 状态。 # # 默认是30秒 sentinel down-after-milliseconds mymaster 30000 # sentinel parallel-syncs # # 在故障转移期间，多少个副本节点进行数据同步 sentinel parallel-syncs mymaster 1 # sentinel failover-timeout # # 指定故障转移超时（以毫秒为单位）。 它以多种方式使用： # # - 在先前的故障转移之后重新启动故障转移所需的时间已由给定的Sentinel针对同一主服务器尝试，是故障转移超时的两倍。 # # - 当一个slave从一个错误的master那里同步数据开始计算时间。直到slave被纠正为向正确的master那里同步数据时。 # # - 取消已在进行但未生成任何配置更改的故障转移所需的时间 # # - 当进行failover时，配置所有slaves指向新的master所需的最大时间。 # 即使过了这个超时，slaves依然会被正确配置为指向master。 # # 默认3分钟 sentinel failover-timeout mymaster 180000 # 脚本执行 # # sentinel notification-script和sentinel reconfig-script用于配置调用的脚本，以通知系统管理员或在故障转移后重新配置客户端。 # 脚本使用以下规则执行以进行错误处理： # # 如果脚本以“1”退出，则稍后重试执行（最多重试次数为当前设置的10次）。 # # 如果脚本以“2”（或更高的值）退出，则不会重试执行。 # # 如果脚本因为收到信号而终止，则行为与退出代码1相同。 # # 脚本的最长运行时间为60秒。 达到此限制后，脚本将以SIGKILL终止，并重试执行。 # 通知脚本 # # sentinel notification-script # # 为警告级别生成的任何Sentinel事件调用指定的通知脚本（例如-sdown，-odown等）。 # 此脚本应通过电子邮件，SMS或任何其他消息传递系统通知系统管理员 监控的Redis系统出了问题。 # # 使用两个参数调用脚本：第一个是事件类型，第二个是事件描述。 # # 该脚本必须存在且可执行，以便在提供此选项时启动sentinel。 # # 举例: # # sentinel notification-script mymaster /var/redis/notify.sh # 客户重新配置脚本 # # sentinel client-reconfig-script # # 当主服务器因故障转移而变更时，可以调用脚本执行特定于应用程序的任务，以通知客户端，配置已更改且主服务器地址已经变更。 # # 以下参数将传递给脚本： # # # # 目前始终是故障转移 \"failover\" # 是 \"leader\" 或 \"observer\" # # 参数 from-ip, from-port, to-ip, to-port 用于传递主服务器的旧地址和所选副本的新地址。 # # 举例: # # sentinel client-reconfig-script mymaster /var/redis/reconfig.sh # 安全 # 避免脚本重置，默认值yes # 默认情况下，SENTINEL SET将无法在运行时更改notification-script和client-reconfig-script。 # 这避免了一个简单的安全问题，客户端可以将脚本设置为任何内容并触发故障转移以便执行程序。 sentinel deny-scripts-reconfig yes # REDIS命令重命名 # # # 在这种情况下，可以告诉Sentinel使用不同的命令名称而不是正常的命令名称。 # 例如，如果主“mymaster”和相关副本的“CONFIG”全部重命名为“GUESSME”，我可以使用： # # SENTINEL rename-command mymaster CONFIG GUESSME # # 设置此类配置后，每次Sentinel使用CONFIG时，它将使用GUESSME。 请注意，实际上不需要尊重命令案例，因此在上面的示例中写“config guessme”是相同的。 # # SENTINEL SET也可用于在运行时执行此配置。 # # 为了将命令设置回其原始名称（撤消重命名），可以将命令重命名为它自身： # # SENTINEL rename-command mymaster CONFIG CONFIGss 7. 通过Redis实现一把安全可靠的分布式锁 SET 'resource_name' 'my_random_value' NX PX 30000 resource_name：锁的key my_random_value：键值应该为随机数，便于唯一指定地释放，也可以防止超时后误释放别人的锁 NX：表示只有当key不存在的时候才能设置成功 PX：表示该键值信息会在指定时间之后自动删除 但是很明显，单个Redis实例存在单点故障问题，即一旦Redis发生故障，就会导致整个分布式服务不可用 这个时候你可能会想为该Redis添加Slave节点，以便实现故障转移，但是这里有一个陷阱，虽然添加从节点后，该实例会成为Master，其会进行主从复制，但是这个过程是异步的，也就是说主节点的锁信息还未能同步到从节点便发生故障后，重新选举出来的主节点不一定有锁信息，就会导致其他服务重新获得锁，出现多个服务获得同一把锁的情况 采用Redis的集群部署方案，即有多个Master节点，采用RedLock算法 RedLock算法：向N个Redis Master循环发送锁请求信息，使用单实例命令，并设置超时时间 SET 'LOCK' '1001' NX EX 3000 获取成功的条件： 半数以上节点返回Success N次加锁请求时间小于超时时间 若超时失败，则会向所有Master节点发送释放锁的命令，并随机延迟时间后再去请求获得锁 参考文档：https://martin.kleppmann.com/2016/02/08/how-to-do-distributed-locking.html 8. 缓存一致性协议 内存使用DRAM的动态随机存储器，缓存使用SRAM的静态随机存储器 MESI: Modified, Exclusive, Shared, Invalid Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-08-18 11:01:18 "},"redis/Raft算法.html":{"url":"redis/Raft算法.html","title":"Raft 算法","keywords":"","body":"1. 什么是 Raft 相比于 Paxos，Raft 最大的特性就是易于理解，为了达到这个目标 Raft 主要做了两方面的事情： 问题分解：把共识算法分为三个子问题，分别是 领导者选举（leader election） 日志复制（log replication） 安全性（safety） 状态简化：对算法做出一些限制，减少状态数量和可能产生的变动 复制状态机的概念：相同的初始状态 + 相同的输入 = 相同的结束状态 状态简化： 只有 leader、follower 或 candidate 三个状态之一。另外可以通过查看一台服务器是否具有某任期内的日志，来判断它是否在这期间出现过宕机。 Raft 算法中服务器节点之间使用 RPC 进行通信，并且 Raft 中只有两种主要的 RPC： RequestVote RPC（请求投票）：由 candidate 在选举期间发起 AppendEntries RPC（追加条目）：由 leader 发起，用来复制日志和提供一种心跳机制 2. 领导者选举 开始一个选举过程后，follower 先增加自己的当前任期号，并转换到 candidate 状态。然后投票给自己，并且并行地向集群中的其他服务器节点发送投票请求（RequestVote RPC）。 最终会有三种结果： 它获得超过半数选票赢得了选举 -> 成为主并开始发送心跳 其他节点赢得了选举 -> 收到新 leader 的心跳后，如果新 leader 的任期号不小于自己当前的任期号，那么就从 candidate 回到 follower 状态 一段时间之后没有任何获胜者 -> 每个 candidate 都在一个自己的随机选举超时时间后增加任期号开始新一轮投票 //请求投票RPC Request type RequestVoteRequest struct { term int //自己当前的任期号 candidateld int //自己的ID lastLogindex int //自已最后一个日志号 lastLogTerm int //自己最后一个日志的任期 } // 请求投票RPC Response type RequestVoteResponse struct { term int //自己当前任期号 voteGranted bool // 自己会不会投票给这个candidate } 第一个认识到集群中没有 leader 的节点会把自己变成 candidate，对于没有成为 candidate 的 follower 节点，对于同一个任期，会按照先来先得的原则投出自己的选票。 3. 日志复制 客户端怎么知道新 leader 是哪个节点呢？客户端随机向一个节点发送请求： 节点正好是 leader 节点是 follower，可以通过心跳得知 leader 的 ID 节点正好宕机，客户端只能再去找另一个节点，重复上述过程 日志需要具有的三个信息： 状态机指令，比如赋值操作 leader 的任期号 日志索引 Leader 并行发送 AppendEntries RPC 给 follower，让它们复制该条目。当该条目被超过半数的 follower 复制后，leader 就可以在本地执行该指令并把结果返回客户端。我们把本地执行指令，也就是leader应用日志与状态机这一步，称作提交。 如何保证所有节点的日志都是完整且顺序一致的呢？ follower 缓慢：如果有 follower 因为某些原因没有给 leader 响应，那么 leader 会不断地重发追加条目请求 （AppendEntries RPC），哪怕 leader 已经回复了客户端。 follower 宕机：如果有 follower 崩溃后恢复，这时 Raft 追加条目的一致性检查生效，保证follower能按顺序恢复崩溃后的缺失的日志。 Raft 的一致性检查：leader 在每一个发往 follower 的追加条目 RPC 中，会放入前一个日志条目的索引位置和任期号，如果 follower 在它的日志中找不到前一个日志，那么它就会拒绝此日志，leader 收到 follower 的拒绝后，会发送前一个日志条目，从而逐渐向前定位到 follower 第一个缺失的日志。（Raft 设计者认为这种优化是没有必要的，因为失败不经常发生并且也不可能有很多不致的日志条目） leader 宕机：如果 leader 崩溃，那么崩溃的 leader 可能已经复制了日志到部分 follower 但还没有提交，而被选出的新 leader 又可能不具备这些日志这样就有部分 follower 中的日志和新 leader 的日志不相同。 Raft 在这种情况下，leader 通过强制 follower 复制它的日志来解决不一致的问题，这意味着 follower 中跟leader 冲突的日志条目会被新 leader 的日志条目覆盖（因为没有提交，所以不违背外部一致性）。 这样的日志复制机制，就可以保证一致性特性： 只要过半的服务器能正常运行，Raft 就能够接受、复制并应用新的日志条目； 在正常情况下，新的日志条目可以在一个 RPC 来回中被复制给集群中的过半机器； 单个运行慢的 follower 不会影响整体的性能 如果 leaderCommit > commitlndex，那么把 commitlndex 设为 min(leaderCommit, index of last new entry) //追加日志RPC Request type AppendEntriesRequest struct { term int //自己当前的任期号 leaderld int //leader(也就是自己)的ID prevLogindex int //前一个日志的日志号 prevLogTerm int //前一个日志的任期号 entries []byte //当前日志内容 leaderCommit int //leader的已提交日志号 } //追加日志RPC Response type AppendEntriesResponse struct { term int //自己当前任期号 success bool //如果follower包括前一个日志，则返回true } prevLoglndex 和 prevLogTerm 是来进行一致性检查的，只有这两个都与 follower 中的相同，follower 才会认为日志是一致的。 4. 安全性 领导者选举和日志复制两个子问题实际上已经涵盖了共识算法的全程，但这两点还不能完全保证每一个状态机会按照相同的顺序（有序且无空洞）执行相同的命令。所以 Raft 通过几个补充规则完善整个算法，使算法可以在各类宕机问题下都不出错这些规则包括（不讨论安全性条件的证明过程）： Leader 宕机处理：选举限制，保证被选出来的 leader 一定包含了之前各任期的所有被提交的日志条目。 RequestVote RPC 执行了这样的限制: RPC中包含了 candidate 的日志信息，如果投票者自己的日志比 candidate 的还新（比较日志号和任期号），它会拒绝掉该投票请求 Leader 宏机处理：新 leader 是否提交之前任期内的日志条目 一旦当前任期内的某日志条目已经存储到过半的服务器节点上，leader 就知道该日志条目可以被提交了。 follower 要等下一个 AppendEntries RPC（心跳 or 新日志）才能触发提交。这就是单点提交和集群提交的区别。 如果 leader 提交之后直接返回客户端，在通知 follower 提交之前，也就是一个心跳的时间之内，如果宕机了，就可能出现返回 client 成功，但是事务提交状态却没有在集群中保留下来。我的看法是，raft 是一种底层的共识算法，本身只是应用实现高可用的一种方法。而与客户端交互本来应该是属于应用端的事情，理论上不是 raft 该担心的，通常来讲要避免这个问题，应用会设置一个集群提交的概念，只有集群中超过半数的节点都完成提交，才认为集群提交完成。（因为 raft 的 leader 可以通过 AppendEntries RPC 返回的 success 与否，判定这个 follower 是否完成提交，所以 leader 可以很容易判断一个日志是否符合集群提交的条件）实际上 leader 单点提交后就返回客户端，已经是安全的了，并没有等待集群提交的必要大家可以自行思考，或者等我解读 Percolator 时再深入讨论这一点。 如果某个 leader 在提交某个日志条目之前崩溃了，以后的 leader 会试图完成该日志条目的复制。复制，而非提交，不能通过心跳提交老日志。 Raft 永远不会通过计算副本数目的方式来提交之前任期内的日志条目。 只有 leader 当前任期内的日志条目才通过计算副本数目的方式来提交旦当前任期的某个日志条目以这种方式被提交，那么由于日志匹配特性，之前的所有日志条目也都会被间接地提交。 官方动画：raft scope Follower 和 Candidate 宕机处理 如果 follower 或 candidate 崩溃了，那么后续发送给他们的 RequestVote 和 AppendEntriesRPCs 都会失败。 Raft 通过无限的重试来处理这种失败。如果崩溃的机器重启了，那么这些RPC就会成功地完成。 如果一个服务器在完成了一个 RPC，但是是幂等的)会再次收到同样的请求。（Raft 的 RPC 都是幂等的） 时间与可用性限制 raft 算法整体不依赖客观时间，也就是说，哪怕因为网络或其他因素，造成后发的RPC先到，也不会影响raft的正确性。（这点和 Spanner 不同） 只要整个系统满足下面的时间要求，Raft 就可以选举出并维持一个稳定的 leader 广播时间（broadcastTime） 广播时间和平均故障时间是由系统决定的，但是选举超时时间是我们自己选择的。Raft 的 RPC 需要接受并将信息落盘，所以广播时间大约是0.5ms到20ms，取决于存储的技术。因此，选举超时时间可能需要在10ms到500ms之间。大多数服务器的平均故障间隔时间都在几个月甚至更长。 5. 集群成员变更 所以配置采用了一种两阶段的方法 集群先切换到一个过渡的配置，称之为联合一致（joint consensus） 第一阶段，leader 发起 Cold,new，使整个集群进入联合一致状态。这时，所有 RPC 都要在新旧两个配置中都达到大多数才算成功。 第二阶段，leader 发起 Cnew，使整个集群进入新配置状态这时，所有 RPC 只要在新配置下能达到大多数就算成功。 待更新 https://www.bilibili.com/video/BV11u411y7q9/ Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-10-31 11:20:13 "},"kafka/介绍和简单部署.html":{"url":"kafka/介绍和简单部署.html","title":"Kafka","keywords":"","body":"1. Why Apache Kafka 由Linkedln创建，现在作为开源项目主要由Confluent、IBM、Cloudera维护 分布式、具有弹性体系结构，并且具有容错能力 水平可扩展性 可扩展到几百个broker 可扩展到每秒百万条信息吞吐量 高性能（低于10ms的延迟）几乎实时 超过2000家公司使用kafka，财富排行100强中有80%的公司使用kafka：airbnb、NETFLIX、Linkedin、UBER、Walmart... 2. Apache Kafka：使用场景 信息系统 活动跟踪系统 从不同位置收集指标数据 收集应用程序日志 流处理（以Kafka Streams API为例） 解耦系统依赖和微服务 集成Spark、Flink、Storm、Hadoop等大数据技术 微服务pub/sub 3. 案例 Netflix使用Apache Kafka在你看电视节目的时候实时应用推荐 Uber使用kafka实时收集用户打车和出行数据，并计算和预测需求，还实时计算你的定价 LinkedIn使用kafka来防止垃圾邮件，收集用户交互，以便实时提供更好的连接建议 4. 本地伪分布式环境搭建 下载及安装： kafka_2.11-1.0.0.tar 2.11表示scala的版本 解压：tar -xzvf kafka_2.11-1.0.0.tar 配置： cp config/server.properties etc/server-0.properties cp config/server.properties etc/server-1.properties cp config/server.properties etc/server-2.properties vi 操作更改配置文件 broker.id=0 #修改broker id #listeners=PLAINTEXT://:9092 #取消注释 log.dirs=/tmp/kafka-logs-0 #区分log listeners：指定broker启动时本机的监听名称、端口，给服务器端使用 默认名称（协议） PLAINTEXT://:9092 PLAINTEXT PLAINTEXT://192.168.1.10:9092 SSL PLAINTEXT://hostname:9092 SASL_PLAINTEXT PLAINTEXT://0.0.0.0:9092 SASL_SSL advertised.listeners：对外发布的访问IP和端口，注册到zookeeper中，给客户端（client）使用 如果 advertised.listeners 没有配置，默认采用 listeners 的配置 外部网络需要访问时，advertised.listeners 需要配置成公网IP ## 实践 listeners=INTERNAL://:9092,EXTERNAL://0.0.0.0:9093 advertised.listeners=INTERNAL://kafka-0:9092,EXTERNAL://公网IP:9093 listener.security.protocol.map=INTERNAL:PLAINTEXT,EXTERNAL:PLAINTEXT inter.broker.listener.name=INTERNAL 启动： 启动zookeeper ./bin/zookeeper-server-start.sh ./etc/zookeeper.properties 不同terminal窗口启动kafka实例 ./bin/kafka-server-start.sh ./etc/server-0.properties 主题创建 ./bin/kafka-topics.sh --zookeeper localhost:2181 --create --topic topic_name --partitions 3 --replication-factor 2 查看主题 > ./bin/kafka-topics.sh --zookeeper localhost:2181 --describe --topic topic_name Topic: topic_name Partition: 0 Leader: 1 Replicas: 1,2 Isr: 1,2 Topic: topic_name Partition: 1 Leader: 2 Replicas: 2,0 Isr: 2,0 Topic: topic_name Partition: 2 Leader: 0 Replicas: 0,1 Isr: 0,1 Replicas: 1,2表示有2个副本，在broker id: 1和broker id: 2维护，Isr表示同步正常的 创建消费者 0.10版本之后，消费者的偏移量保存在kafka主题，不再在zookeeper上保存 ./bin/kafka-console-consumer.sh --bootstrap-server localhost:9092,localhost:9093,localhost:9094 --topic topic_name 从头消费可以加--from-beginning 创建Producer ./bin/kafka-consle-producer.sh --broker-list localhost:9092,localhost:9093,localhost:9094 --topic topic_name Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"kafka/基本概念.html":{"url":"kafka/基本概念.html","title":"基本概念","keywords":"","body":"1. Kafka Topics Topics：一种特殊的数据流 就像数据库中的表，但没有所有的约束 可以有任意多的 Topics 一个 Topic 由它的 name 定义 任意格式的消息格式 Topic 中的消息序列称为 data stream 你无法像数据库一样查询 Topics 2. Partitions and offsets Topics 被划分为 Partitions 每个分区中的消息会被排序 每个分区中的消息会有一个递增的 id，即 offset Kafka topics是不可变的，一旦数据写入到分区就不可修改 数据只保留有限时间（默认是一周，可配置） 即使前面的数据被删除，offset 也不会被复用 消息的顺序只在一个分区内得到保证 当数据（Record消息记录，包含key和value）被发送到kafka主题时，但key为空时，会以轮询的方式写入不同的分区，key不为空时，相同key的消息会被写入到同一个分区 3. Leader, Follower, and In-Sync Replica (ISR) List ISR 集合包含了当前可用的、与 Leader 分区保持同步的副本（Replica） ISR 集合的信息保存在每个 Broker 的日志目录中的元数据文件中，文件名为 isr-[partition-id]。该元数据文件包含了每个分区的 ISR 集合及其相关的信息，比如 Leader 副本、副本列表、最后一次同步的位置等。 如果某个副本不能正常同步数据或者落后的比较多，那么kafka会从ISR中将这个节点移除，直到它追赶上来之后再重新加入 4. Producers 生产者向主题分区写入数据 生产者事先知道写入到哪个分区，哪个kafka代理拥有它 5. send()异步发送 缓冲区会为主题的每个分区创建一个大小用来存放消息，生产者首先将消息放入到对应分区的缓冲区中，当他放入消息后立刻返回，不等消息是否发送给服务端，也不管它是否成功发送消息，后台IO线程负责将消息发送给broker。 6. 同步发送 Future result = producer.send(new ProducerRecord(\"topic_name\", \"\" + (i%5), Integer.toString(i))); try{ RecordMetadata recordMetadata = result.get(); //阻塞 } catch (InterruptedException e) { e.printStackTrace(); } catch (ExecutionException e) { e.printStackTrace(); } 7. 批量发送 linger.ms 每一批消息最大大小 batch.size 延迟时间 满足任意一项即可 8. acks acks=0 生产者不会等待服务器端的任何请求，当消息被放到缓冲区的时候，我们就认为他成功发送了，可能会造成数据丢失 acks=1 消息已经被服务器端的leader存入到本地了，leader未同步到follower就宕机会导致数据丢失 acks=all 至多一次 acks=0或1 至少一次 acks=-1或all, retries>0 9. Producers: Message keys 生产者可以发送带有key (string, number, binary, etc...) 的消息 如果key=null，数据将循环发送到各分区 如果key!=null，那么数据将一直发送到相同分区（哪个分区由生产者决定） 如果你需要根据指定字段对消息进行排序，那么就需要发送key 10. Kafka 消息剖析 key、value（可以为空） 压缩格式：none，gzip，snappy，lz4，zstd Headers：可选的键值对列表 消息要发送到的分区及其偏移量 时间戳（由系统或者用户设置） 11. Kafka 消息 key 哈希算法 targetPartition = Math.abs(Utils.murmur2(keyBytes))%(numPartitions-1) 12. Consumers 消费者根据 name 从某个 topic 读取数据，是 pull model，不是kafka server把数据推送给消费者 消费者自动知道从哪个broker读取数据 如果broker失效了，消费者知道如何恢复 在每个分区中，数据根据 offset 从低到高被读取 13.生产者：精确一次 enable.idempotence=true ##retries=Integer.MAX_VALUE acts=all 14. 消费者：精确一次 通过offset来防止重复消费不是一个好的办法 通常在消息中加入唯一ID (例如流水ID，订单D)，在处理业务时，通过判断 ID来防止重复处理 15. 事务 在kafka中，消息会尽可能地发送到服务端，如果提交成功了，消息后面会有成功提交的标志，如果未成功提交，那么它的状态是未提交的。 Isolation_level 隔离级别，默认为 read_uncommitted 脏读，如果只读取成功提交的数据，可以设置为 read_committed 16. Consumer Groups 在一个应用中的所有消费者作为消费者组读取数据 组内的每个消费者从独立的分区读取 如果消费者多于分区，那么一些消费者会处于非活动状态（作为备用的消费者） 分区是最小的并行单位 一个消费者可以消费多个分区 一个分区可以被多个消费者组里的消费者消费 但是，一个分区不能同时被同一个消费者组里的多个消费者消费 16.1 发布 - 订阅模式 每条消息需要被每个消费者都进行消费：每个消费者都属于不同的消费者组 16.2 点对点（一对一） 一条消息只要有被消费就可以：所有消费者都属于同一个消费者组 17. Consumer Offsets kafka 保存消费者组的 offsets 提交的 offsets 在 kafka topic 中被称为 __consumer_offsets 当组内的一个消费者处理完从 kafka 收到的数据后，它会阶段性地提交 offsets （kafka 代理会写入到__consumer_offsets，而不是消费者组自身） 如果一个消费者崩溃，重启后能根据提交的消费者偏移量重新开始读取数据 18. 消费者交付语义 Java 消费者默认会自动提交偏移量（至少一次） 手动提交有3种语义 至少一次（推荐） 消息被处理后提交偏移量 如果处理失败，消息会再被读取 这意味着，我们可以对消息进行重复处理，因此，我们需要确保我们的处理是幂等的（指再次处理不会影响我们的系统） 最多一次 消息收到就提交偏移量 如果处理失败，消息就会丢失（当然也不会再次被读取） 正好一次 对于 kafka => kafka workflows：使用 Transactional API 对于 kafka => 外部系统 workflows：使用幂等消费者 19. Kafka Brokers 一个kafka集群由多个 brokers（servers）组成 每个代理由ID（整数）标识 每个代理只包含特定的主题分区 连接到任何kafka代理（也称为引导代理后），客户端、生产者或使用者将连接到整个kafka集群 最好是从3个代理开始，但在有些大型集群中会有超过100个代理 代理的编号可以选择从任意数开始 20. Kafka 代理机制 每个 kafka 代理也被称为“引导服务器” 每个代理知道所有的代理、主题和分区（元数据） 21. 主题复制因子 主题应有一个复制因子>1（通常在2与3之间） 因此，万一有一个代理挂了，其他代理仍可以提供服务 22. 分区领导者的概念 在任何时刻，一个分区只会有一个代理作为领导者 生产者只会把数据发送给作为分区领导者的代理 其他的代理会从分区领导者复制数据 因此，每个分区拥有一个领导者和多个ISR（in-sync replica 同步副本） 23. 生产者、消费者和领导者之间的默认行为 kafka 生产者只会向分区的领导者代理写入数据 kafka 消费者默认会从分区的领导者读取数据 24. afka 消费者副本获取（Kafka v2.4+） 自从 Kafka 2.4，可以配置让消费者从最近的副本进行读取，这可以降低延迟，如果是在云上，则可以降低网络成本 25. 生产者的确认机制 acks 生产者可以选择是否接受写入数据的确认消息： acks=0：生产者不会等待确认，如果代理崩溃，可能会导致数据丢失 acks=1：生产者会等待领导者的确认，可能会导致有限的数据丢失 acks=all：要求领导者和所有副本的确认，不会有数据丢失 26. Zookeeper Zookeeper管理代理，保留一份代理的名单 Zookeeper帮助完成分区的领导者选举 当kafka有更改时，Zookeeper会发送通知，比如新的主题、代理崩溃、代理启动、删除主题等等 Kafka 2.x 版本运行必需要有 Zookeeper Kafka 3.x 可以使用 Raft (KIP-500) 作为代替 Kafka 4.x 没有 Zookeeper Zookeeper 以单数个数运行，1、3、5、7...通常不会超过7个 Zookeeper 拥有一个领导者作为写入，其他的作为追随者进行读取 v0.10 版本之后，Zookeeper不再存储消费者的偏移量 27. 你应该使用 Zookeeper 吗？ 和 Kafka 代理？ 是的，除非4.0版本发布并可用于生产环境 和 Kafka 客户端？ 随着时间的推移，Kafka客户端和CLI已经被迁移，以利用代理作为唯一的连接端点，而不是Zookeeper 自从 0.10 版本之后，消费者将偏移量储存在 Kafka 中，不再连接到 Zookeeper 从 2.2 版本之后，CLI命令 kafka-topics.sh 用 Kafka 代理而不是 Zookeeper来进行主题管理，Zookeeper CLI命令已被废弃 所有使用 Zookeeper 的API和命令会被迁移，这样新版本的集群可以不再绑定 Zookeeper，这些操作对于客户端是不可见的 Zookeeper 的安全性比 Kafka 低，这意味着如果你应该用 Zookeeper 只接受来自代理的连接，拒绝客户端的连接 28. 关于 Kafka KRaft 在2020年，Apache Kafka项目做开始着手移除 Zookeeper 依赖（KIP-500） 当Kafka集群拥有超过10万个分区时，Zookeeper 有扩展问题 删除 Zookeeper 之后，Apache Kafka 可以 扩展到百万级分区，变得更容易维护和设置 提升稳定性，更易监控、支持和管理 为整个系统提供单一的安全模型 启动 Kafka 也会容易很多 更快的关闭和恢复时间 Kafka 3.x 实现了 Raft 协议，以替代 Zookeeper（Not production ready） 29. Kafka KRaft Architecture Kafka Raft Readme Kafka performance improvement Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"kafka/Kafka_Topics_CLI.html":{"url":"kafka/Kafka_Topics_CLI.html","title":"Kafka Topics & CLI","keywords":"","body":"1. Kafka CLI: kafka-topics.sh # Replace \"kafka-topics.sh\" # by \"kafka-topics\" or \"kafka-topics.bat\" based on your system # (or bin/kafka-topics.sh or bin\\windows\\kafka-topics.bat if you didn't setup PATH / Environment variables) kafka-topics.sh kafka-topics.sh --bootstrap-server localhost:9092 --list kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --create kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --create --partitions 3 kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --create --partitions 3 --replication-factor 2 # Create a topic (working) kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --create --partitions 3 --replication-factor 1 # List topics kafka-topics.sh --bootstrap-server localhost:9092 --list # Describe a topic kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --describe # Delete a topic kafka-topics.sh --bootstrap-server localhost:9092 --topic first_topic --delete # (only works if delete.topic.enable=true) 2. Kafka CLI: kafka-console-producer.sh # Replace \"kafka-console-producer.sh\" # by \"kafka-console-producer\" or \"kafka-console-producer.bat\" based on your system # (or bin/kafka-console-producer.sh or bin\\windows\\kafka-console-producer.bat if you didn't setup PATH / Environment variables) kafka-console-producer.sh # producing kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first_topic > Hello World >My name is Conduktor >I love Kafka >^C ( some message that is acked > just for fun > fun learning! # producing to a non existing topic # 会自动帮你创建一个新的主题，默认partition=1 kafka-console-producer.sh --bootstrap-server localhost:9092 --topic new_topic > hello world! # our new topic only has 1 partition kafka-topics.sh --bootstrap-server localhost:9092 --list kafka-topics.sh --bootstrap-server localhost:9092 --topic new_topic --describe # edit config/server.properties or config/kraft/server.properties # num.partitions=3 # produce against a non existing topic again kafka-console-producer.sh --bootstrap-server localhost:9092 --topic new_topic_2 hello again! # this time our topic has 3 partitions kafka-topics.sh --bootstrap-server localhost:9092 --list kafka-topics.sh --bootstrap-server localhost:9092 --topic new_topic_2 --describe # overall, please create topics before producing to them! # produce with keys kafka-console-producer --bootstrap-server localhost:9092 --topic first_topic --property parse.key=true --property key.separator=: >example key:example value >name:Stephane 3. Kafka CLI: kafka-console-consumer.sh # Replace \"kafka-console-consumer.sh\" # by \"kafka-console-consumer\" or \"kafka-console-consumer.bat\" based on your system # (or bin/kafka-console-consumer.sh or bin\\windows\\kafka-console-consumer.bat if you didn't setup PATH / Environment variables) kafka-console-consumer.sh # consuming kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic # other terminal kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first_topic # consuming from beginning # 默认不是从头开始读，是因为消息可能很多，比如累积了一周的消息 kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --from-beginning # display key, values and timestamp in consumer kafka-console-consumer --bootstrap-server localhost:9092 --topic first_topic --formatter kafka.tools.DefaultMessageFormatter --property print.timestamp=true --property print.key=true --property print.value=true --from-beginning 4. CLI Consumer in Groups with kafka-console-consumer.sh # Replace \"kafka-console-consumer.sh\" # by \"kafka-console-consumer\" or \"kafka-console-consumer.bat\" based on your system # (or bin/kafka-console-consumer.sh or bin\\windows\\kafka-console-consumer.bat if you didn't setup PATH / Environment variables) # start one consumer kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application # start one producer and start producing kafka-console-producer.sh --bootstrap-server localhost:9092 --topic first_topic # start another consumer part of the same group. See messages being spread kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application # start another consumer part of a different group from beginning kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-second-application --from-beginning 5. Consumer Group Management CLI kafka-consumer-groups.sh # Replace \"kafka-consumer-groups.sh\" # by \"kafka-consumer-groups\" or \"kafka-consumer-groups.bat\" based on your system # (or bin/kafka-consumer-groups.sh or bin\\windows\\kafka-consumer-groups.bat if you didn't setup PATH / Environment variables) # documentation for the command kafka-consumer-groups.sh # list consumer groups kafka-consumer-groups.sh --bootstrap-server localhost:9092 --list # describe one specific group kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-second-application # describe another group kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-first-application # start a consumer # 启动consumer时，若不指定group，则会创建一个临时的group，在consumer关闭后删除，并且不会提交offset kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application # describe the group now kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-first-application # describe a console consumer group (change the end number) kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group console-consumer-10592 # start a console consumer kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application # describe the group again kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-first-application 6. Consumer Groups -Reset Offsets kafka-consumer-groups.sh # Replace \"kafka-consumer-groups\" # by \"kafka-consumer-groups.sh\" or \"kafka-consumer-groups.bat\" based on your system # (or bin/kafka-consumer-groups.sh or bin\\windows\\kafka-consumer-groups.bat if you didn't setup PATH / Environment variables) # look at the documentation again kafka-consumer-groups # reset the offsets to the beginning of each partition # 停止运行consumer才可以reset kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest # execute flag is needed kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute # topic flag is also needed kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --to-earliest --execute --topic first_topic # consume from where the offsets have been reset kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application # describe the group again kafka-consumer-groups.sh --bootstrap-server localhost:9092 --describe --group my-first-application # documentation for more options kafka-consumer-groups.sh # shift offsets by 2 (forward) as another strategy kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --shift-by 2 --execute --topic first_topic # shift offsets by 2 (backward) as another strategy kafka-consumer-groups.sh --bootstrap-server localhost:9092 --group my-first-application --reset-offsets --shift-by -2 --execute --topic first_topic # consume again kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic first_topic --group my-first-application Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-08-18 11:01:18 "},"kafka/Java_Integration.html":{"url":"kafka/Java_Integration.html","title":"Java Integration","keywords":"","body":"1. Java Producer public class ProducerDemo { private static final Logger log = LoggerFactory.getLogger(ProducerDemo.class.getSimpleName()); public static void main(String[] args) { log.info(\"I am a Kafka Producer\"); // create Producer Properties Properties properties = new Properties(); properties.setProperty(ProducerConfig.BOOTSTRAP_SERVERS_CONFIG, \"127.0.0.1:9092\"); properties.setProperty(ProducerConfig.KEY_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); properties.setProperty(ProducerConfig.VALUE_SERIALIZER_CLASS_CONFIG, StringSerializer.class.getName()); // create the Producer KafkaProducer producer = new KafkaProducer<>(properties); // create a producer record ProducerRecord producerRecord = new ProducerRecord<>(\"demo_java\", \"hello world\"); // send the data - asynchronous producer.send(producerRecord); // flush data - synchronous producer.flush(); // flush and close producer producer.close(); } } 2. Java Producer Callbacks producer.send(producerRecord, new Callback() { @Override public void onCompletion(RecordMetadata metadata, Exception e) { // executes every time a record is successfully sent or an exception is thrown if (e == null){ // the record was successfully sent log.info(\"Received new metadata. \\n\" + \"Topic: \" + metadata.topic() + \"\\n\" + \"Partition: \" + metadata.partition() + \"\\n\" + \"Offset: \" + metadata.offset() + \"\\n\" + \"Timestamp: \" + metadata.timestamp()); } else { log.error(\"Error while producing\", e); } } }); 粘性分区：如果发送速度足够快，几条消息可能作为同一批发送到同一分区 partitioner.class = class org.apache.kafka.clients.producer.internals.DefaultPartitioner DefaultPartitioner 初始化时默认采用 StickyPartitionCache 3. Java Consumer String boostrapServers = \"127.0.0.1:9092\"; String groupId = \"my-second-application\"; String topic = \"demo_java\"; // create consumer configs Properties properties = new Properties(); properties.setProperty(ConsumerConfig.BOOTSTRAP_SERVERS_CONFIG, boostrapServers); properties.setProperty(ConsumerConfig.KEY_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.VALUE_DESERIALIZER_CLASS_CONFIG, StringDeserializer.class.getName()); properties.setProperty(ConsumerConfig.GROUP_ID_CONFIG, groupId); properties.setProperty(ConsumerConfig.AUTO_OFFSET_RESET_CONFIG, \"earliest\"); // create consumer KafkaConsumer consumer = new KafkaConsumer<>(properties); // subscribe consumer to our topic(s) consumer.subscribe(Arrays.asList(topic)); // poll for new data while(true) { log.info(\"Polling\"); ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { log.info(\"Key: \" + record.key() + \", Value: \" + record.value()); log.info(\"Partition: \" + record.partition() + \", Offset: \" + record.offset()); } } 4. Kafka Consumer - Graceful shutdown // create consumer KafkaConsumer consumer = new KafkaConsumer<>(properties); // get a reference to the current thread final Thread mainThread = Thread.currentThread(); // adding the shutdown hook Runtime.getRuntime().addShutdownHook(new Thread() { public void run() { log.info(\"Detected a shutdown, let's exit by calling consumer.wakeup()...\"); consumer.wakeup(); // 会让下面的poll() throw exception // join the main thread to allow the execution of the code in the main thread try { mainThread.join(); } catch (InterruptedException e) { e.printStackTrace(); } } }); try { // subscribe consumer to our topic(s) consumer.subscribe(Arrays.asList(topic)); // poll for new data while(true) { ConsumerRecords records = consumer.poll(Duration.ofMillis(1000)); for (ConsumerRecord record : records) { log.info(\"Key: \" + record.key() + \", Value: \" + record.value()); log.info(\"Partition: \" + record.partition() + \", Offset: \" + record.offset()); } } } catch (WakeupException e) { log.info(\"Wake up exception!\"); // we ignore this as this is an expected exception when closing a consumer } catch (Exception e){ log.error(\"Unexpected exception\"); } finally { consumer.close(); // this will also commit the offsets if need be log.info(\"The consumer is now gracefully closed\"); } 5. 自动提交-至多一次 props.put(\"enable.auto.commit\", \"true\"); props.put(\"auto.commit.interval.ms\", \"1000\"); consumer.poll(100); 6. 手动提交-至少一次 props.put(\"enable.auto.commit\", \"false\"); consumer.commitSync(); 7. 手动指定消费分区和消费位置 指定消费分区 String topic =\"foo\"; TopicPartition partition0 = new TopicPartition(topic, 0); TopicPartition partition1 =new TopicPartition(topic,1); consumer.assign(Arrays.asList(partition0,partition1)); 指定消费位置 seek(TopicPartition, long) Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-08-18 11:01:18 "},"k8s/introduction.html":{"url":"k8s/introduction.html","title":"Kubernetes","keywords":"","body":"1. 常用名词 官方词汇表 考试报名链接CN 考试报名链接EN Master：manage, plan, schedule, monitor nodes（主节点上也可以安装容器引擎） worker nodes：host application containers etcd cluster：存储所有集群数据的数据库 kube-scheduler：调度程序，确定要放置容器的正确节点 controller-manager： node-controller：加载新节点，替换不可用节点 replication-controller：保证一定数量的容器运行 kube-api：协调集群中的所有操作（api是公开的），定期从kubelet获取状态报告 kubelet：集群中在每个节点上运行的代理 kube-proxy：负责节点间通信 2. ETCD 等组件 kubectl 命令看到的所有信息都来自 ETCD k8s 部署的2种方式：kubeadm工具和 scratch ETCD 监听地址：--advertise-client-urls https://${INTERNAL_IP}:2379，kube-api访问etcd时，应配置这个。ETCD 之间连接用2380接口，与其他控制面板组件连接用2379。 kubeadm 设置集群，则会将ETCD服务器部署为kube-system名称空间中的PODkubectl get pods -n kube-system etcdctl get / --prefix -keys-only列出k8s存储的所有密钥 k8s 存储目录 registry：下面有 minions、pods、replicasets、deployments、roles、secrets kubectl 命令运行实际是在访问 kube-apiserver kube-apiserver 负责认证和验证请求，检索和更新ETCD数据存储中的数据 node-controller：每5s访问一次，等待40s然后标记为不可访问（通过kube-apiserver），会给节点5min时间恢复，如果node-controller没有恢复节点，则会删除分配给该节点的pod。 scheduler：只负责决定哪个pod在哪个node上运行，不执行放置（kubelet干这个） kubelet：kubeadm 不会自动部署 kubelet，必须手动在 node 上安装 service：没有任何接口或主动监听进程，存在于k8s内存中 kube-proxy：在每个node上运行的进程，作为守护者进程部署，它的工作是寻找新的service。 IPTABLES规则：将流量从节点的10.32.0.x发送到service的10.96.0.12 3. PODs kubectl run xxx --image=xxx这类命令首先自动创建一个pod，并部署容器引擎如docker img的实例 查看cluster中pod列表：kubectl get pods -o wide pod-definition.yml apiVersion: v1|apps/v1 kind: Pod|Service|ReplicaSet|Deployment metadata: name: myapp-pod labels: app: myapp type: front-end spec: containers: - name: nginx-container image: nginx kubectl create -f pod-definition.yml kubectl describe pod nginx：查看pod详细信息 kubectl delete pod nginx：删除pod kubectl edit pod redis：修改pod配置，或者修改yml文件后kubectl apply -f redis-definition.yaml kubectl run --help：查看帮助 kubectl run redis --image=redis123 --dry-run=client -o yaml > redis.yaml：模拟运行输出到yaml文件 4. ReplicaSets Replication Controller：管理跨越集群中多个node rc-definition.yml apiVersion: v1 kind: ReplicationController metadata: name: myapp-rc labels: app: myapp type: front-end spec: template: metadata: labels: app: type: spec: containers: - name: image: replicas: 3 kubectl get replicationcontroller：查看复制控制器 replicaset-definition.yml apiVersion: apps/v1 kind: ReplicaSet metadata: name: myapp-replicaset labels: app: myapp type: front-end spec: template: metadata: labels: app: type: spec: containers: - name: image: replicas: 3 selector: matchLabels: type: front-end Scale： 修改replicas，然后kubectl replace -f replicaset-definition.yml kubectl scale --replicas=6 -f replicaset-definition.yml kubectl scale --replicas=6 replicaset myapp-replicaset type + name 不会更改yaml文件中的replicas kubectl delete replicaset myapp-replicaset：也会删除所有依赖的PODs replicaset 可以缩写为 rs deployment-definition.yml kind: Deployment kubectl create deployment my-dep-name --image==busybox --replicas=3 5. Service NodePorts范围：30000 - 32767 service-definition.yml apiVersion: V1 kind: Service metadata: name: myapp-service spec: type: NodePort ##LoadBalancer ports: - targetPort: 80 ##pod端口 不填则默认和port相同 port: 80 ##Servoce端口 必填 nodePort: 30008 ##对外端口 不填自动分配30000-32767 selector: ##选择pods的label内容 app: myapp type: front-end kubectl create -f service-definition.yml 192.168.1.2:30008 6. Namespace mysql.connect(\"db-service\") mysql.connect(\"db-service.dev.svc.cluster.local\") cluster.local : domain svc : sub domain for service dev : Namespace db-service : Service Name kubectl get pods --namespace=kube-system 确保在dev Namespace中创建的pod kubectl create -f pod-definition.yml --namespace=dev 在yaml文件中，指定metadata.namespace=dev namespace-dev.yml apiVersion: v1 kind: Namespace metadata: name: dev 设置永久切换Namespace kubectl config set-context $(kubectl config current-context) --namespace=dev 查看所有Namespace中的pods：kubectl get pods --all-namespaces，-Ashort for --all-namespaces 设置Resource Quota，Compute-quota.yaml： apiVersion: v1 kind: ResourceQuota metadata: name: compute-quota namespace: dev spec: hard: pods: \"10\" requests.cpu: \"4\" requests.memory: 5Gi limits.cpu: \"10\" limits.memory: 10Gi kubectl run redis --image=redis -n=finance其中-nshort for --namespace Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/imperative_declarative.html":{"url":"k8s/imperative_declarative.html","title":"命令式和声明式命令及原理","keywords":"","body":"1. 命令式和声明式 命令式 Imperative： kubectl run nginx --image=nginx --port=80 --labels=\"tier=db,env=prod\" --expose=true ##container的port kubectl create deployment --image=nginx nginx kubectl expose deployment nginx --port 80 ## cluster的port kubectl edit deployment nginx ##仅修改活动对象配置文件 kubectl scale deployment nginx --replicas=5 kubectl set image deployment nginx nginx=nginx:1.18 kubectl create -f nginx.yaml kubectl replace -f nginx.yaml ##更新本地配置文件后运行 kubectl replace --force -f nginx.yaml ##删除后再创建 kubectl delete -f nginx.yaml 声明式 Declarative： kubectl apply -f nginx.yaml kubectl apply -f /path ##根据路径创建多个对象 创建 NGINX Pod kubectl run nginx --image=nginx 生成 POD 清单 YAML 文件-o yaml，如果不想实际创建则用--dry-run kubectl run nginx --image=nginx --dry-run=client -o yaml 创建部署 kubectl create deployment --image=nginx nginx 生成部署 YAML 文件-o yaml，如果不想实际创建则用--dry-run kubectl create deployment --image=nginx nginx --dry-run=client -o yaml 使用 4 个副本生成部署 kubectl create deployment nginx --image=nginx --replicas=4 您还可以使用该kubectl scale命令扩展部署。 kubectl scale deployment nginx --replicas=4 另一种方法是将 YAML 定义保存到文件并修改 kubectl create deployment nginx --image=nginx --dry-run=client -o yaml > nginx-deployment.yaml 然后，您可以在创建部署之前使用副本或任何其他字段更新 YAML 文件。 创建一个名为 redis-service 的 ClusterIP 类型的 Service 以在端口 6379 上公开 pod redis kubectl expose pod redis --port=6379 --name redis-service --dry-run=client -o yaml （这将自动使用 pod 的标签作为选择器） 或者 kubectl create service clusterip redis --tcp=6379:6379 --dry-run=client -o yaml （这不会将 pods 标签用作选择器，而是将选择器假定为app=redis。您不能将选择器作为选项传递。因此，如果您的 pod 具有不同的标签集，它就不能很好地工作。所以生成文件并在创建服务之前修改选择器） 创建一个名为 nginx 的 NodePort 类型的 Service 以在节点上的 30080 端口上公开 pod nginx 的 80 端口： kubectl expose pod nginx --type=NodePort --port=80 --name=nginx-service --dry-run=client -o yaml （这将自动使用 pod 的标签作为选择器，但您不能指定节点端口。您必须生成定义文件，然后手动添加节点端口，然后再使用 pod 创建服务。） 或者 kubectl create service nodeport nginx --tcp=80:80 --node-port=30080 --dry-run=client -o yaml （这不会使用 pods 标签作为选择器） 上述两个命令都有自己的挑战。虽然其中一个不能接受选择器，但另一个不能接受节点端口。我建议使用kubectl expose命令。如果需要指定节点端口，请使用相同的命令生成定义文件，并在创建服务之前手动输入节点端口。 参考： https://kubernetes.io/docs/reference/generated/kubectl/kubectl-commands https://kubernetes.io/docs/reference/kubectl/conventions/ 2. kubectl apply 原理 本地的yaml配置文件会转换成 json 格式的文件 kubectl apply 会对本地配置文件、最后一次 apply 的配置文件（Json）和实时对象配置文件进行对比，当本地配置文件更新后也会同时更新其他2个配置文件 合并更改：https://kubernetes.io/docs/tasks/manage-kubernetes-objects/declarative-config/#merging-changes-to-primitive-fields Json 内容实际保存在实时对象配置文件中的 metadata: annotations: kubectl.kubernetes.io/last-applied-configuration: {Json Content} 实际请不要这样配置 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/scheduling.html":{"url":"k8s/scheduling.html","title":"调度方式","keywords":"","body":"1. 绑定 Pod 未命名隐藏字段nodeName的 pod 将作为候选进行调度 spec: nodeName: node02 但对于已创建的 pod，pod-definition.yaml中的nodeName字段不允许修改，因此将Node分配给 pod 的另一种方法是创建一个绑定对象，并向 pod 绑定 API 发送 post 请求： Pod-bind-definition.yaml apiVersion: v1 kind: Binding metadata: name: nginx target: apiVersion: v1 kind: Node name: node02 把 yaml 文件转换成 Json 格式 curl --header \"Content-Type:application/json\" --request POST --data '{\"apiVersion\":\"v1\", \"kind\": \"Binding\", ...}' http://$SERVER/api/v1/namespaces/default/pods/$PODNAME/binding 2. Labels kubectl get all kubectl get pods -l env=dev | wc -l : 列出env=dev的pod个数（包含header），-l short for --selector kubectl get pods -l env=dev --no-headers | wc -l : 列出env=dev的pod个数（不包含header） kubectl label node node01 color=blue : 添加Label 3. Taints - Node kubectl taint nodes node-name key=value:taint-effect taint-effect : NoSchedule | PreferNoSchedule | NoExecute NoExecute : 新Pod不会部署，已存在的节点也会终止 4. Tolerations - Pods pod-definition.yml spec: tolerations: - key: app operator: Equal value: blue effect: NoSchedule kubectl describe node kubemaster | grep Taint : 查看master node的Taint kubectl taint node node-name key=value:NoSchedule- : 清除taint 5. Node Selectors pod-definition.yml spec: nodeSelector: size: Large ##生效前需要先标记 node kubectl label nodes = : 标记node 6. Node Affinity pod-definition.yml spec: affinity: nodeAffinity: requiredDuringSchedulingIgnoredDuringExecution: nodeSelectorTerms: - matchExpressions: - key: size operator: NotIn | In | Exists ## Exists运算符甚至不需要下面的values values: - Large - Medium Available ： requiredDuringSchedulingIgnoredDuringExecution preferredDuringSchedulingIgnoredDuringExecution Planned : requiredDuringSchedulingrequiredDuringExecution DuringScheduling：Pod 不存在且是首次创建 DuringScheduling DuringExecution Type1 Required Ignored Type2 Preferred Ignored Type3 Required Required 7. Multiple Schedulers kube-scheduler.service ExecStart=/usr/local/bin/kube-scheduler \\\\ --config=/etc/kubernetes/config/kube-scheduler.yaml \\\\ --scheduler-name= default-scheduler /etc/kubernetes/manifests/kube-scheduler.yaml apiVersion: v1 kind: Pod metadata: name: my-custom-scheduler namespace: kube-system spec: containers: - command: ##包含用于启动调度程序的命令和相关选项 - kube-scheduler - --address=127.0.0.1 - --kubeconfig=/etc/kubernetes/scheduler.conf - --leader-elect=true ##没有多个主节点运行scheduler就设置为false - --scheduler-name=my-custom-scheduler ##设置调度程序的自定义名称 - --lock-object-name=my-custom-scheduler ##用于区分自定义调度程序和默认调度程序 image: k8s.gcr.io/kube-scheduler-amd64:v1.11.3 name: kube-scheduler pod-definition.yaml apiVersion: v1 kind: Pod metadata: name: nginx spec: containers: - image: nginx name: nginx schedulerName: my-custom-scheduler kubectl get events：查看哪个 scheduler 调用 kubectl logs my-custom-scheduler --name-space=kube-system：查看日志 kubectl create configmap my-scheduler-config --from-file=/root/my-scheduler-config.yaml -n kube-system：创建 configmap Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/set_resources.html":{"url":"k8s/set_resources.html","title":"设置硬件资源","keywords":"","body":"pod-definition.yaml : 也可以手动设定所需资源 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 resources: requests: ##没有足够的资源创建，则pod会pending memory: \"1Gi\" cpu: 1 limits: memory: \"2Gi\" cpu: 2 单位： CPU：最小0.1，代表100M，M - million。1 CPU代表1 vCPU aws | 1 core in GCP | 1 hyper thread memory：1G - Gigabyte，1M - Megabyte，1K - Kilobyte，1Gi - Gibibyte，1Mi - Mebibyte，1Ki - Kibibyte，带i是精确的用1024换算 默认： Docker默认不限制container的CPU使用，Kubernetes默认设置1 CPU 对于memory也是类似，Kubernetes默认设置512 Mi 超限： Kubernetes默认throttle，CPU不会超出限制 container可以超限使用内存，pod使用超出限制的内存，则会被terminate 使用默认值前需要在Namespace创建LimitRange apiVersion: v1 kind: LimitRange metadata: name: mem-limit-range spec: limits: - default: memory: 512Mi defaultRequest: memory: 256Mi type: Container https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/memory-default-namespace/ apiVersion: v1 kind: LimitRange metadata: name: cpu-limit-range spec: limits: - default: cpu: 1 defaultRequest: cpu: 0.5 type: Container https://kubernetes.io/docs/tasks/administer-cluster/manage-resources/cpu-default-namespace/ References: https://kubernetes.io/docs/tasks/configure-pod-container/assign-memory-resource 已存在的pod属性不可编辑，除了以下部分属性： spec.containers[*].image spec.initContainers[*].image spec.activeDeadlineSeconds spec.tolerations 运行edit编辑pod不可修改的属性，会显示 Forbidden: pod updates may not change fields other than ... 的详细信息，并生成临时copy，可以利用copy修改需要的属性，删除原有pod后可根据临时文件生成新的pod。 编辑deployment不受影响，新的配置会自动删除原来的pod生成新的。 kubectl replace --force -f pod-def.yaml Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/daemonsets_staticpod.html":{"url":"k8s/daemonsets_staticpod.html","title":"守护进程和静态Pod","keywords":"","body":"1. Daemon Sets Daemon Sets 确保 pod 的一个副本始终存在于集群的所有节点中，常用于 Monitoring Solution、Logs Viewer、Kube-porxy、Weave-net（networking）。 daemon-set-definition.yaml apiVersion: apps/v1 kind: DaemonSet ##唯一区别 metadata: name: elasticsearch namespace: kube-system labels: k8s-app: fluentd-logging spec: selector: matchLabels: name: fluentd-elasticsearch template: metadata: labels: name: fluentd-elasticsearch spec: containers: - name: fluentd-elasticsearch image: k8s.gcr.io/fluentd-elasticsearch:1.20 在 v1.12 之前，pod 可以设置 nodeName 以放置到想要的 node 上，之后使用 scheduler 和 affinity。 因为没有kubectl create daemonset相关的命令，所以创建DaemonSets时可以先用create deployment 命令生成 yaml 模板，kubectl create deployment ds-name -n=namespace-name --image=image-name --dry-run=client -o yaml > app.yaml，修改后 apply。 2. Static Pods kubelet 依赖于 kube-apiserver 来获得关于在其 node 上加载哪些 pod 的指令，这是基于存储在 etcd 数据库中的 kube-scheduler 所做的决定。 kubelet 也可以独立运行，可以创建 pod，可以指定用于存储 pod 信息的目录中读取 pod 定义文件。kubelet 会每隔一段时间确认 pod 定义文件的信息，并保持一致。 replicasets、deployment、service无法独立运行。它们都是整个 Kubernetes 架构的概念组成部分，需要复制和部署控制器等其他控制平面组件。 kubelet 在 pod 级别工作，只能理解 pod，这也是为什么它能够创建 static pod。 指定目录可以是任意地址，指定方式为kubelet.service文件中 --pod-manifest-path=/etc/Kubernetes/manifests --config=kubeconfig.yaml ps : kubeadmin 也是用这种方式实现的 其中kubeconfig.yaml staticPodPath: /etc/Kubernetes/manifests 用docker ps查看 Static Pod 生成结果，如果没有 Kubernetes cluster。 如果有 Kubernetes cluster，kube-apiserver 会知道 Static Pod 的情况。（kube-apiserver 上会有个 Static Pod 的只读镜像，pod 的 name 会附加 node 的名称） 可以使用 Static Pod 将控制平面组件本身作为 pod 部署在 node 上，这样就可以在本地进行部署，不必下载二进制文件配置服务或担心服务崩溃，这也是 kubeadmin 工具设置 Kubernetes 集群的方式。 Static Pods vs DaemonSets Static Pods DaemonSets Created by the Kubelet Created by Kube-API server (DaemonSet Controller) Deploy Control Plane components as Static Pods Deploy Monitoring Agents, Logging Agents on nodes Ignored by the Kube-Scheduler Ignored by the Kube-Scheduler 判断是 Static Pod 的几种方式： pod name 结尾带有 node name kubectl get pod pod-name -n=kube-system -o yaml中查看配置文件，ownerReferences 属性下 kind 为 Node，普通的为 ReplicaSet 等 查看 Static Pod 的配置文件位置： 查找config文件的方式 ps -aux | grep kubelet 查看 --config 项 查看/var/lib/kubelet/config.yaml中的staticPodPath 添加 command 的方式：在 kubectl 命令后加上--command -- sleep 1000，请保证--command放在整条命令之后，所有在--后的都会被视为添加的 command。 创建 Static Pod 的方式就是把 pod 定义文件放到 staticPath 切换 node 的方式ssh node-ip-address Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/monitor_env.html":{"url":"k8s/monitor_env.html","title":"监控和环境变量","keywords":"","body":"1. 监控 Kubernetes 没有提供功能全面的内置监控解决方案，但有许多开源解决方案可用，如 Metrics-Server、Prometheus、Elastic Stack、DATADOG、dynatrace。 Heapster 是 Kubernetes 启用监控和分析功能的原始项目之一，但现已弃用，并形成了一个精简版本，称为 Metrics Server（In-Memory）。 Kubelet 包含一个称为 cAdvisor 或 container advisor 的子组件，负责从 pod 中检索性能指标，发送给 apiserver。 2. 使用 Metrics Server 安装 minikube : minikube addons enable metrics-server others : git clone https://github.com/kubernetes-incubator/metrics-server.git 部署：pods、services、roles kubectl create -f deploy/1.8+/ : 在下载后 repo 中执行-f . 查看：kubectl top (node | pod) 3. Logs docker run kodekloud/event-simulator，event-simulator 用来生成随机事件模拟 web 服务器，它本身有 std ouput，但如果在 detach 模式下运行run -d，那么想查看日志可以使用docker logs -f container-id，-f选项帮助查看实时日志跟踪。 在 Kubernetes 中也是一样： kubectl create -f event-simulator.yaml apiVersion: v1 kind: Pod metadata: name: event-simulator-pod spec: containers: - name: event-simulator image: kodekloud/event-simulator kubectl logs -f pod-name container-name：如果 pod 中包含多个 container，则必须要指明 container-name，否则会报错，或用-c指定 container。 当 Pod 有多个容器时打开 shell：kubectl -n elastic-stack exec -it app -- cat /log/app.log或kubectl exec -i -t my-pod --container main-app -- /bin/bash，-iand-tare the same as the long options --stdin and --tty 5. 命令和参数（Commands and Arguments） pod-definition.yml 实际执行为 command 后面跟着 args apiVersion: v1 kind: Pod metadata: name: ubuntu-sleeper-pod spec: containers: - name: ubuntu-sleeper image: ubuntu-sleeper command: [\"sleep2.0\"] ##覆写dockerfile中的ENTRYPOINT args: [\"10\"] ##覆写dockerfile中的CMD 也可以写成 command: - \"sleep\" - \"10\" 6. Kubernetes 中的环境变量 docker run -e APP_COLOR=pink simple-webapp-color pod-definition.yaml spec: containers: env: - name: APP_COLOR value: pink valueFrom: ## 2. 单个环境变量导入 configMapKeyRef: ##ConfigMap name: key: secretKeyRef: ##Secrets 7. ConfigMaps spec: containers: envFrom: ## 1. 多个环境变量导入 - configMapRef: name: app-color volumes: ##3. 作为文件导入volume - name: app-config-volume configMap: name: app-config 创建： kubectl create configmap --from-literal== --from-literal=... 或使用文件创建 --from-file=app_config.properties kubectl create -f apiVersion: v1 kind: ConfigMap metadata: name: app-config data: APP_COLOR: blue APP_MODE: prod Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/secrets.html":{"url":"k8s/secrets.html","title":"Secrets","keywords":"","body":"1. 创建 kubectl create secret generic --from-literal== 或 --from-file= kubectl create -f secret-data.yaml secret-data.yaml apiVersion: v1 kind: Secret metadata: name: app-secret data: DB_Host: mysql DB_User: root DB_Password: paswrd 2. 转换编码 编码：echo -n 'mysql' | base64 解码：echo -n 'bXlzcWw=' | base64 --decode 3. 查看 kubectl get secret app-secret -o yaml 4. 添加到pod pod-definition.yaml : secrets 可以作为数据卷挂载或公开为环境变量由 Pod 中的容器使用 apiVersion: v1 kind: Pod metadata: name: simple-webapp-color labels: name: simple-webapp-color spec: containers: - name: simple-webapp-color image: simple-webapp-color ports: - containerPort: 8080 envFrom: - secretRef: name: app-config env: - name: DB_Password valueFrom: secretKeyRef: name: app-secret key: DB_Password volumes: - name: app-secret-volume secret: secretName: app-secret 如果以文件形式创建secret，则对每个secret会生成对应的文件：ls /opt/app-secret-volumes，cat /opt/app-secret-volumes/DB_Password 最佳实践： 未将secret对象定义文件签入源代码存储库 为Secret启用静态加密，以便将它们加密存储在ETCD中 Kubernetes处理secret的方式： 仅当节点上的pod需要时，才会将secret发送到该节点 Kubelet将secret存储到tmpfs中，这样secret就不会写入磁盘存储 一旦依赖于secret的Pod被删除，kubelet也会删除其本地的secret数据副本 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/upgrade.html":{"url":"k8s/upgrade.html","title":"版本升级","keywords":"","body":"1. Upgrades 默认 pod 超时时间为5分钟：kube-controller-manager --pod-eviction-timeout=5m0s 系统升级时更安全的方式是：kubectl drain node-1，节点会被标记为不可调度，会清除 pod，但当 pod 不属于 replicaSet 时，drain 会失败，需要加上 --force，这个 pod 就会丢失，不会再在其他node 上被创建 升级完成之后：kubectl uncordon node-1 kubectl cordon node-2不会清除pod 使用 kubectl get nodes 可以在 VERSION 列查看版本 v1.11.3 MAJOR MINOR : Features, Functionalities PATCH : Bug Fixes ETCD、CoreDNS 的版本独立，kube-apiserver 的版本一般比其他的组件更高，kubectl 的版本可能会高或者低1个版本，其他 Controller-manager、kube-scheduler、kubelet、kube-proxy 版本保持一致。 Kubernetes 只支持最近的3个版本，当使用的 k8s 版本不再支持时，建议一次只升级一个版本。 使用 kubeadm 升级： kubeadm upgrade plan 查看最近稳定可用的版本 kubeadm upgrade apply 先升级 master node，再升级 work node 2. Releases 升级master node： apt-get upgrade -y kubeadm=1.12.0-00 kubeadm upgrade apply v1.12.0 kubectl get nodes 上显示的是apiserver版本 如果master node上有kubelet，则apt-get upgrade -y kubelet=1.12.0-00，然后systemctl restart kubelet 升级work node： kubectl drain node-1 把节点上的pod转移到其他节点 apt-get upgrade -y kubeadm=1.12.0-00 apt-get upgrade -y kubelet=1.12.0-00 kubeadm upgrade node config --kubelet-version v1.12.0 systemctl restart kubelet kubectl uncordon node-1 查看系统版本：cat /etc/*release* Ubuntu 升级 master node 流程：升级 node 用 upgrade apply 版本 On the controlplane node, run the command run the following commands: apt update : This will update the package lists from the software repository. apt install kubeadm=1.20.0-00 : This will install the kubeadm version 1.20 kubeadm upgrade apply v1.20.0 : This will upgrade kubernetes controlplane. Note that this can take a few minutes. apt install kubelet=1.20.0-00 : This will update the kubelet with the version 1.20. You may need to restart kubelet after it has been upgraded. Run: systemctl daemon-reload, systemctl restart kubelet Ubuntu 升级 work node 流程：升级 node 用 upgrade node If you are on the master node, run ssh node01 to go to node01 apt update : This will update the package lists from the software repository. apt install kubeadm=1.20.0-00 : This will install the kubeadm version 1.20 kubeadm upgrade node : This will upgrade the node01 configuration. 不用写node-name apt install kubelet=1.20.0-00 : This will update the kubelet with the version 1.20. You may need to restart kubelet after it has been upgraded. Run: systemctl daemon-reload, systemctl restart kubelet Type exit or enter CTL + d to go back to the controlplane node. uncordon work node 需要切换到 master node 官方文档：https://kubernetes.io/docs/tasks/administer-cluster/kubeadm/kubeadm-upgrade/ 3. Rolling Updates and Rollbacks kubectl rollout status deployment/myapp-deployment：查看状态 kubectl rollout history deployment/myapp-deployment：查看历史 部署策略： Recreate：一次销毁所有旧的，新建新版本 Rolling Update：滚动更新，一次更新一个 更新 image 的方式： kubectl apply -f deployment-definition.yml kubectl set image deployment/myapp-deployment nginx=nginx:1.9.1：会导致部署定义文件具有不同的配置 回滚：kubectl rollout undo deployment/myapp-deployment，回滚会在原来的replicasets上重新创建 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/backup_tls.html":{"url":"k8s/backup_tls.html","title":"备份和TLS","keywords":"","body":"1. 备份 kubectl get all --all-namespaces -o yaml > all-deploy-services.yaml 与其备份单个资源，不如备份ETCD： etcd.service --data-dir=/var/lib/etcd etcd也自带快照功能 ETCDCTL_API=3 etcdctl \\ ##根据etcdctl版本 不想重复写就export ETCDCTL_API=3 设置全局参数 snapshot save snapshot.db snapshot status snapshot.db ##查看备份状态 2. 还原 Restore：会初始化新的集群配置，将etcd配置为新成员，以防止新成员加入现有集群 ETCDCTL_API=3 etcdctl \\ service kube-apiserver stop snapshot restore snapshot.db --data-dir /var/lib/etcd-from-backup ##使用新的数据目录，将备份还原到此数据目录 systemctl daemon-reload service etcd restart service kube-apiserver start 可以对备份指定访问端口和密钥信息： ETCDCTL_API=3 etcdctl \\ snapshot save snapshot.db ##如果启用了TLS，以下选项是强制性的 --endpoints=https://127.0.0.1:2379 --cacert=/etc/etcd/ca.crt --cert=/etc/etcd/etcd-server.crt --key=/etc/etcd/etcd-server.key 还原后需要设置新的 etcd-data 的 hostPath： volumes: - hostPath: path: /var/lib/etcd ##修改为etcd-from-backup新目录 type: DirectoryOrCreate name: etcd-data 查看 node 相关的 clusters： kubectl config view kubectl config get-clusters 更换 node 上的 cluster context： kubectl config use-context cluster1 查看ETCD服务器所属的ETCD集群中有多少节点： ETCDCTL_API=3 etcdctl \\ --endpoints=https://127.0.0.1:2379 \\ --cacert=/etc/etcd/pki/ca.pem \\ --cert=/etc/etcd/pki/etcd.pem \\ --key=/etc/etcd/pki/etcd-key.pem \\ member list 跨 node 复制文件：scp cluster1-controlplane:/opt/cluster1.db /opt/cluster1.db 如果是外部的 etcd，需要修改 /etc/systemd/system/etcd.service中的 data-dir，添加新路径的 etcd 权限 chown -R etcd:etcd /var/lib/etcd-data-new，最后重启服务 systemctl daemon-reload, systemctl restart etcd。 3. Authentication 禁用基于密码的身份验证 仅提供基于SSH密钥的身份验证 第一道防线：控制对API服务器本身的访问 用户名+密码 用户名+Token 证书 与LDAP等外部身份验证提供程序集成 服务账户 可以做什么？ RBAC Authorization 基于角色的访问控制 ABAC Authorization 基于属性的访问控制 Node Authorization 基于节点的访问控制 Webhook Mode kubectl create serviceaccount sa1 kubectl get serviceaccount 验证方式kube-apiserver： Static Password File Static Token File Certificates Identity Services，第三方身份验证协议，如LDAP、Kerberos等 4. Static Password File user-details.csv : password,username,userid(,groupname optional) ... user-token-details.csv : token,username,userid(,groupname optional) ... 定义方式： --basic-auth-file=user-details.csv --token-auth-file=user-details.csv apiserver重启才能生效 kubeadm /etc/kubernetes/manifests/kube-apiserver.yaml apiVersion: v1 kind: Pod metadata: creationTimestamp: null name: kube-apiserver namespace: kube-system spec: containers: - command: - kube-apiserver - --authorization-mode=Node,RBAC - --advertise-address=172.17.0.107 - --allow-privileged=true - --enable-admission-plugins=NodeRestriction - --enable-bootstrap-token-auth=true image: k8s.gcr.io/kube-apiserver-amd64:v1.11.3 name: kube-apiserver 访问： curl -v -k https://master-node-ip:6443/api/v1/pods -u \"username:password\" {data...} curl -v -k --header \"Authorization: Bearer xxxxxxxx\" 注意点： 不推荐使用静态储存（在v1.19已弃用） Consider volume mount while providing the auth file in a kubeadm setup Setup Role Based Authorization for the new users 5. TLS 查看 Common Name (CN)：openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text Symmetric Encryption：对称加密，使用相同的密钥来加密和解密数据，必须在发送方和接收方之间交换，因此存在风险 Asymmetric Encryption：非对称加密，Private Key 和 Public Lock ssh-keygen 生成私钥 id_rsa 和公钥 id_rsa.pub 添加公钥：通常是在服务器SSH授权的下划线密钥文件中添加一个包含公钥的条目来完成的 cat ~/.ssh/authorized_keys ssh-rsa AAAAB3Nza... user1 ssh-rsa AAAXCV2b8... user2 指定私钥 ssh -i id_rsa user1@server1 使用openssl生成私钥、公钥： openssl genrsa -out my-bank.key 1024 ## private key ## my-bank.key openssl rsa -in my-bank.key -pubout > mybank.pem ## public key ## my-bank.key mybank.pem 证书包含： 有关谁向该服务器的公钥颁发证书的信息 该服务器地址 ... 生成证书的时候必须带有签名，自签名的证书会被认为非法 Certificate Authority (CA) 负责签署和验证证书，比较著名的有 Symantec、Desert、Comodo、Global Sign 等等 签名的方式： 用之前生成的密钥和网站域名生成Certificate Signing Request (CSR) openssl req -new -key my-bank.key -out my-bank.csr -subj \"/C=US/ST=CA/O=MyOrg, Inc./CN=mydomain.com\" ## 对于hacker Validate Information会失败 ## my-bank.key my-bank.csr 验证通过，Sign and Send Certificate CA合法性的验证：签名用私钥，所有CA的公钥都存在浏览器中 私有CA：一样的运作方式，为组织内所有浏览器安装私有CA的公钥 Kubernetes要求集群至少有一个证书颁发机构CA，也可以设置多个 之前尝试使用命令行登陆证书过期的https失败 openssl x509 -inform der -in \\*.thoughtworks.cn.cer -out certificate.pem 但是缺少私钥，一般会生成公钥和私钥，或者合并为同一份pem文件。 curl --cert certificate.pem --header 'Content-Type: application/json' -d '{\"captcha\": \"111\", \"captchaId\": \"captchaId\", \"password\": \"password\", \"username\": \"user\"}' --request POST https://sample.com 更改hosts方法 sudo vim /etc/hosts 如果api-server不可用，用命令查看频繁退出的container的id crictl ps -a | grep kube-apiserver，并查看日志 crictl logs --tail=2 1fb242055cff8找出原因 - Container Runtime Interface (CRI) 6. Certificate 6.1 证书创建 证书生成工具：easyrsa、openssl、cfssl 生成CA certificate步骤： Generate Keys openssl genrsa -out ca.key 2048 Certificate Signing Request openssl req -new -key ca.key -subj \"/CN=KUBERNETES-CA\" -out ca.csr Sign Certificates : CA创建root certificate是自我签名 openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt 生成client certificate步骤： Generate Keys openssl genrsa -out admin.key 2048 Certificate Signing Request openssl req -new -key admin.key -subj \"/CN=kube-admin\" -out admin.csr Sign Certificates : CA创建root certificate是自我签名 openssl x509 -req -in ca.csr -signkey ca.key -out ca.crt 可以通过在证书中添加用户组详细信息以区分不同的注册用户：openssl req -new -key admin.key -subj \"/CN=kube-admin/O=system:masters\" -out admin.csr 系统组件其名称必须以关键字system作为前缀 kube-api server 有多个别名：kubernetes、kubernetes.default、kubernetes.default.svc、kubernetes.default.svc.cluster.local kube-api 添加别名方式： openssl req -new -key apiserver.key -subj \"/CN=kube-apiserver\" -out apiserver.csr -config openssl.cnf openssl.cnf： [alt_names] DNS.1 = kubernetes DNS.2 = kubernetes.default DNS.3 = kubernetes.default.svc DNS.4 = kubernetes.default.svc.cluster.local IP.1 = 10.96.0.1 IP.2 = 1720.17.0.87 6.2 查看证书细节 x509解码证书以查看详细信息： openssl x509 -in /etc/kubernetes/pki/apiserver.crt -text -noout 如果核心组件（如kubernetes api-server或etcd-server关闭），kubectl命令无法使用，可以用docker命令查看日志 Kubernetes Certificate Health Check Spreadsheet : https://github.com/mmumshad/kubernetes-the-hard-way/tree/master/tools 7. Certificates API 用户创建key： openssl genrsa -out jane.key 2048 用户把key发给admin，admin用这个key创建certificate signing request对象： openssl req -new -key jane.key -subj \"/CN=jane\" -out jane.csr Jane-csr.yaml kubectl create -f Jane-csr.yaml apiVersion: certificates.k8s.io/v1beta1 kind: CertificateSigningRequest metadata: name: jane spec: groups: - system:authenticated usages: - digital signature - key encipherment - server auth request: ## 请求内容用base64 encode ## cat jane.csr | base64 | tr -d \"\\n\" 或 base64 -w 0 禁用换行 创建对象后，admin可以通过 kubectl get csr 查看所有证书请求，通过 kubectl certificate approve jane，拒绝则用 deny，删除则用 kubectl delete csr jane 通过以yaml格式查看证书 kubectl get csr jane -o yaml，用base64解码 echo \"...=\" | base64 --decode 所有与证书相关的操作都由Controller Manage执行：其中包含CSR-APPROVING、CSR-SIGNING等控制器 其中的key和根证书在 /etc/kubernetes/manifests/kube-controller-manager.yaml 中的 spec: containers: - command: - --cluster-signing-cert-file=/etc/kubernetes/pki/ca.crt - --cluster-signing-key-file=/etc/kubernetes/pki/ca.key Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/rbac.html":{"url":"k8s/rbac.html","title":"RBAC","keywords":"","body":"1. KubeConfig 使用访问API获取pods信息： curl https://my-kube-playground:6443/api/v1/pods \\ --key admin.key --cert admin.crt --cacert ca.crt 用kubectl达成相同的目的： kubectl get pods --server my-kube-playground:6443 --client-key admin.key --client-certificate admin.crt --certificate-authority ca.crt 使用kubeconfig代替每次冗长的输入： kubectl get pods --kubeconfig config $HOME/.kube/config apiVersion: v1 kind: Config current-context: dev-user@google ## 指定默认的上下文 clusters: - name: production cluster: certificate-authority: /etc/kubernetes/pki/ca.crt ## 也可以直接使用证书内容代替 certificate-authority-data: 以cat ca.crt | base64格式写入 server: https://172.17.0.51:6443 contexts: - name: admin@production context: cluster: production user: admin namespace: finance users: - name: admin user: client-certificate: /etc/kubernetes/pki/users/admin.crt client-key: /etc/kubernetes/pki/users/admin.key 使用 kubectl config view 查看当前kubeconfig 可以 kubectl config view --kubeconfig=my-custom-config 指定kubeconfig 使用 kubectl config use-context prod-user@prodection --kubeconfig /root/my-kube-config 指定kubeconfig，更换上下文 设置默认config：mv .kube/config .kube/config.bak 备份，然后复制 cp /root/my-kube-config .kube/config 2. RBAC 集群层面请使用 ClusterRole 和 ClusterRoleBindings developer-role.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: developer rules: - apiGroups: [\"\"] ## 可以用 kubectl api-resources 查看 apiversion resources: [\"pods\"] verbs: [\"list\", \"get\", \"create\", \"update\", \"delete\"] - apiGroups: [\"\"] resources: [\"ConfigMap\"] verbs: [\"create\"] 创建角色：kubectl create -f developer-role.yaml devuser-developer-binding.yaml apiVersion: rbac.authorization.k8s.io/v1 kind: RoleBinding metadata: name: devuser-developer-binding subjects: - kind: User name: dev-user apiGroup: rbac.authorization.k8s.io roleRef: kind: Role name: developer apiGroup: rbac.authorization.k8s.io 创建角色绑定：kubectl create -f devuser-developer-binding.yaml 查询： kubectl get roles kubectl get rolebindings kubectl describe role developer kubectl describe rolebinding devuser-developer-binding 验证权限： kubectl auth can-i create deployments kubectl auth can-i delete nodes --as dev-user --namespace test 使用命令行创建、绑定、编辑角色： kubectl create role developer --namespace=default --verb=list,create,delete --resource=pods kubectl create rolebinding dev-user-binding --namespace=default --role=developer --user=dev-user kubectl edit role developer -n blue apiVersion: rbac.authorization.k8s.io/v1 kind: Role metadata: name: developer namespace: blue rules: - apiGroups: - apps resourceNames: - dark-blue-app resources: - pods verbs: - get - watch - create - delete - apiGroups: - apps resources: - deployments verbs: - get - watch - create - delete 3. Service Accounts kubectl create serviceaccount dashboard-sa kubectl get serviceaccount kubectl describe serviceaccount dashboard-sa 当创建Service Account时，首先创建 Service Account 对象，然后为服务账户生成令牌 查看secret token：kubectl describe secret dashboard-sa-token-kbbdm 利用 Service Account 创建 token：kubectl create token sa-name 默认采用 default 的 Service Account，可以在deployment指明 Service Account，在 pod spec 下添加 serviceAccountName: sa-name 使用curl访问： curl https://192.168.56.70:6443/api -insecure --header \"Authorization: Bearer \" 每个namespace都会创建 default 的 Service Account：kubectl exec -it my-kubernetes-dashboard ls /var/run/secrets/kubernetes.io/serviceaccount，默认只有运行基本的Kubernetes API 查询的权限 无法编辑pod中的Service Account，只能删除后新建，在deployment中可以。 如果不想自动挂载Service Account，可以设置： spec: automountServiceAccountToken: false 4. Image Security image: docker.io/library/nginx：不指定用户名或账户名，则会假定为library。Registry，User/Account，Image/Repository docker login private-registry.io docker run private-registry.io/apps/internal-app kubectl create secret docker-registry regcred \\ ## 取名为regcred --docker-server= private-registry.io \\ --docker-username= registry-user \\ ## 类型为docker-registry --docker-password= registry-password \\ --docker-email= registry-user@org.com deployment 应用 secret 需要添加以下设置： spec: imagePullSecrets: - name: regcred 查看哪个user在运行pod：kubectl exec pod-name -- whoami 修改 securityContext spec: securityContext: runAsUser: 1010 ## 设置在pod或container level spec: securityContext: capabilities: ## 设置在container level add: [\"SYS_TIME\"] Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/volumes.html":{"url":"k8s/volumes.html","title":"Volumes","keywords":"","body":"1. Volumes !!!注意题目给的 mountPath 后面有没有 /，可能会判定为不同路径。 apiVersion: v1 kind: Pod metadata: name: random-number-generator spec: containers: - image: alpine name: alpine command: [\"/bin/sh\",\"-c\"] args: [\"shuf -i 0-100 -n 1 >> /opt/number.out;\"] volumeMounts: - mountPath: /opt ## Container内的存储路径 name: data-volume volumes: - name: data-volume hostPath: path: /data ## 对应存储在host上的路径 type: Directory ## awsElasticBlockStore: ## 可替换hostPath ## volumeID: ## fsType: ext4 2. Persistent Volumes Persistent Volumes Claim (PVC) pv-definition.yaml apiVersion: v1 kind: PersistentVolume metadata: name: pv-vol1 spec: accessModes: - ReadWriteOnce capacity: storage: 1Gi awsElasticBlockStore: volumeID: fsType: ext4 管理员创建一组PV，用户创建PVC以用于存储。单个PVC绑定单个PV。 可以使用label进行匹配，如果所有其他条件都匹配，并且没有更好的选项，则较小的PVC可能会绑定到较大的PV，且剩下的存储空间无法再被其他PVC利用。没有PV可用，则PVC会保持pending状态。 selector: matchLabels: name: my-pv ## PVC labels: name: my-pv ## PV pvc-definition.yaml apiVersion: v1 kind: PersistentVolumeClaim metadata: name: myclaim spec: accessModes: - ReadWriteOnce resources: requests: storage: 500Mi PVC被删除后，PV的状态取决于： persistentVolumeReclaimPolicy: Retain | Delete | Recycle 3. Storage Class 在应用程序需要时自动配置 volumes sc-definition.yaml 不需要再配置 pv，由 Storage Class 自动创建 apiVersion: storage.k8s.io/v1 kind: StorageClass metadata: name: google-storage provisioner: kubernetes.io/gec-pd parameters: ## 取决于磁盘配置 type: pd-standard replication-type: none pvc-definition.yaml spec: storageClassName: google-storage 4. 存储 安装Docker后，默认存储路径： /var/lib/docker |- aufs |- containers |- image |- volumes 对于Dockerfile buid时的分层架构： Dockerfile FROM Ubuntu RUN apt-get update && apt-get -y install python RUN pip install flask flask-mysql COPY . /opt/source-code ENTRYPOINT FLASK_APP=/opt/source-code/app.py flask run docker build Dockerfile -t my-name/my-custom-app Layer 1. Base Ubuntu Layer ## 120MB Layer 2. Changes in apt packages ## 360MB Layer 3. Changes in pip packages ## 6.3MB 前3层可以复用 Layer 4. Source code ## 229B Layer 5. Update Entrypoint ## 0B 以上都是Image Layers（readonly），Container Layer只有当Container存在时存在 Layer 6. Container Layer ## read & write COPY-ON-WRITE：Image Layers的文件虽然只读，但仍然可以修改，在保存修改之前，Docker会自动在读写层中创建该文件的副本，然后在读写层中修改该文件的不同版本。 volumes：保留持久化数据，即使container被破坏 docker volume create data_volume /var/lib/docker |- volumes |- data_volume 设置image的默认存储数据位置：docker run -v data_volume:/var/lib/mysql mysql，即使在执行这条命令前没有运行创建卷 docker volume create，Docker将会自动创建相对应的卷。 创建的数据卷会挂载到container中的 /var/lib/mysql 绑定挂载：如果数据已存在，则需要给出完整的路径，docker run -v /data/mysql:/var/lib/mysql mysql -v是旧式的写法，现在一般推荐使用--mount，更清晰明了：docker run --mount type=bind,source=/data/mysql,target=/var/lib/mysql mysql 5. Container Storage Interface 如同 Container Runtime Interface (CRI)、Container Storage Interface (CSI)可以适配不同的存储驱动，而不用依赖于Kubernetes本身的代码。 RPC (Remote Procedure Call)：远程过程调用 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/namespace.html":{"url":"k8s/namespace.html","title":"Namespace","keywords":"","body":"1. Network Namespace Docker 等容器使用 namespace 来实现网络隔离，容器是使用 namespace 与底层主机分离的。如果把 host 当成房子，那么 namespace 就是房子里你分配给每个孩子的房间。 就容器而言，它只看到由它运行的进程，并认为它是独立的。但是，底层 host 可以看到所有进程，包括在容器内运行的进程。 host 有自己的路由表和 ARP 表（IP -> MAC）。创建容器时，我们为其创建一个 namespace，这样，它就无法查看主机上的任何网络相关信息。在其 namespace 内，容器可以有自己的虚拟接口、路由和 ARP 表。 2. Create Network NS ip netns add red ip netns 3. Exec in Network NS ip link ip netns exec red ip link ## 在ns中查看 ip link，在前面加 ip netns exec ip -n red link ## 或者使用这个命令查看LOOPBACK接口，看不到eth0接口 arp ## 在host上执行能看到条目 ip nets exec red arp ## 在新建的container内看不到条目，用route命令也是同样 4. 建立 NS 之间的网络接口连接 ## 1. 创建veth并连接 ip link add veth-red type veth peer name veth-blue ## 2. 把每个veth附加到ns ip link set veth-red netns red ip link set veth-blue netns blue ## 3. 为每个ns分配IP地址 ip -n red addr add 192.168.15.1 dev veth-red # dev 表示 device ip -n blue addr add 192.168.15.2 dev veth-blue ## 4. 启动每个ns中的相应设备 ip -n red link set veth-red up ip -n blue link set veth-blue up 接口等都建立完成后 ip netns exec red ping 192.168.15.2 才能ping通 arp 命令才会显示mac地址： arp ip netns exec red arp ip netns exec blue arp host上的ARP表不知道我们创建的新ns，也不知道我们在其中创建的接口。 ARP表：地址解析协议（ARP）是在只知道主机的IP地址时查找主机的链路层（MAC）地址的方法。ARP表用于维护每个MAC地址与其对应IP地址之间的相关性。 5. Linux Bridge 当 host 内需要连接的 ns 很多时，解决方案有：Linux bridge、Open vSwitch 等 ip link add v-net-0 type bridge ## 对host来说，它只是另一个接口 添加后 v-net-0 会出现在 ip link 的命令结果中 ip link set dev v-net-0 up 把新的连接到bridge v-net-0 # 1. 删除之前的ns之间的连接，另一端的接口会自动删除 ip -n red link del veth-red # 2. 创建veth并连接 ip link add veth-red type veth peer name veth-red-br ip link add veth-blue type veth peer name veth-blue-br # 3. 把两端veth连接到相应的ns和bridge ip link set veth-red netns red # 把接口veth-red连接到red namespace ip link set veth-red-br master v-net-0 # 把接口veth-red-br连接到bridge，主设备定为v-net-0 # 4. 分配IP地址 ip -n red addr add 192.168.15.1 dev veth-red # 分配地址 ip -n blue addr add 192.168.15.2 dev veth-blue # 分配地址 # 5. 启动设备 ip -n red link set veth-red up ip -n blue link set veth-blue up 此时在host上仍ping不通其他ns，因为它们属于不同的网络，host上的ARP表不知道我们创建的新ns，也不知道我们在其中创建的接口。 而bridge交换机实际是host上的网络接口，如果想在host和ns之间建立连接，我们需要做的就是为bridge分配一个host层面的IP地址， ip addr add 192.168.15.5/24 dev v-net-0，此时host才可以ping通ns 6. 如何配置 bridge 通过以太网接口到达 LAN 网络？ 本地host其实是连接两个网络的Gateway，host此时有两个IP地址，一个在bridge网络上 192.168.15.5，另一个在外部网络上192.168.1.2 内部ns想要访问到host的eth0 ip netns exec blue ip route add 192.168.1.0/24 via 192.168.15.5 此时 ping 192.168.1.3 不会等到无法访问的消息，ip netns exec blue ping 192.168.1.3，但仍然没有收到任何response，原因类似于家庭网络具有我们的内部私有IP地址，而目标网络不知道这些地址，因此他们无法返回获取这些地址。 需要在作为网关的主机上启用NAT（Network Address Translation），以便它可以使用自己的名称和地址将消息发送到LAN： iptables -t nat -A POSTROUTING -s 192.168.15.0/24 -j MASQUERADE 在postrouting链中的网络IP表中添加新规则，以伪装或替换来自源网络192.168.15.0/24的数据包。这样，任何在网络外部接收到这些数据包的人都将认为它们来自主机。 此时，ping已通 ip netns exec blue ping 192.168.1.3，但 ping 外部Internet网络8.8.8.8仍不通（LAN连接到外部Internet），因为 ns 目前可以到达 host 可以到达的任何网络，我们可以简单地说，要访问任何外部网络，请与我们的主机进行通信 ip netns exec blue ip route add default via 192.168.15.5 7. 外部网络访问内部专用网络的ns端口的2种解决方案： 给host添加IP路由条目，就像上面的过程一样 使用IP表添加一个端口转发规则，规定任何到达本地主机上端口80的流量都将被转发到分配给blue ns的IP上的端口80：iptables -t nat -A PREROUTING --dport 80 --to-destination 192.168.15.2:80 -j DNAT Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/networking.html":{"url":"k8s/networking.html","title":"Networking","keywords":"","body":"1. Docker Networking 当您运行容器时，您有不同的网络选项可供选择： container无法到达外部，外部也无法访问container docker run --network none nginx 容器链接到host，host和容器之间没有网络隔离无需接口转发，但两个进程无法同时侦听同一端口 docker run --network host nginx 第三种网络选项是bridge，在这种情况下，会创建一个内部专用网络，Docker主机和容器将连接到该网络。 Docker在内部使用了一种类似于我们在network ns中讲的技术，即运行类型设置为bridge的IP link add 命令，ip link add docker0 type bridge。 当Docker安装在主机上时，默认情况下，会创建一个称为 bridge 的内部专用网络，可由 docker network ls 查看。但在host上由 ip link 查看，显示为 docker0。 根据ip link 查看 docker0接口状态为 DOWN，接口或网络当前已关闭。 还记得我们说过，bridge网络就像是host的接口，但它也是host内部ns或者container的交换机。根据ip addr可以查看docker0被分配的IP地址。 每当创建container时，Docker都会为其创建一个ns，运行 ip netns命令可以列出ns（需要设置）。 docker inspect ns-name可以看到与每个容器关联的ns。 docker或者说 ns 连接到 bridge 上的方式和之前说的一样。如果在docker host上运行 ip link，我们会看到接口的一端连接到本地 bridge master docker0 用 ip -n b3165c10a92b link 连接到container上，可以看到对应的接口，用 ip -n b3165c10a92b addr 查看对应IP地址 接口通常成对匹配，container偶数，bridge接口为奇数 将docker上的8080端口映射到container的80上：docker run -p 8080:80 nginx，实现原理也一样 iptables -t nat -A DOCKER --dport 80 --to-destination 172.17.0.3:80 -j DNAT，查看的命令 iptables -nvL -t nat 2. CNI - Container Networking Interface VETH : virtual Ethernet devices Network Namespaces docker 1. Create Network Namespace 1. Create Network Namespace 2. Create Bridge Network/Interface 2. Create Bridge Network/Interface 3. Create VETH Pairs (Pipe, Virtual Cable) 3. Create VETH Pairs (Pipe, Virtual Cable) 4. Attach VETH to Namespace 4. Attach VETH to Namespace 5. Attach Other VETH to Bridge 5. Attach Other VETH to Bridge 6. Assign IP Address 6. Assign IP Address 7. Bring the interfaces up 7. Bring the interfaces up 8. Enable NAT - IP Masquerade 8. Enable NAT - IP Masquerade 统一的步骤2~8步，可以用bridge命令运行，指定将容器添加到ns： bridge add 2e34dcf34 /var/run/netns/2e34dcf34 bridge add CNI (Container Networking Interface) : container runtime 容器运行时必须创建网络命名空间 标识容器必须连接到的网络 添加容器时调用网络插件（网桥）的容器运行时 删除容器时调用网络插件（网桥）的容器运行时 网络配置的JSON格式 插件方面 必须支持命令行参数ADD/DEL/CHECK 必须支持参数container id、network ns等 必须管理POD的IP地址分配 必须以特定格式返回结果 CNI已经附带了一组支持的插件，如bridge、vlan、ipvlan、macvlan、windows，以及IPAM插件，如DHCP、host-local，还有一些其他第三方组织提供的插件。 但Docker有一套自己的标准，称为CNM（Container Network Model），不适配CNI，所以无法运行 docker run --network=cni-bridge nginx，但这并不意味着Docker无法应用CNI，你可以创建一个没有任何网络配置的Docker容器 docker run --network=none nginx，然后手动调用bridge插件，k8s就是这么做的 bridge add 2e34dcf34 /var/run/netns/2e34dcf34 https://kubernetes.io/docs/setup/independent/install-kubeadm/#check-required-ports 查看所有支持的CNI插件：/opt/cni/bin 查看当前使用的CNI插件：ls /etc/cni/net.d/ 查看kubelet的container runtime：ps -aux | grep kubelet | grep --color container-runtime 3. 常用命令 ifconfig -a：显示所有接口，包括环回接口、集群使用的实际物理接口等 cat /etc/network/interfaces：显示所有物理接口，环回接口、物理接口 ip link：显示此系统上的所有物理链路，ip link show eth0 = ifconfig eth0 ip route show default：查看默认网关，或者 ip r netstat -natulp | grep scheduler：-p programs，-l listening，-t tcp 4. Cluster Networking kubernetes集群由master node和worker node组成。每个节点必须至少有一个连接到网络的接口，每个接口必须配置一个IP地址，host必须设置唯一的主机名以及唯一的mac地址。如果通过现在VM克隆来创建VM，则应特别注意这一点。还有一些端口也需要打开，这些由控制平面中的各种组件使用。 参考文档：https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/#check-required-ports 因此，当您在防火墙中为节点设置网络时，或者在GCP、Azure或AWS等云环境中设置ip表规则或网络安全组时，请考虑这些问题。 5. Pod Networking 到目前为止，k8s还没有为此提供内置解决方案，它希望您实施一个网络解决方案来解决这些难题。 但是k8s已经明确列出了对pod networking的要求： 每个POD都应该有一个IP地址 每个POD都应该能够与同一节点中的其他POD通信 每个POD应该能够在没有NAT的情况下与其他节点上的每个其他POD通信 步骤： 在每个节点上创建一个bridge网络 ip link add v-net-0 type bridge 然后 ip link set dev v-net-0 up 设置bridge的IP地址 ip addr add 10.244.1.0/24 dev v-net-0 向默认网关添加IP地址 ip route add 10.244.1.0/24 via 192.168.15.5 但与其在每台服务器上配置路由，不如在路由器上配置路由。如果您的网络中有一个网关，并指定所有主机使用该网关作为默认网关，这样，您就可以轻松地管理路由器上路由表中所有网络的路由。 然后，我们编写了一个脚本，可以为每个容器运行该脚本，那么当我们在 k8s 上创建端口时，我们如何自动运行脚本呢？这就是 CNI 充当中间人的原因。CNI 告诉 k8s，这是您在创建容器后应该立即调用脚本的方式。 根据 CNI 标准，脚本应该有 ADD 和 DEL 部分。 执行步骤： kubelet 在运行时查看之前的配置 --cni-conf-dir=/etc/cni/net.d 并识别脚本名称 然后在目录 --cni-bin-dir=/etc/cni/bin 中寻找脚本 使用命令 ./net-script.sh add 执行脚本 6. CNI in kubernetes ps -aux | grep kubelet：查看设置为CNI的网络插件和一些与CNI相关的其他选项，如CNI bin目录和CNI config目录 ls /opt/cni/bin：bin目录包含所有支持的CNI插件作为可执行文件，如bridge、DHCP、flannel等 ls /etc/cni/net.d：conf文件是kubelet查找需要使用哪个插件的地方 7. CNI weave weave CNI插件部署在集群上，会在每个节点上部署一个代理或服务 一个pod可以连接到多个bridge 安装：kubectl apply -f \"https://cloud.weave.works/k8s/net?k8s-version=$(kubectl version | base64 | tr -d '\\n')&env.IPALLOC_RANGE=10.50.0.0/16\"，可以指定IP范围以防止和host系统IP重叠 8. Service Networking Service承载在整个cluster上，整个cluster的pod都可以访问到这个节点，Service没有绑定到特定节点，但只能从集群内部访问该服务。 NodePort可以不仅让cluster内部的节点访问，它还会再cluster中所有节点的端口上公开application。 查看IPtable转发规则：iptables -L -t nat | grep db-service 或者查看日志：cat /var/log/kube-proxy.log，文件位置可能因安装而异 查看services的IP范围：cat /etc/kubernetes/manifests/kube-apiserver.yaml | grep cluster-ip-range 查看pod的IP范围和使用的proxy类型：kubectl logs weave -n kube-system 9. DNS in kubernetes Hostname Namespace Type Root IP Address web-service apps svc cluster.local 10.107.37.188 10-244-2-5 apps pod cluster.local 10.244.2.5 curl http://10-244-2-5.apps.pod.cluster.local 10. CoreDNS in Kubernetes 建立DNS的方式： 设置每个pod上的 /etc/hosts 设置CoreDNS /etc/resolv.conf，pod名字为用短横线连接的IP 在v1.12版本之前k8s实施的DNS为kube-dns，之后为CoreDNS。 CoreDNS服务器作为POD部署在kubernetes集群的kube-system namespace中，它们被部署为两个pod以实现冗余，作为replicaSet的一部分。 查看CoreDNS的配置文件：kubectl -n kube-system describe deployments.apps coredns | grep -A2 Args | grep Corefile 11. Switching ip link：用于列出和修改host上的接口， ip link只能看链路层的状态，看不到ip地址 ip addr：查看分配给这些接口的IP地址，即使网卡处于down状态，也能显示出网卡状态，但是ifconfig查看就看不到 cat /etc/network/interfaces：查看物理接口 ip addr add 192.168.1.10/24 dev eth0 # 重启失效 ip addr add 192.168.1.11/24 dev eth0 # 如需永久生效，需要在相应的网络接口文件中修改 之后，两个系统之间就可以通过交换机进行通信。路由器连接到2个网络，可以使它们之间可以通信。 一旦链路建立并分配了IP地址，计算机就可以通过交换机相互通信。交换机只能在同一网络内进行通信，这意味着它可以从网络上的主机接收数据包，并将其传送到同一网络内的其他系统。 ip a | grep internal-ip -B 2：查看cluster内node间的网络接口，ip a 和 ip addr 一样 12. Routing 路由器可帮助连接两个网络，可以将其视为另一台具有许多网络端口的服务器，它获取IP分配，每个网络一个。 13. Gateway 如果说网络是一个房间，那么网关就是通向外部世界、其他网络或Internet的一扇门。系统需要知道门在哪里才能通过。 要查看系统上现有的路由配置，运行 route命令，它会显示内核的路由表。 ip route show default ## 查看默认网关 netstat -nplt | grep scheduler ## 查看目前节点上被scheduler监听的接口 ip route add 192.168.2.0/24 via 192.168.1.1 ## 这样B就可以访问网络2上的系统 ip route add 192.168.1.0/24 via 192.168.2.1 ## 这样C就可以访问网络1上的系统 ip route add 172.217.194.0/24 via 192.168.2.1 ## 如果C需要访问Internet上的某个地址，则也需要把新的路由添加到路由表 在互联网上的不同网络上有许多不同的网站，您不必为每个网络的相同路由器IP地址添加路由表条目，而只需简单地说，对于任何您不知道路由的网络，都可以使用此路由器作为默认网关。 ip route add default via 192.168.1.1 # default = 0.0.0.0 在 Gateway 属性列中 0.0.0.0表示不需要网关，因为它在自己的网络中。 当网络中有多个路由器时，一个用于 Internet，一个用于内部专用网络，那么您需要为每个网络创建两个单独的 entries 条目。 如何将Linux主机配置为路由器？ 数据包要到达host C，则C必须向A发回响应，所以两边都得配置。 当2个系统双向都建立好路由时，在Linux中默认情况下，数据包不会从一个接口转发到下一个接口，所以此时仍还ping不通。这是出于安全考虑，例如您将eth0连接到专用网络，而将eth1连接到公共网络，除非您明确允许，否则我们不希望公共网络中的任何人轻松地向专用网络发送消息。 转发的设置在/proc/sys/net/ipv4/ip_forward，默认情况下文件中的值为0，表示没有转发，设置为1，ping可以通过，重启后失效。 永久设置：/etc/sysctl.conf 中 net.ipv4.ip_forward = 1 14. DNS hostname 命令可以查看本机的hostname 记录在本地 /etc/hosts中，ssh 和 curl 命令都会根据此文件查找对应的ip地址，先查看本地记录，本地优先级高，如果有则优先使用： 192.168.1.11 db DNS配置在 /etc/resolv.conf 文件中，将主机指向DNS服务器： nameserver 192.168.1.100 ## 可添加多个 为了防止也在每个主机上配置多个DNS服务器地址，可以在DNS服务器上将任何未知的主机名转发到Internet上的公共DNS服务器 优先级顺序可以在 /etc/nsswitch.conf 中修改： ... hosts: files dns ... 15. Domain Names www.google.com 根域名 . 顶级域名 .com 分配给google的域名 google 子域名 www 16. Search Domain 在 /etc/resolv.conf 中添加 search 条目并指定要附加的搜索域名 search mycompany.com prod.mycompany.com 输入 web 时，就会尝试匹配 web.mycompany.com 、web.prod.mycompany.com 17. Record Types A web-server 192.168.1.1 AAAA web-server 2001:0db8:85a3:0000:0000:8a2e:0370:7334 CNAME food.web-server eat.web-server, hungry.web-server 18. nslookup、dig Ping不总是测试DNS解析的正确工具，还有 nslookup，如 nslookup www.google.com，但 nslookup 不会考虑本地hosts文件中的条目，仅查询DNS服务器。 类似的还有 dig www.google.com，返回更多详细信息 19. 将主机配置成DNS服务器 下载CoreDNS 解压，运行可执行文件coredns 默认侦听端口53 配置Corefile文件 . { hosts /etc/hosts } 其他参考信息： https://github.com/kubernetes/dns/blob/master/docs/specification.md https://coredns.io/plugins/kubernetes/ 20. Network Policy apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: db-policy spec: podSelector: matchLabels: role: db policyTypes: - Ingress ## 设置入口规则，不会影响出口 - Egress ingress: - from: - podSelector: ## 规则1 matchLabels: name: api-pod namespaceSelector: ## 限制namespace，这里必须满足pod筛选条件 matchLabels: name: prod - ipBlock: ## 2个规则其一通过即可 cidr: 192.168.5.10/32 ports: - protocol: TCP port: 3306 egress: - to: - ipBlock: cidr: ports: - protocol: TCP port: 80 查询方式：kubectl get netpol 或 kubectl get networkpolicy Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/lab.html":{"url":"k8s/lab.html","title":"KodeKloud Lightning Lab - 1","keywords":"","body":"Question 1 (15') Upgrade the current version of kubernetes from 1.23.0 to 1.24.0 exactly using the kubeadm utility. Make sure that the upgrade is carried out one node at a time starting with the controlplane node. To minimize downtime, the deployment gold-nginx should be rescheduled on an alternate node before upgrading each node. Upgrade controlplane node first and drain node node01 before upgrading it. Pods for gold-nginx should run on the controlplane node subsequently. Details Cluster Upgraded? pods 'gold-nginx' running on controlplane? Solution Here is the solution for this task. Please note that the output of these commands have not been added here. On the controlplane node: root@controlplane:~# kubectl drain controlplane --ignore-daemonsets root@controlplane:~# apt update root@controlplane:~# apt-get install kubeadm=1.24.0-00 root@controlplane:~# kubeadm upgrade plan v1.24.0 root@controlplane:~# kubeadm upgrade apply v1.24.0 root@controlplane:~# apt-get install kubelet=1.24.0-00 root@controlplane:~# systemctl daemon-reload root@controlplane:~# systemctl restart kubelet root@controlplane:~# kubectl uncordon controlplane Before draining node01, we need to remove the taint from the controlplane node. # Identify the taint first. root@controlplane:~# kubectl describe node controlplane | grep -i taint # Remove the taint with help of \"kubectl taint\" command. root@controlplane:~# kubectl taint node controlplane node-role.kubernetes.io/control-plane:NoSchedule- # Verify it, the taint has been removed successfully. root@controlplane:~# kubectl describe node controlplane | grep -i taint Now, drain the node01 as follows: - root@controlplane:~# kubectl drain node01 --ignore-daemonsets SSH to the node01 and perform the below steps as follows: root@node01:~# apt update root@node01:~# apt-get install kubeadm=1.24.0-00 root@node01:~# kubeadm upgrade node root@node01:~# apt-get install kubelet=1.24.0-00 root@node01:~# systemctl daemon-reload root@node01:~# systemctl restart kubelet To exit from the specific node, type exit or logout on the terminal. Back on the controlplane node: root@controlplane:~# kubectl uncordon node01 root@controlplane:~# kubectl get pods -o wide | grep gold (make sure this is scheduled on node) Question 2 (15') Print the names of all deployments in the admin2406 namespace in the following format: DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE . The data should be sorted by the increasing order of the deployment name. Example: DEPLOYMENT CONTAINER_IMAGE READY_REPLICAS NAMESPACE deploy0 nginx:alpine 1 admin2406 Write the result to the file /opt/admin2406_data. Details Task completed? Solution Run the below command to get the correct output: kubectl -n admin2406 get deployment -o custom-columns=DEPLOYMENT:.metadata.name,CONTAINER_IMAGE:.spec.template.spec.containers[].image,READY_REPLICAS:.status.readyReplicas,NAMESPACE:.metadata.namespace --sort-by=.metadata.name > /opt/admin2406_data Question 3 (8') A kubeconfig file called admin.kubeconfig has been created in /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it. Details Fix /root/CKA/admin.kubeconfig Solution Make sure the port for the kube-apiserver is correct. So for this change port from 4380 to 6443. Run the below command to know the cluster information: kubectl cluster-info --kubeconfig /root/CKA/admin.kubeconfig Question 4 (12') Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update. Details Image: nginx:1.16 Task: Upgrade the version of the deployment to 1:17 Solution Make use of the kubectl create command to create the deployment and explore the --record option while upgrading the deployment image. Run the below command to create a deployment nginx-deploy: kubectl create deployment nginx-deploy --image=nginx:1.16 Run the below command to update the new image for nginx-deploy deployment and to record the version: kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record Question 5 (20') A new deployment called alpha-mysql has been deployed in the alpha namespace. However, the pods are not running. Troubleshoot and fix the issue. The deployment should make use of the persistent volume alpha-pv to be mounted at /var/lib/mysql and should use the environment variable MYSQL_ALLOW_EMPTY_PASSWORD=1 to make use of an empty root password. Important: Do not alter the persistent volume. Details Troubleshoot and fix the issues Solution Use the command kubectl describe and try to fix the issue. Solution manifest file to create a pvc called mysql-alpha-pvc as follows: --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: mysql-alpha-pvc namespace: alpha spec: accessModes: - ReadWriteOnce resources: requests: storage: 1Gi storageClassName: slow Question 6 (10') Take the backup of ETCD at the location /opt/etcd-backup.db on the controlplane node. Details Troubleshoot and fix the issues Solution Take a help of command etcdctl snapshot save --help options. export ETCDCTL_API=3 etcdctl snapshot save --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key --endpoints=127.0.0.1:2379 /opt/etcd-backup.db Question 7 (20') Create a pod called secret-1401 in the admin1401 namespace using the busybox image. The container within the pod should be called secret-admin and should sleep for 4800 seconds. The container should mount a read-only secret volume called secret-volume at the path /etc/secret-volume. The secret being mounted has already been created for you and is called dotfile-secret. Details Pod created correctly? Solution Use the command kubectl run to create a pod definition file. Add secret volume and update container name in it. Alternatively, run the following command: kubectl run secret-1401 -n admin1401 --image=busybox --dry-run=client -o yaml --command -- sleep 4800 > admin.yaml Add the secret volume and mount path to create a pod called secret-1401 in the admin1401 namespace as follows: --- apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: secret-1401 name: secret-1401 namespace: admin1401 spec: volumes: - name: secret-volume # secret volume secret: secretName: dotfile-secret containers: - command: - sleep - \"4800\" image: busybox name: secret-admin # volumes' mount path volumeMounts: - name: secret-volume readOnly: true mountPath: \"/etc/secret-volume\" Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/exam2.html":{"url":"k8s/exam2.html","title":"KodeKloud Mock Exam - 2","keywords":"","body":"Question 1 (10') Take a backup of the etcd cluster and save it to /opt/etcd-backup.db. Details Backup Completed Solution Run the following command to take a backup: export ETCDCTL_API=3 etcdctl snapshot save --endpoints https://[127.0.0.1]:2379 --cacert /etc/kubernetes/pki/etcd/ca.crt --cert /etc/kubernetes/pki/etcd/server.crt --key=/etc/kubernetes/pki/etcd/server.key /opt/etcd-backup.db Question 2 (10') Create a Pod called redis-storage with image: redis:alpine with a Volume of type emptyDir that lasts for the life of the Pod. Specs on the below. Details Pod named 'redis-storage' created Pod 'redis-storage' uses Volume type of emptyDir Pod 'redis-storage' uses volumeMount with mountPath = /data/redis Solution Use the command kubectl run and create a pod definition file for redis-storage pod and add volume. Alternatively, run the command: kubectl run redis-storage --image=redis:alpine --dry-run=client -oyaml > redis-storage.yaml and add volume emptyDir in it. Solution manifest file to create a pod redis-storage as follows: --- apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: redis-storage name: redis-storage spec: containers: - image: redis:alpine name: redis-storage volumeMounts: - mountPath: /data/redis name: temp-volume volumes: - name: temp-volume emptyDir: {} Question 3 (8') Create a new pod called super-user-pod with image busybox:1.28. Allow the pod to be able to set system_time. The container should sleep for 4800 seconds. Details Pod: super-user-pod Container Image: busybox:1.28 SYS_TIME capabilities for the conatiner? Solution Solution manifest file to create a pod super-user-pod as follows: --- apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: super-user-pod name: super-user-pod spec: containers: - command: - sleep - \"4800\" image: busybox:1.28 name: super-user-pod securityContext: capabilities: add: [\"SYS_TIME\"] dnsPolicy: ClusterFirst restartPolicy: Always Question 4 (12') A pod definition file is created at /root/CKA/use-pv.yaml. Make use of this manifest file and mount the persistent volume called pv-1. Ensure the pod is running and the PV is bound. mountPath: /data persistentVolumeClaim Name: my-pvc Details persistentVolume Claim configured correctly pod using the correct mountPath pod using the persistent volume claim? Solution Add a persistentVolumeClaim definition to pod definition file. Solution manifest file to create a pvc my-pvc as follows: --- apiVersion: v1 kind: PersistentVolumeClaim metadata: name: my-pvc spec: accessModes: - ReadWriteOnce resources: requests: storage: 10Mi And then, update the pod definition file as follows: apiVersion: v1 kind: Pod metadata: creationTimestamp: null labels: run: use-pv name: use-pv spec: containers: - image: nginx name: use-pv volumeMounts: - mountPath: \"/data\" name: mypd volumes: - name: mypd persistentVolumeClaim: claimName: my-pvc Finally, create the pod by running: kubectl create -f /root/CKA/use-pv.yaml Question 5 (15') Create a new deployment called nginx-deploy, with image nginx:1.16 and 1 replica. Next upgrade the deployment to version 1.17 using rolling update. Details Deployment : nginx-deploy. Image: nginx:1.16 Image: nginx:1.16 Task: Upgrade the version of the deployment to 1:17 Task: Record the changes for the image upgrade Solution Explore the --record option while creating the deployment while working with the deployment definition file. Then make use of the kubectl apply command to create or update the deployment. To create a deployment definition file nginx-deploy: $ kubectl create deployment nginx-deploy --image=nginx:1.16 --dry-run=client -o yaml > deploy.yaml To create a resource from definition file and to record: $ kubectl apply -f deploy.yaml --record To view the history of deployment nginx-deploy: $ kubectl rollout history deployment nginx-deploy To upgrade the image to next given version: $ kubectl set image deployment/nginx-deploy nginx=nginx:1.17 --record To view the history of deployment nginx-deploy: $ kubectl rollout history deployment nginx-deploy Question 6 (15') Create a new user called john. Grant him access to the cluster. John should have permission to create, list, get, update and delete pods in the development namespace . The private key exists in the location: /root/CKA/john.key and csr at /root/CKA/john.csr. Important Note: As of kubernetes 1.19, the CertificateSigningRequest object expects a signerName. Please refer the documentation to see an example. The documentation tab is available at the top right of terminal. Details CSR: john-developer Status:Approved Role Name: developer, namespace: development, Resource: Pods Access: User 'john' has appropriate permissions Solution Solution manifest file to create a CSR as follows: --- apiVersion: certificates.k8s.io/v1 kind: CertificateSigningRequest metadata: name: john-developer spec: signerName: kubernetes.io/kube-apiserver-client request: LS0tLS1CRUdJTiBDRVJUSUZJQ0FURSBSRVFVRVNULS0tLS0KTUlJQ1ZEQ0NBVHdDQVFBd0R6RU5NQXNHQTFVRUF3d0VhbTlvYmpDQ0FTSXdEUVlKS29aSWh2Y05BUUVCQlFBRApnZ0VQQURDQ0FRb0NnZ0VCQUt2Um1tQ0h2ZjBrTHNldlF3aWVKSzcrVVdRck04ZGtkdzkyYUJTdG1uUVNhMGFPCjV3c3cwbVZyNkNjcEJFRmVreHk5NUVydkgyTHhqQTNiSHVsTVVub2ZkUU9rbjYra1NNY2o3TzdWYlBld2k2OEIKa3JoM2prRFNuZGFvV1NPWXBKOFg1WUZ5c2ZvNUpxby82YU92czFGcEc3bm5SMG1JYWpySTlNVVFEdTVncGw4bgpjakY0TG4vQ3NEb3o3QXNadEgwcVpwc0dXYVpURTBKOWNrQmswZWhiV2tMeDJUK3pEYzlmaDVIMjZsSE4zbHM4CktiSlRuSnY3WDFsNndCeTN5WUFUSXRNclpUR28wZ2c1QS9uREZ4SXdHcXNlMTdLZDRaa1k3RDJIZ3R4UytkMEMKMTNBeHNVdzQyWVZ6ZzhkYXJzVGRMZzcxQ2NaanRxdS9YSmlyQmxVQ0F3RUFBYUFBTUEwR0NTcUdTSWIzRFFFQgpDd1VBQTRJQkFRQ1VKTnNMelBKczB2czlGTTVpUzJ0akMyaVYvdXptcmwxTGNUTStsbXpSODNsS09uL0NoMTZlClNLNHplRlFtbGF0c0hCOGZBU2ZhQnRaOUJ2UnVlMUZnbHk1b2VuTk5LaW9FMnc3TUx1a0oyODBWRWFxUjN2SSsKNzRiNnduNkhYclJsYVhaM25VMTFQVTlsT3RBSGxQeDNYVWpCVk5QaGhlUlBmR3p3TTRselZuQW5mNm96bEtxSgpvT3RORStlZ2FYWDdvc3BvZmdWZWVqc25Yd0RjZ05pSFFTbDgzSkljUCtjOVBHMDJtNyt0NmpJU3VoRllTVjZtCmlqblNucHBKZWhFUGxPMkFNcmJzU0VpaFB1N294Wm9iZDFtdWF4bWtVa0NoSzZLeGV0RjVEdWhRMi80NEMvSDIKOWk1bnpMMlRST3RndGRJZjAveUF5N05COHlOY3FPR0QKLS0tLS1FTkQgQ0VSVElGSUNBVEUgUkVRVUVTVC0tLS0tCg== usages: - digital signature - key encipherment - client auth To approve this certificate, run: kubectl certificate approve john-developer Next, create a role developer and rolebinding developer-role-binding, run the command: $ kubectl create role developer --resource=pods --verb=create,list,get,update,delete --namespace=development $ kubectl create rolebinding developer-role-binding --role=developer --user=john --namespace=development To verify the permission from kubectl utility tool: $ kubectl auth can-i update pods --as=john --namespace=development Question 7 (15') Create a nginx pod called nginx-resolver using image nginx, expose it internally with a service called nginx-resolver-service. Test that you are able to look up the service and pod names from within the cluster. Use the image: busybox:1.28 for dns lookup. Record results in /root/CKA/nginx.svc and /root/CKA/nginx.pod Details Pod: nginx-resolver created Service DNS Resolution recorded correctly Pod DNS resolution recorded correctly Solution Use the command kubectl run and create a nginx pod and busybox pod. Resolve it, nginx service and its pod name from busybox pod. To create a pod nginx-resolver and expose it internally: kubectl run nginx-resolver --image=nginx kubectl expose pod nginx-resolver --name=nginx-resolver-service --port=80 --target-port=80 --type=ClusterIP To create a pod test-nslookup. Test that you are able to look up the service and pod names from within the cluster: kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service > /root/CKA/nginx.svc Get the IP of the nginx-resolver pod and replace the dots(.) with hyphon(-) which will be used below. kubectl get pod nginx-resolver -o wide kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup > /root/CKA/nginx.pod Question 8 Create a static pod on node01 called nginx-critical with image nginx and make sure that it is recreated/restarted automatically in case of a failure. Use /etc/kubernetes/manifests as the Static Pod path for example. Details static pod configured under /etc/kubernetes/manifests ? Pod nginx-critical-node01 is up and running Solution To create a static pod called nginx-critical by using below command: kubectl run nginx-critical --image=nginx --dry-run=client -o yaml > static.yaml Copy the contents of this file or use scp command to transfer this file from controlplane to node01 node. root@controlplane:~# scp static.yaml node01:/root/ To know the IP Address of the node01 node: root@controlplane:~# kubectl get nodes -o wide # Perform SSH root@controlplane:~# ssh node01 OR root@controlplane:~# ssh On node01 node: Check if static pod directory is present which is /etc/kubernetes/manifests, if it's not present then create it. root@node01:~# mkdir -p /etc/kubernetes/manifests Add that complete path to the staticPodPath field in the kubelet config.yaml file. root@node01:~# vi /var/lib/kubelet/config.yaml now, move/copy the static.yaml to path /etc/kubernetes/manifests/. root@node01:~# cp /root/static.yaml /etc/kubernetes/manifests/ Go back to the controlplane node and check the status of static pod: root@node01:~# exit logout root@controlplane:~# kubectl get pods Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/exam3.html":{"url":"k8s/exam3.html","title":"KodeKloud Mock Exam - 3","keywords":"","body":"Question 1 (12') Question Create a new service account with the name pvviewer. Grant this Service account access to list all PersistentVolumes in the cluster by creating an appropriate cluster role called pvviewer-role and ClusterRoleBinding called pvviewer-role-binding. Next, create a pod called pvviewer with the image: redis and serviceAccount: pvviewer in the default namespace. Details ServiceAccount: pvviewer ClusterRole: pvviewer-role ClusterRoleBinding: pvviewer-role-binding Pod: pvviewer Pod configured to use ServiceAccount pvviewer ? Solution Pods authenticate to the API Server using ServiceAccounts. If the serviceAccount name is not specified, the default service account for the namespace is used during a pod creation. Reference: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/ Now, create a service account pvviewer: kubectl create serviceaccount pvviewer To create a clusterrole: kubectl create clusterrole pvviewer-role --resource=persistentvolumes --verb=list To create a clusterrolebinding: kubectl create clusterrolebinding pvviewer-role-binding --clusterrole=pvviewer-role --serviceaccount=default:pvviewer Solution manifest file to create a new pod called pvviewer as follows: --- apiVersion: v1 kind: Pod metadata: labels: run: pvviewer name: pvviewer spec: containers: - image: redis name: pvviewer # Add service account name serviceAccountName: pvviewer Question 2 (12') Question List the InternalIP of all nodes of the cluster. Save the result to a file /root/CKA/node_ips. Answer should be in the format: InternalIP of controlplaneInternalIP of node01 (in a single line) Solution Explore the jsonpath loop. kubectl get nodes -o jsonpath='{.items[*].status.addresses[?(@.type==\"InternalIP\")].address}' > /root/CKA/node_ips Question 3 (12') Question Create a pod called multi-pod with two containers. Container 1, name: alpha, image: nginx Container 2: name: beta, image: busybox, command: sleep 4800 Environment Variables: container 1: name: alpha Container 2: name: beta Details Pod Name: multi-pod Container 1: alpha Container 2: beta Container beta commands set correctly? Container 1 Environment Value Set Container 2 Environment Value Set Solution Solution manifest file to create a multi-container pod multi-pod as follows: --- apiVersion: v1 kind: Pod metadata: name: multi-pod spec: containers: - image: nginx name: alpha env: - name: name value: alpha - image: busybox name: beta command: [\"sleep\", \"4800\"] env: - name: name value: beta Question 4 (8') Question Create a Pod called non-root-pod , image: redis:alpine runAsUser: 1000 fsGroup: 2000 Details Pod non-root-pod fsGroup configured Pod non-root-pod runAsUser configured Solution Solution manifest file to create a pod called non-root-pod as follows: --- apiVersion: v1 kind: Pod metadata: name: non-root-pod spec: securityContext: runAsUser: 1000 fsGroup: 2000 containers: - name: non-root-pod image: redis:alpine Verify the user and group IDs by using below command: kubectl exec -it non-root-pod -- id Question 5 (14') Question We have deployed a new pod called np-test-1 and a service called np-test-service. Incoming connections to this service are not working. Troubleshoot and fix it. Create NetworkPolicy, by the name ingress-to-nptest that allows incoming connections to the service over port 80. Important: Don't delete any current objects deployed. Details Important: Don't Alter Existing Objects! NetworkPolicy: Applied to All sources (Incoming traffic from all pods)? NetWorkPolicy: Correct Port? NetWorkPolicy: Applied to correct Pod? Solution Solution manifest file to create a network policy ingress-to-nptest as follows: --- apiVersion: networking.k8s.io/v1 kind: NetworkPolicy metadata: name: ingress-to-nptest namespace: default spec: podSelector: matchLabels: run: np-test-1 policyTypes: - Ingress ingress: - ports: - protocol: TCP port: 80 Question 6 (12') Question Taint the worker node node01 to be Unschedulable. Once done, create a pod called dev-redis, image redis:alpine, to ensure workloads are not scheduled to this worker node. Finally, create a new pod called prod-redis and image: redis:alpine with toleration to be scheduled on node01. key: env_type, value: production, operator: Equal and effect: NoSchedule Details Key = env_type Value = production Effect = NoSchedule pod 'dev-redis' (no tolerations) is not scheduled on node01? Create a pod 'prod-redis' to run on node01 Solution To add taints on the node01 worker node: kubectl taint node node01 env_type=production:NoSchedule Now, deploy dev-redis pod and to ensure that workloads are not scheduled to this node01 worker node. kubectl run dev-redis --image=redis:alpine To view the node name of recently deployed pod: kubectl get pods -o wide Solution manifest file to deploy new pod called prod-redis with toleration to be scheduled on node01 worker node. --- apiVersion: v1 kind: Pod metadata: name: prod-redis spec: containers: - name: prod-redis image: redis:alpine tolerations: - effect: NoSchedule key: env_type operator: Equal value: production To view only prod-redis pod with less details: kubectl get pods -o wide | grep prod-redis Question 7 (8') Question Create a pod called hr-pod in hr namespace belonging to the production environment and frontend tier . image: redis:alpine Use appropriate labels and create all the required objects if it does not exist in the system already. Details hr-pod labeled with environment production? hr-pod labeled with tier frontend? Solution Create a namespace if it doesn't exist: kubectl create namespace hr and then create a hr-pod with given details: kubectl run hr-pod --image=redis:alpine --namespace=hr --labels=environment=production,tier=frontend Question 8 (8') Question A kubeconfig file called super.kubeconfig has been created under /root/CKA. There is something wrong with the configuration. Troubleshoot and fix it. Details Fix /root/CKA/super.kubeconfig Solution Verify host and port for kube-apiserver are correct. Open the super.kubeconfig in vi editor. Change the 9999 port to 6443 and run the below command to verify: kubectl cluster-info --kubeconfig=/root/CKA/super.kubeconfig Question 9 (14') Question We have created a new deployment called nginx-deploy. scale the deployment to 3 replicas. Has the replica's increased? Troubleshoot the issue and fix it. Details deployment has 3 replicas Solution Use the command kubectl scale to increase the replica count to 3. kubectl scale deploy nginx-deploy --replicas=3 The controller-manager is responsible for scaling up pods of a replicaset. If you inspect the control plane components in the kube-system namespace, you will see that the controller-manager is not running. kubectl get pods -n kube-system The command running inside the controller-manager pod is incorrect. After fix all the values in the file and wait for controller-manager pod to restart. Alternatively, you can run sed command to change all values at once: sed -i 's/kube-contro1ler-manager/kube-controller-manager/g' /etc/kubernetes/manifests/kube-controller-manager.yaml This will fix the issues in controller-manager yaml file. At last, inspect the deployment by using below command: kubectl get deploy Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"k8s/experience.html":{"url":"k8s/experience.html","title":"CKA考试资料及经验分享","keywords":"","body":"1. 已通过 2022.12.04 记 考试版本：1.25.2 考试价格：原价395刀（一直在涨价，Cyber Monday 单门半价，两门三五折） 注意点： 考试选中文证书无差别，只要 Verify Name 时填对（根据汇率可能会便宜几块钱） 购买后用考试券在1年内参加，如果第一次未通过，免费的 retake 也是按购买时间算的1年内 购买后没有送模拟考试，不会像 aws 送一次模拟考试机会让你熟悉PSI考试界面 使用 PSI 内远程浏览器，可以多开tab，但不能导入书签，需要自己搜索，所以请熟悉完整命令（有些 node 上没有alias）和 yaml 格式 考前请仔细阅读考试手册（预约考试页面有链接），特别是熟悉考试界面（右上角有notepad），牢记复制粘贴的按键 为了防止把cluster搞挂，etcd备份与恢复和集群版本升级请在最后做 课程推荐：Udemy上的 Certified Kubernetes Administrator (CKA) with Practice Tests，配套有免费的 KodeKloud 练习平台，实时更新 2. 经验贴 LinuxFoundation官方手册：包含考试要求、报销等 CKA 认证笔记 - CKA 认证经验帖：2022年2月考试经验，真题模糊描述 CKA考试经验：报考和考纲：考试经验整理 CKA、CKAD考试经验Github：知识点 CKA考试指南和攻略：有界面截图 3. 常用命令 kubectl explain replicaset ## List the fields for supported resources kubectl api-resources ## list a complete list of supported resources kubectl run test-nslookup --image=busybox:1.28 --rm -it --restart=Never -- nslookup nginx-resolver-service ## 运行后删除pod kubectl top pods --selector=\"app=demo\" | grep -v NAME | sort -k 2 -nr ## 找到指定service下的pod中，cpu利用率按高到底排序 ## 删除 .spec.claimRef，可让pv从released状态重新变为available 4. 刷题 CKA Exercises CKA 真题 2022年8月 CKA 考试真题整理 2022.9.9更详细 5. 考试 Tips 书签栏搜索插件：Bookmark Sidebar 考试时有 Notepad 可供复制 Ctrl+X,Ctrl+E 可编辑以 \\ 分隔的多行命令 复制后 cat > static.yaml，有时会比跨 node scp static.yaml node01:/root/ 方便的多 6. 深入 Kubernetes 网络通信原理 《Kubernetes In Action》 《Kubernetes Patterns》 极客时间专栏《深入剖析 Kubernetes》 《kubernetes-best-practices》 7. Application Failure 查看Service的状态 使用应用程序或者 curl 检查是否可以在节点端口的IP上访问web服务器 curl http://web-service-ip:node-port 检查服务 kubectl describe svc web-service 的Endpoints 如果没有查询到以上Endpoint，比对一下 Selector 查看Pod的状态 kubectl describe pod web kubectl logs web，-f 全称 --follow，因为应用程序关闭就无法看到日志，只能等的应用程序再次失败，或者使用--previous查看上一个pod的日志 查看依赖Service 检查 db-service状态 检查 db pod 自身 参考文档：https://kubernetes.io/docs/tasks/debug/debug-application/debug-pods/ 8. Control Plane Failure 查看 Node 状态 kubectl get nodes 查看 Pod 状态 kubectl get pods 查看 Control Plane 组件状态 如果 controlplane 组件部署为 pod，使用 kubeadm 工具部署集群的情况下，查看controlplane pod 状态 kubectl -n kube-system get pods 如果 controlplane 组件被部署为 service，则检查 master node 上的 service 的状态：如 service kube-apiserver status，service kube-controller-manager status，service kube-scheduler status，service kubelet status，service kube-proxy status 查看 Service 日志 如果使用 kubeadm 工具部署集群，则使用kubectl -n kube-system logs kube-apiserver-master 如果服务是在主节点上本地配置的，清使用host的日志解决方案查看 service 日志，journalctl -u kube-apiserver 更多参考资料：https://kubernetes.io/docs/tasks/debug-application-cluster/debug-cluster 9. Worker Node Failure 查看 Node 状态 kubectl get nodes，查看节点详细 kubectl describe node worker-1 检查节点上可能得CPU、内存和磁盘空间 top，df -h 查看 kubelet 状态 service kubelet status，sudo journalctl -u kubelet 检查 kubelet 证书，确保它们未过期，属于正确的组，并且证书由正确的 CA 颁发 openssl x509 -in /var/lib/kubelet/worker-1.crt -text 10. Network Troubleshooting CoreDNS： 如果 CoreDNS pod 处于 pending 状态，请首先检查 CNI 是否安装 如果 CoreDNS pod 处于 CrashLoopBackOff 或 Error 状态： 如果节点使用旧版本的 Docker 运行 SELinux，可能会遇到 CoreDNS pod 未启动的情况，要解决此问题，可以尝试以下选项 升级到更新版本的 Docker 禁用 SELinux 修改 CoreDNS 部署，将 allowPrivilegeEscalation 设置为 true CoreDNS 发生 CrashLoopBackOff 的另一个原因是 CoreDNS pod 检测到循环 解决方式有： 将以下内容添加到您的 kubelet 配置 yaml：resolvConf: 此标志告诉 kubelet 将备用 resolv.conf 传递给 Pod 。对于使用 systemd-resolved 的系统，/run/systemd/resolve/resolv.conf 通常是“真实” resolv.conf 的位置，尽管这可能因您的发行版而异 禁用主机节点上的本地 DNS 缓存，并将 /etc/resolv.conf 恢复为原始 一个快速的解决方法是编辑您的 Corefile，替换 forward /etc/resolv.conf 与您的上游 DNS 的 IP 地址，例如 forward 8.8.8.8。但这仅解决了 CoreDNS 的问题，kubelet 将继续将无效的 resolv.conf 转发到所有默认的 dnsPolicy Pod，使它们无法解析 DNS 如果 CoreDNS pod 和 kube-dns 服务工作正常，请检查 kube-dns 服务是否有有效的端点。如果服务没有端点，请检查服务并确保它使用正确的选择器和端口 kube-proxy： kube-proxy 负责监视与每个服务关联的服务和端点。当客户端要使用虚拟 IP 连接到服务时，kube-proxy 负责将流量发送到实际的 Pod 检查 kube-system ns 中的 kube-proxy pod 正在运行 检查 kube-proxy 日志 检查 configmap 是否正确定义，运行 kube-proxy 二进制文件的配置文件是否正确 kube-config 在 config map 中定义 检查 kube-proxy 是否在容器内运行 更多参考资料： https://kubernetes.io/docs/tasks/debug-application-cluster/debug-service/ https://kubernetes.io/docs/tasks/administer-cluster/dns-debugging-resolution/ Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/introduction.html":{"url":"cloud/introduction.html","title":"Cloud","keywords":"","body":"1. AWS 考试证书类型 2. SAA-C02参加最新考试的最后日期是 2022 年 8 月 29 日 AWS 认证解决方案架构测试考试概览 AWS Certification 帐户的“Benefits”中的半价优惠券，可以用于再认证或用于您之后参加的任何认证考试 参加同等或更高级别的考试可以满足再认证的要求。如果您未通过考试，必须等待14天才能重考 重考次数没有限制，但每次重考都必须全额支付报名费（2022年9月30日之前，PSI考试不过免费补考） 一旦通过了考试，则在两年内不能重考同一门课程 费用 Foundational 100美元，Associate 150美元，Professional 和 Specialty 300美元 130分钟65道题，其中包括15道不计分（AWS收集评估以作为将来计分使用），及格分720分（可能会根据难易程度换算分数） 单选四选一，多选在五个或更多答案选项中具有两个或更多正确答案（会告诉你正确的个数） 3. 备考资料 AWS官方文档 AWS官方白皮书 在线题库 ExamTopics AWS认证模拟题 Free Briefing exams 博客 Jayendra's Cloud Certification Blog LiuYuchen Bing哥的博客 Udemy 视频课程 Ultimate AWS Certified Solutions Architect Associate SAA-C0N，全英文带习题 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/iam.html":{"url":"cloud/iam.html","title":"IAM","keywords":"","body":"1. IAM: Users & Groups 身份和访问管理 全球服务 根帐户默认创建，不应使用或共享 用户是组织内的人员，可以分组 组只包含用户，不包含其他组 用户不必属于一个组，用户可以属于多个组 2. IAM: Permissions 用户或组可以分配称为策略的JSON文档 这些策略定义用户的权限 在AWS中，您应用最小权限原则：不要授予用户所需的权限 3. IAM Policies Structure 版本：策略语言版本，始终包含“2012-10-17” Id：策略的标识符（可选） 语句：一个或多个单独的语句（必需） Sid：语句的标识符（可选） 效果：语句是允许还是拒绝访问（Allow，Deny） 主体：应用此策略的帐户/用户/角色 操作：此策略允许或拒绝的操作列表 资源：应用操作的资源列表 条件：此策略生效的条件（可选） 使用标签控制对 IAM 用户和角色的访问以及他们进行的访问 4. How can users access AWS ? AWS管理控制台（受密码+MFA保护） AWS命令行界面（CLI）：受访问密钥保护（基于AWS SDK for Python） AWS软件开发工具包（SDK）用于代码：受访问密钥保护 5. IAM Security Tools IAM凭据报告（帐户级别）： 列出帐户的所有用户及其各种凭据状态的报告 IAM Access Advisor（用户级别）： Access advisor显示授予用户的服务权限以及上次访问这些服务的时间 您可以使用这些信息来修改您的策略 6. IAM Guidelines & Best Practices 除了AWS帐户设置之外，不要使用根帐户 一个物理用户=一个AWS用户 将用户分配给组并将权限分配给组 创建强密码策略 使用和强制使用多因素身份验证（MFA） 创建并使用角色授予AWS服务权限 使用访问密钥进行编程访问（CLI/SDK） 使用IAM凭据报告审核您帐户的权限 永远不要共享IAM用户和访问密钥 7. Shared Responsibility Model for IAM AWS you 基础设施（全球网络安全）配置和漏洞分析合规性验证 用户、组、角色、策略管理和监控在所有帐户上启用MFA经常轮换所有密钥使用IAM工具应用适当的权限分析访问模式并审查权限 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/ec2.html":{"url":"cloud/ec2.html","title":"EC2 & Security Groups","keywords":"","body":"1. Amazon EC2 (Elastic Compute Cloud) 它主要包括以下能力： 租用虚拟机（EC2） 在虚拟驱动器（EBS）上存储数据 跨机器分配负载（ELB） 使用自动缩放组（ASG）缩放服务 2. EC2 Instance Types - Overview m5.2xlarge m：实例类 5：生成（AWS会随着时间的推移对其进行改进） 2xlarge：实例类中的大小 3. Introduction to Security Groups 安全组仅包含allow规则 安全组规则可以按 IP 或按安全组引用 4. Security Groups Good to know 可以附加到多个实例 锁定到某个地区/VPC组合 是否位于EC2的“外部”？如果流量被阻塞，EC2实例将看不到它 最好为SSH访问维护一个单独的安全组 如果您的应用程序不可访问（超时），则这是安全组问题 如果您的应用程序出现“拒绝连接”错误，那么这是应用程序错误或未启动 默认情况下，所有入站流量都被阻止 默认情况下，所有出站流量都已授权 5. 要了解的经典端口 22=SSH（Secure Shell）-登录到Linux实例 21=FTP（文件传输协议）-将文件上传到文件共享 22=SFTP（安全文件传输协议）-使用SSH上传文件 80=HTTP–访问不安全的网站 443=HTTPS–访问安全网站 3389=RDP（远程桌面协议）-登录到Windows实例 6. SSH Summary Table SSH Putty EC2 Instance Connect Mac √ √ Linux √ √ Windows √ √ Windows > 10 √ √ √ 7. EC2 Instances Purchasing Options On-Demand Instances：工作负载短，价格可预测 Reserved：（至少1年） Reserved Instances：工作负载长 Convertible Reserved Instances：具有灵活实例的长工作负载 Scheduled Reserved Instances：例如，每周四下午3点至6点 Spot Instances：工作负载短、成本低、可能丢失实例（可靠性较低） 适用于具有故障恢复能力的工作负载：批处理作业、数据分析、图像处理、任何分布式工作负载、具有灵活开始和结束时间的工作负载 如果主动终止一个竞价实例，需要为当前这个完整小时付费 如果因为价格上涨，AWS 终止了你的竞价实例，那么这个小时的费用会被免除 Dedicated Hosts：预订整个物理服务器，控制实例放置 专用主机允许您使用现有的绑定到服务器的软件许可证，可以帮助您满足法规遵从性要求并降低成本 Dedicated Instances：没有其他客户会共享您的硬件 可以与同一帐户中的其他实例共享硬件 无法控制实例放置（可以在停止/启动后移动硬件） 8. Differences between options *note: %折扣与视频不同，因为AWS会随着时间的推移而改变折扣-考试不需要确切的数字。这只是为了说明 Spot实例：Spot块在指定的时间范围内（1到6小时）不中断（从2021年7月1日起不再可用，支持到2022年12月31日） Discount Reservation Period Upfront Payment On Demand / / No Reserved Up to 72% 1 year/ 3 years No / Partial / All Convertible Reserved Up to 66% Scheduled Reserved / 1 year only Spot Up to 90% Dedicated Hosts More expensive 3 years Dedicated Instances / / 9. 如何终止 Spot Instances? 只能取消打开、活动或禁用的点实例请求。 取消Spot Request不会终止实例 您必须首先取消现场请求，然后终止关联的现场实例 10. Spot Fleets Spot Fleets=一组Spot实例+（可选）随需应变实例 现货车队将努力在价格限制的情况下达到目标产能 定义可能的启动池：实例类型（m5.large）、操作系统、可用性区域 可以有多个发射池，以便舰队可以选择 Spot Fleet在达到容量或最大成本时停止启动实例 分配点实例的策略： 最低价：来自价格最低的池（成本优化，工作量短） 多样化：分布在所有池中（非常适合可用性和长工作负载） capacityOptimized：具有针对实例数量的最佳容量的池 11. EC2 Nitro 下一代EC2实例的基础平台，高速EBS（Nitro对于64000 EBS IOPS是必要的，非Nitro上的最大值为32000） 12. EC2 – Capacity Reservations 容量预订确保您在需要时拥有EC2容量 预订的手动或计划结束日期无需1年或3年的承诺 容量访问是即时的，它一开始就向您收费 指定： 要在其中保留容量的可用性区域（仅一个） 要为其保留容量的实例数 实例属性，包括实例类型、租赁和平台/OS 与保留实例和节约计划相结合以节省成本 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/s3.html":{"url":"cloud/s3.html","title":"S3","keywords":"","body":"1. Amazon S3 Overview - Buckets AmazonS3允许人们将对象（文件）存储在“bucket”（目录）中 Buckets必须具有全局唯一名称 Buckets是在区域级别定义的 命名约定 无大写字母 没有下划线 3-63个字符长 不是IP 必须以小写字母或数字开头 2. Amazon S3 Overview – Objects 对象值是主体的内容： 最大对象大小为5TB（5000GB） 如果上传超过5GB，必须使用“多部分上传” 元数据（文本键/值对列表——系统或用户元数据） 标签（Unicode键/值对-最多10个）-对安全性/生命周期非常有用 版本ID（如果启用了版本控制） 3. S3 Encryption for Objects SSE-S3：使用AWS处理和管理的密钥加密S3对象 对象是加密的服务器端 AES-256加密类型 必须设置标头：“x-amz-server-side-encryption”：“AES256” SSE-KMS：利用AWS密钥管理服务管理加密密钥 SSE-KMS：使用KMS处理和管理的密钥进行加密 KMS优势：用户控制+审计跟踪 对象是加密的服务器端 必须设置标头：“x-amz-server-side-encryption”：“aws:kms” SSE-C：当您想要管理自己的加密密钥时 服务器端加密，使用完全由AWS以外的客户管理的数据密钥 Amazon S3不存储您提供的加密密钥 必须使用HTTPS 必须在HTTP标头中为每个HTTP请求提供加密密钥 客户端加密 客户端库，如Amazon S3加密客户端 客户端在发送到S3之前必须自己加密数据 从S3检索时，客户端必须自己解密数据 客户全面管理密钥和加密周期 4. S3 Security User based IAM策略——应允许从IAM控制台为特定用户调用API Resource Based Bucket Policies-来自S3控制台的Bucket范围规则-允许跨帐户 对象访问控制列表（ACL）–细粒度 Bucket访问控制列表（ACL）–不太常见 Note: IAM主体可以访问S3对象，如果 用户IAM权限允许或资源策略允许 和没有明确的DENY 5. CORS (Cross-Origin Resource Sharing) using CORS Headers (ex: Access-Control-Allow-Origin) 6. AWS EC2 Instance Metadata AWS EC2实例元数据功能强大，但却是开发人员最不了解的功能之一 它允许AWS EC2实例“了解自己”，而无需为此目的使用IAM角色。 URL是http://169.254.169.254/latest/meta-data 您可以从元数据中检索IAM角色名称，但无法检索IAM策略 7. Amazon FSx 在AWS上推出第三方高性能文件系统 全面管理的服务 8. Amazon FSx for Windows (File Server) EFS是一个用于Linux系统的共享POSIX系统。 FSx for Windows是一个完全管理的Windows文件系统共享驱动器 支持SMB协议和Windows NTFS Microsoft Active Directory集成、ACL、用户配额 内置SSD，可扩展至10 GB/s，数百万IOPS，100s PB数据 可以从您的内部部署基础架构访问 可以配置为Multi-AZ（高可用性） 数据每天备份到S3 9. Amazon FSx for Lustre Lustre是一种并行分布式文件系统，用于大规模计算 Lustre这个名字来源于“Linux”和“集群” 机器学习，高性能计算（HPC） 视频处理、财务建模、电子设计自动化 可扩展到100s GB/s、数百万IOPS、亚毫秒延迟 与S3无缝集成 可以作为文件系统“读取S3”（通过FSx） 可以将计算结果写回S3（通过FSx） 可从内部部署服务器使用 10. AWS Storage Gateway S3中本地数据和云数据之间的桥梁 使用案例：灾难恢复、备份和恢复、分层存储 11. File Gateway 可以使用NFS和SMB协议访问配置的S3存储桶 支持S3标准、S3 IA、S3 One Zone IA 使用IAM角色对每个文件网关进行存储桶访问 最近使用的数据缓存在文件网关中 可以安装在许多服务器上 与Active Directory（AD）集成，用于用户身份验证 12. Volume Gateway 使用S3支持的iSCSI协议的块存储 由EBS快照支持，这可以帮助恢复本地卷！ 缓存卷：对最新数据的低延迟访问 存储的卷：整个数据集都是内部部署的，计划备份到S3 13. Tape Gateway 有些公司有使用物理磁带的备份过程（！） 使用磁带网关，公司使用相同的流程，但在云中 由Amazon S3和Glacier支持的虚拟磁带库（VTL） 使用现有的基于磁带的进程（和iSCSI接口）备份数据 与领先的备份软件供应商合作 14. Storage Comparison S3：对象存储 冰川：实物档案 EFS：Linux实例的网络文件系统，POSIX文件系统 FSx for Windows：适用于Windows服务器的网络文件系统 FSx for Lustre：高性能计算Linux文件系统 EBS卷：一次一个EC2实例的网络存储 实例存储：EC2实例的物理存储（高IOPS） 存储网关：文件网关、卷网关（缓存和存储）、磁带网关 Snowball/Snowmobile：将大量数据物理地移动到云中 数据库：用于特定的工作负载，通常带有索引和查询 15. S3 MFA-Delete MFA（多因素身份验证）强制用户在S3上执行重要操作之前，在设备（通常是手机或硬件）上生成代码 要使用MFA Delete，请在S3存储桶上启用Versioning 您需要MFA 永久删除对象版本 挂起bucket的版本控制 您不需要MFA 启用版本控制 列出已删除的版本 只有bucket所有者（root帐户）才能启用/禁用MFA Delete MFA Delete当前只能使用CLI启用 *注意：Bucket策略在“默认加密”之前进行评估 16. S3 Replication (CRR & SRR) 必须在源和目标中启用版本控制 跨区域复制（CRR） 同一区域复制（SRR） Buckets可以在不同的帐户中 复制是异步的 必须向S3授予适当的IAM权限 CRR—使用案例：法规遵从性、较低延迟的访问、跨帐户复制 SRR–用例：日志聚合、生产和测试帐户之间的实时复制 17. S3 Replication – Notes 激活后，仅复制新对象（不可追溯） 对于DELETE操作： 可以将删除标记从源复制到目标（可选设置） 不复制具有版本ID的删除（以避免恶意删除） 不存在复制的“链接” 如果存储桶1具有到存储桶2中的复制，则存储桶2具有到存储盒3中的复制 然后，在bucket 1中创建的对象不会复制到bucket 3 18. S3 Pre-Signed URLs 可以使用SDK或CLI生成预签名URL 对于下载（很容易，可以使用CLI） 对于上传（更难，必须使用SDK） 有效期默认为3600秒，可以在[TIME_BY_seconds]参数中使用--expires更改超时 给定预签名URL的用户将继承生成GET/PUT URL的人的权限 19. Amazon S3存储类别 S3 Storage Classes Comparison 20. S3 – Moving between storage classes 21. S3 Lifecycle Rules 转换操作：它定义对象何时转换到另一个存储类。 创建后60天将对象移动到标准IA类 6个月后移到Glacier存档 过期操作：将对象配置为在一段时间后过期（删除） 访问日志文件可以设置为在365天后删除 可用于删除旧版本的文件（如果启用了版本控制） 可用于删除不完整的多部分上传 可以为某个前缀创建规则（例如-s3://mybucket/mp3/*） 可以为某些对象标记创建规则（例如部门：财务） 22. S3 Analytics – Storage Class Analysis 您可以设置S3 Analytics以帮助确定何时将对象从Standard转换为Standard_IA 不适用于ONEZONE_IA或GLACIER 报告每天更新 首次启动大约需要24小时到48小时 制定生命周期规则（或改进它们）的第一步很好！ 23. S3 – Baseline Performance Amazon S3自动扩展到高请求率，延迟100-200毫秒 您的应用程序在一个bucket中的每个前缀每秒至少可以实现3500 PUT/COPY/POST/DELETE和5500 GET/HEAD请求 24. S3 Performance Multi-Part upload: 建议用于大于100MB的文件，必须用于大于5GB的文件 可以帮助并行上传（加快传输速度） S3 Transfer Acceleration 通过将文件传输到AWS边缘位置来提高传输速度，该位置将数据转发到目标区域中的S3存储桶 兼容多部分上传 25. Athena 一次性SQL查询、S3上的无服务器查询、日志分析 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/ebs.html":{"url":"cloud/ebs.html","title":"EBS","keywords":"","body":"1. What's an EBS Volume? 它们一次只能挂载到一个实例（CCP级别） 它们绑定到特定的可用性区域 2. EBS – 终止时删除属性 控制EC2实例终止时的EBS行为 默认情况下，根EBS卷被删除（属性已启用） 默认情况下，不会删除任何其他连接的EBS卷（禁用属性） 这可以通过AWS控制台/AWS CLI进行控制 用例：实例终止时保留根卷 3. EBS Snapshots 在某个时间点备份EBS卷（快照） 不需要分离卷来进行快照，但建议 可以跨AZ或Region复制快照 4. AMI (Amazon Machine Image) AMI是EC2实例的自定义 您添加自己的软件、配置、操作系统、监控… 更快的启动/配置时间，因为您的所有软件都是预打包的 AMI是为特定区域构建的（并且可以跨区域复制） 您可以从以下位置启动EC2实例： 公共AMI：提供AWS 你自己的AMI：你自己制作和维护它们 AWS Marketplace AMI：其他人制造（并可能销售）的AMI 5. AMI Process (from an EC2 instance) 启动EC2实例并进行自定义 停止实例（为了数据完整性） 构建AMI–这也将创建EBS快照 从其他AMI启动实例 6. EBS Volume Types EBS卷有6种类型 gp2/gp3（SSD）：通用SSD卷，可平衡各种工作负载的性价比 io1/io2（SSD）：用于任务关键型低延迟或高吞吐量工作负载的最高性能SSD卷 st1（HDD）：低成本HDD卷，专为频繁访问、吞吐量密集型工作负载而设计 sc1（HDD）：为访问频率较低的工作负载设计的成本最低的HDD卷 EBS卷的特征是大小|吞吐量| IOPS（每秒I/O操作数） 如果有疑问，请务必查阅AWS文档——这很好！ 只有gp2/gp3和io1/io2可以用作启动卷 7. EBS –Volume Types Summary Amazon EBS卷类型 8. EBS Multi-Attach – io1/io2 family 将同一EBS卷附加到同一AZ中的多个EC2实例 每个实例都具有对卷的完全读写权限 用例： 在集群Linux应用程序中实现更高的应用程序可用性（例如：Teradata） 应用程序必须管理并发写入操作 必须使用支持集群的文件系统（不是XFS、EX4等） 9. EBS Encryption 创建加密EBS卷时，您会得到以下信息： 静止数据在卷内加密 在实例和卷之间移动的所有数据都是加密的 所有快照都已加密 从快照创建的所有卷 加密和解密是透明处理的（您无需做任何事情） 加密对延迟的影响最小 EBS加密利用KMS（AES-256）的密钥 复制未加密的快照允许加密 加密卷的快照已加密 10. 加密未加密的EBS卷 创建卷的EBS快照 加密EBS快照（使用副本） 从快照创建新ebs卷（该卷也将被加密） 现在您可以将加密卷附加到原始实例 11. EFS – Elastic File System 可安装在许多EC2上的托管NFS（网络文件系统） EFS与多AZ中的EC2实例一起工作 高可用性、可扩展性、价格昂贵（3倍gp2）、按次付费 使用案例：内容管理、网络服务、数据共享、Wordpress 使用NFSv4.1协议 使用安全组控制对EFS的访问 与基于Linux的AMI（非Windows）兼容 使用KMS进行静态加密 POSIX（可移植操作系统接口可移植操作系统接口）具有标准文件API的文件系统（~Linux） 文件系统自动扩展，按次付费，无需容量规划！ 12. EFS – Performance & Storage Classes EFS Scale 1000个并发NFS客户端，10 GB+/s吞吐量 自动扩展到Petabyte规模的网络文件系统 Performance mode (set at EFS creation time) 通用（默认）：延迟敏感用例（web服务器、CMS等） 最大I/O–更高的延迟、吞吐量、高度并行（大数据、媒体处理） Throughput mode 突发（1 TB=50MiB/s+高达100MiB/s的突发） Provisioned：无论存储大小如何，都可以设置吞吐量，例如：1 TB存储为1 GiB/s Storage Tiers (lifecycle management feature – move file after N days) 标准：用于频繁访问的文件 不频繁访问（EFS-IA）：检索文件的成本更低，存储成本更低 13. EBS vs EFS – Elastic Block Storage EBS volumes… 一次只能附加到一个实例 锁定在可用区（AZ）级别 gp2:IO在磁盘大小增加时增加 io1：可以独立增加IO 跨AZ迁移EBS卷 拍摄快照 将快照恢复到另一个AZ EBS备份使用IO，您不应该在应用程序处理大量流量时运行它们 如果EC2实例被终止，实例的根EBS卷默认情况下会被终止（您可以禁用它） 14. EBS vs EFS – Elastic File System 在AZ上安装100个实例 EFS共享网站文件（WordPress） 仅适用于Linux实例（POSIX） EFS的价位高于EBS 可以利用EFS-IA节省成本 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/rds.html":{"url":"cloud/rds.html","title":"RDS & ElastiCache","keywords":"","body":"1. RDS Backups RDS中自动启用备份 自动化备份： 数据库的每日完整备份（在维护窗口期间） RDS每5分钟备份一次事务日志 =>能够恢复到任何时间点（从最旧的备份到5分钟前） 7天保留期（可增加到35天） DB Snapshots: 用户手动触发 根据需要保留备份 2. RDS – Storage Auto Scaling 帮助您动态增加RDS数据库实例的存储空间 当RDS检测到您的可用数据库存储空间不足时，它会自动扩展 避免手动扩展数据库存储 您必须设置最大存储阈值（数据库存储的最大限制） 在以下情况下自动修改存储： 可用存储空间小于已分配存储空间的10% 低存储时间至少5分钟 距离上次修改已经过去了6个小时 适用于具有不可预测工作负载的应用程序 支持所有RDS数据库引擎（MariaDB、MySQL、PostgreSQL、SQL Server、Oracle） 3. 用于读取可扩展性的RDS读取副本 最多5个读取复制副本 AZ内、跨AZ或跨区域 复制是ASYNC，因此读取最终是一致的 副本可以升级到自己的数据库 应用程序必须更新连接字符串才能利用读取副本 4. RDS Read Replicas – Network Cost 在AWS中，当数据从一个AZ传输到另一个时，会产生网络成本 对于同一区域内的RDS读取副本，您不需要支付该费用 5. RDS Multi AZ (Disaster Recovery) SYNC replication, Increase availability 6. RDS – From Single-AZ to Multi-AZ 零停机操作（无需停止数据库） 只需点击数据库的“修改” 以下情况在内部发生： 拍摄快照 在新的AZ中从快照恢复新的DB 在两个数据库之间建立同步 7. RDS Security - Encryption 静态加密 可以使用AWS KMS-AES-256加密对主副本和读取副本进行加密 必须在启动时定义加密 如果主机未加密，则读取的复制副本无法加密 可用于Oracle和SQL Server的透明数据加密（TDE） In-flight encryption SSL证书将数据加密到飞行中的RDS 在连接到数据库时提供带有信任证书的SSL选项 要强制执行SSL： PostgreSQL：在AWS rds控制台中rds.force_ssl=1（参数组） MySQL：在数据库中： GRANT USAGE ON *.* TO 'mysqluser'@'%' **REQUIRE SSL**; 8. RDS Security – IAM Access Management IAM策略有助于控制谁可以管理AWS RDS（通过RDS API） 传统用户名和密码可用于登录数据库 基于IAM的身份验证可用于登录RDS MySQL和PostgreSQL 9. RDS - IAM Authentication IAM数据库身份验证适用于MySQL和PostgreSQL 您不需要密码，只需通过IAM和RDS API调用获得身份验证令牌 身份验证令牌的生存期为15分钟 优点： 网络输入/输出必须使用SSL加密 IAM将集中管理用户而不是DB 可以利用IAM角色和EC2实例配置文件进行轻松集成 10. Amazon Aurora Aurora是AWS的专有技术（非开源） Postgres和MySQL都支持Aurora数据库（这意味着你的驱动程序将像Aurora是Postgres或MySQL数据库一样工作） Aurora是“AWS云优化”，声称在RDS上的性能比MySQL提高了5倍，是Postgres在RDS上性能的3倍以上 Aurora存储以10GB的增量自动增长，最高可达128 TB。 Aurora可以有15个副本，而MySQL有5个，并且复制过程更快（副本延迟不到10毫秒） Aurora中的故障切换是即时的。它是HA（高可用性）本机。 Aurora的成本高于RDS（高出20%），但效率更高 11. Aurora High Availability and Read Scaling 6 copies of your data across 3 AZ: 写入需要6份副本中的4份 6份中有3份需要被读取 使用对等复制实现自我修复 存储跨100个卷进行条带化 一个Aurora实例需要写入（主） 在不到30秒内实现主设备的自动故障切换 Master+最多15个Aurora Read副本提供读取 支持跨区域复制 12. Aurora – Custom Endpoints 将Aurora实例的子集定义为自定义端点 示例：对特定复制副本运行分析查询 在定义自定义端点之后，通常不会使用读取器端点 13. Global Aurora Aurora Cross Region Read Replicas: 对灾难恢复有用 易于安装 Aurora Global Database (recommended): 1主区域（读/写） 最多5个辅助（只读）区域，复制滞后时间小于1秒 每个辅助区域最多16个读取副本 有助于降低延迟 升级另一个区域（用于灾难恢复）的RTO小于1分钟 14. ElastiCache – Redis vs Memcached Redis： 带自动故障切换的多AZ 读取副本*以扩展读取并具有高可用性 使用AOF持久性的数据持久性 备份和恢复功能** Memcached： 用于数据分区（分片）的多节点 没有高可用性（复制） *非持久性 无备份和恢复** 多线程体系结构 15. ElastiCache – Cache Security ElastiCache中的所有缓存： 不支持IAM身份验证 ElastiCache上的IAM策略仅用于AWS API级安全 Redis AUTH 创建Redis集群时可以设置“密码/令牌” 这是缓存的额外安全级别（位于安全组之上） 支持SSL飞行中加密 Memcached 支持基于SASL的身份验证（高级） 16. Patterns for ElastiCache 懒惰加载：所有读取的数据都被缓存，数据可能会在缓存中变得过时 直写：在写入数据库时添加或更新缓存中的数据（无陈旧数据） 会话存储：将临时会话数据存储在缓存中（使用TTL功能） 引用：在计算机科学中只有两件困难的事情：缓存失效缓存失效 并命名事物 17. ElastiCache – Redis Use Case 游戏排行榜的复杂计算 Redis排序集保证了唯一性和元素顺序 每次添加新元素时，都会对其进行实时排名，然后添加到正确的顺序 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/elasticip.html":{"url":"cloud/elasticip.html","title":"弹性IP","keywords":"","body":"IPv4允许在公共空间中提供 37 亿个不同的地址 1. Elastic IPs - 为您的实例提供固定的公共IP 使用弹性IP地址，您可以通过快速将地址重新映射到帐户中的另一个实例来掩盖实例或软件的故障。 你的账户中只能有 5 个弹性IP（你可以要求 AWS 增加）。 总体而言，尽量避免使用弹性IP 它们往往反映出糟糕的体系结构决策 相反，使用随机的公共IP并向其注册DNS名称 或者，正如我们稍后将看到的，使用负载均衡器而不使用公共IP 2. Placement Groups 群集-将实例群集到单个可用性区域中的低延迟组中 优点：出色的网络（启用增强网络的实例之间的10 Gbps带宽-推荐） 缺点：如果机架出现故障，则所有实例都会同时出现故障 用例： 需要快速完成的大数据工作 需要极低延迟和高网络吞吐量的应用程序 排列-在底层硬件之间排列实例（每个AZ每组最多7个实例） 优点： 可以跨越可用性区域（AZ） 同时发生故障可降低风险 EC2实例位于不同的物理硬件上 缺点：每个放置组每个AZ最多7个实例 用例： 需要最大限度地提高高可用性的应用程序 关键应用程序，其中每个实例必须相互隔离，避免发生故障 分区——将实例分布在一个AZ中的许多不同分区（这些分区依赖于不同的机架集）上。每个组可扩展到100个EC2实例（Hadoop、Cassandra、Kafka） 每个AZ最多7个分区 可以跨越同一地区的多个AZ 多达100个EC2实例 分区中的实例不与其他分区中的示例共享机架 分区故障可能会影响许多EC2，但不会影响其他分区 EC2实例可以访问作为元数据的分区信息 用例：HDFS、HBase、Cassandra、Kafka 3. Elastic Network Interfaces (ENI) 专有网络中代表虚拟网卡的逻辑组件 ENI可以具有以下属性： 主专用IPv4，一个或多个辅助IPv4 每个专用IPv4一个弹性IP（IPv4） 一个公共IPv4 一个或多个安全组 MAC地址 您可以独立创建ENI，并在EC2实例上动态附加（移动）它们以进行故障切换 绑定到特定可用性区域（AZ） 4. EC2 Hibernate 在后台：RAM状态被写入根EBS卷中的一个文件，该文件必须加密 使用案例： 长时间运行的处理 保存RAM状态 初始化需要时间的服务 5. EC2 Hibernate – Good to know 支持的实例族-C3、C4、C5、M3、M4、M5、R3、R4和R5 实例RAM大小*-必须小于150 GB 实例大小-裸金属实例不支持 AMI：亚马逊Linux 2、Linux AMI、Ubuntu和Windows… 根卷：必须是EBS，加密的，不是实例存储，并且很大 可用于随需应变和保留实例 实例的休眠时间不能超过60天 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/lb.html":{"url":"cloud/lb.html","title":"LB & ASG","keywords":"","body":"1. Why use a load balancer? 跨多个下游实例分散负载 向应用程序公开单点访问（DNS） 无缝处理下游实例的故障 定期对实例进行运行状况检查 为您的网站提供SSL终止（HTTPS） 使用Cookie强制粘性 跨区域的高可用性 将公共交通与私人交通分开 2. Types of load balancer on AWS AWS有 4 种托管负载平衡器 Classic Load Balancer (v1 - old generation) – 2009 – CLB HTTP, HTTPS, TCP, SSL (secure TCP) Supports TCP (Layer 4), HTTP & HTTPS (Layer 7) 运行状况检查基于 TCP 或 HTTP 固定主机名：xxx.region.elb.amazonaws.com Application Load Balancer (v2 - new generation) – 2016 – ALB HTTP、HTTPS、WebSocket 应用程序负载均衡器是第7层（HTTP） 跨计算机（目标组）实现多个HTTP应用程序的负载平衡 同一台机器上多个应用程序的负载平衡（例如：容器） 支持HTTP/2和WebSocket 支持重定向（例如从HTTP到HTTPS） 将表路由到不同的目标组： 基于URL中路径的路由（example.com/用户&example.com/帖子） 基于URL中主机名的路由（one.example.com&other.examplecom） 基于查询字符串、标头的路由（例如.com/users？id=123&order=false） ALB非常适合微服务和基于容器的应用程序（例如：Docker和Amazon ECS） 具有端口映射功能，可重定向到ECS中的动态端口 相比之下，每个应用程序需要多个经典负载均衡器 Target Groups: EC2实例（可由自动缩放组管理）–HTTP ECS任务（由ECS自己管理）-HTTP Lambda函数–HTTP请求被转换为JSON事件 IP地址–必须是专用IP ALB可以路由到多个目标群体 健康检查在目标群体级别 固定主机名（xxx.aregion.elb.amazonaws.com） 应用程序服务器无法直接看到客户端的IP 客户端的真实IP插入标头X-Forwarded-For请求标头可自动添加并帮助您识别客户端的 IP地址 We can also get Port (X-Forwarded-Port 请求标头可帮助您识别客户端与您的负载均衡器连接时所用的目标端口) and proto (X-Forwarded-Proto 请求标头可帮助您识别客户端与您的负载均衡器连接时所用的协议 (HTTP 或 HTTPS)) Network Load Balancer (v2 - new generation) – 2017 – NLB TCP、TLS（安全TCP）、UDP 网络负载平衡器（第4层）允许： 将TCP和UDP流量转发到您的实例 每秒处理数百万个请求 延迟更短～100毫秒（ALB为400毫秒） NLB每个AZ有一个静态IP，并支持分配弹性IP（有助于将特定IP列入白名单） NLB用于极端性能、TCP或UDP通信 目标群体： EC2实例 IP地址–必须是专用IP 应用程序负载平衡器 网关负载均衡器 – 2020 – GWLB 在第3层（网络层）运行——IP协议 在AWS中部署、扩展和管理第三方网络虚拟设备 示例：防火墙、入侵检测和预防系统、深度数据包检测系统、有效负载操作等… 在第3层（网络层）操作–IP数据包 组合了以下功能： 透明网络网关–所有流量的单一入口/出口 负载均衡器–将流量分配到虚拟设备 在端口6081上使用GENEVE协议 目标群体：* EC2 instances IP Addresses – must be private IPs 3. Sticky Sessions (Session Affinity) 可以实现粘性，以便始终将同一客户端重定向到负载平衡器后面的同一实例 这适用于经典负载平衡器和应用程序负载平衡器 用于粘性的“cookie”的有效期由您控制 用例：确保用户不会丢失会话数据 启用粘性可能会导致后端EC2实例的负载失衡 4. Sticky Sessions – Cookie Names Application-based Cookies Custom cookie 由目标生成 可以包括应用程序所需的任何自定义属性 必须为每个目标组单独指定Cookie名称 不要使用AWSALB、AWSALBAPP或AWSALBTG（保留供ELB使用） Application cookie 由负载平衡器生成 Cookie名称为AWSALBAPP 基于持续时间的Cookie 负载平衡器生成的Cookie Cookie名称为AWSALB表示ALB，AWSELB表示CLB 5. Cross-Zone Load Balancing Application Load Balancer 始终打开（无法禁用） AZ间数据不收费 Network Load Balancer 默认情况下禁用 如果启用，您将为AZ间数据支付费用（$） Classic Load Balancer 默认情况下禁用 如果启用，则不收取AZ间数据的费用 6. SSL – Server Name Indication (SNI) SSL指的是安全套接字层，用于加密连接 TLS指的是较新版本的传输层安全性 SNI解决了将多个SSL证书加载到一个web服务器（为多个网站服务）的问题 这是一个“较新”的协议，要求客户端在初始SSL握手中指示目标服务器的主机名 然后，服务器将找到正确的证书，或返回默认证书 *注： 仅适用于ALB和NLB（新一代），CloudFront 不适用于CLB（老一代） 7. Elastic Load Balancers – SSL Certificates Classic Load Balancer (v1) 仅支持一个SSL证书 必须为具有多个SSL证书的多个主机名使用多个CLB Application Load Balancer (v2) & Network Load Balancer (v2) 支持具有多个SSL证书的多个侦听器 使用服务器名称指示（SNI）使其工作 8. ASG Brain Dump 缩放策略可以基于CPU、网络……甚至可以基于自定义指标或基于时间表（如果您知道访客模式） ASG使用启动配置或启动模板（更新） 要更新ASG，您必须提供新的启动配置/启动模板 附加到ASG的IAM角色将分配给EC2实例 ASG是免费的。您为正在启动的基础资源付费 在ASG下拥有实例意味着，如果它们因任何原因被终止，ASG将自动创建新实例作为替换。额外的安全！ ASG可以终止LB标记为不健康的实例（并因此替换它们） 9. Auto Scaling Groups - Scaling Cooldowns 缩放活动发生后，您处于冷却期（默认为300秒） 在冷却期间，ASG不会启动或终止其他实例（以使指标稳定） 建议：使用现成的AMI来减少配置时间，以便为请求禁食者提供服务并缩短冷却时间 10. ASG for Solutions Architects ASG默认终止规则（简化版）： 找到实例数量最多的AZ 如果AZ中有多个实例可供选择，请删除启动配置最旧的实例 ASG尝试在默认情况下平衡AZ中的实例数量 11. ASG for Solutions Architects - Lifecycle Hooks 默认情况下，一旦在ASG中启动实例，它就处于服务状态。 您可以在实例投入服务之前执行额外的步骤（挂起状态） 您可以在实例终止之前执行一些操作（正在终止状态） Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/route.html":{"url":"cloud/route.html","title":"Route","keywords":"","body":"1. DNS Terminologies Domain Registrar: Amazon Route 53, GoDaddy, … DNS Records: A, AAAA, CNAME, NS, … Zone File: contains DNS records Name Server: resolves DNS queries (Authoritative or Non-Authoritative) Top Level Domain (TLD): .com, .us, .in, .gov, .org, … Second Level Domain (SLD): amazon.com, google.com, … 2. Amazon Route 53 一个高度可用、可扩展、完全管理且具有权威性的DNS 权威=客户（您）可以更新DNS记录 Route 53也是域名注册商 能够检查资源的健康状况 唯一提供100%可用性SLA（服务级别协议服务级协定) 为什么选择Route 53？53是对传统DNS端口的引用 每个托管区域每月0.50美元 3. Route 53 – Records Domain/subdomain Name – e.g., example.com Record Type – e.g., A or AAAA Value – e.g., 12.34.56.78 Routing Policy – how Route 53 responds to queries TTL – amount of time the record cached at DNS Resolvers 4. Route 53 – Record Types A – maps a hostname to IPv4 AAAA – maps a hostname to IPv6 CNAME – maps a hostname to another hostname 目标是必须具有a或AAAA记录的域名 无法为DNS命名空间（Zone Apex）的顶部节点创建CNAME记录 示例：您不能为Example.com创建，但可以为www.Example.com创建 NS – 托管区域的命名服务器 控制域的流量路由方式 5. Route 53 – Records TTL (Time To Live) 除了Alias记录外，TTL对于每个DNS记录都是强制性的 6. CNAME vs Alias AWS Resources (Load Balancer, CloudFront...) expose an AWS hostname: lb1-1234.us-east-2.elb.amazonaws.com and you want myapp.mydomain.com CNAME: 将主机名指向任何其他主机名（app.mydomain.com=>blabla.anything.com） 仅适用于非根域（又名something.mydomain.com） Alias: 将主机名指向AWS资源（app.mydomain.com=>blabla.amazonaws.com） 适用于根域和非根域（又名mydomain.com） 免费 本地健康检查 对于AWS资源（IPv4/IPv6），别名记录的类型始终为A/AAAA 您无法设置TTL 不能为EC2 DNS名称设置ALIAS记录 7. Routing Policies - Simple 如果返回多个值，则客户端将随机选择一个值 启用Alias后，仅指定一个AWS资源 无法与健康检查关联 8. Routing Policies – Weighted 控制发送到每个特定资源的请求的百分比 为每条记录分配一个相对权重，权重总和不需要达到100 DNS记录必须具有相同的名称和类型 可以与健康检查关联 用例：区域之间的负载平衡，测试新的应用程序版本… 为记录分配权重0以停止向资源发送流量 如果所有记录的权重都为0，则所有记录都将平等返回 9. Route 53 – Health Checks HTTP运行状况检查仅适用于公共资源 运行状况检查=>自动DNS故障转移： 监控端点（应用程序、服务器、其他AWS资源）的运行状况检查 监测其他健康检查的健康检查（计算健康检查） 监控CloudWatch警报的运行状况检查（完全控制！！）——例如，DynamoDB的节流阀、RDS上的警报、自定义指标……（有助于私人资源） 健康检查与CW指标集成 10. Health Checks – Monitor an Endpoint 大约15名全球健康检查人员将检查端点健康状况 健康/不健康阈值–3（默认值） 间隔–30秒（可设置为10秒–成本更高） 支持的协议：HTTP、HTTPS和TCP 如果>18%的健康检查人员报告终点是健康的，则路线53认为它是健康的。否则就是不健康 能够选择您希望Route 53使用的位置 只有当端点以2xx和3xx状态代码响应时，运行状况检查才通过 健康检查可以根据响应的前5120字节中的文本设置为通过/失败 配置路由器/防火墙以允许来自Route 53 Health Checkers的传入请求 11. Routing Policies – Multi-Value 将流量路由到多个资源时使用 Route 53返回多个值/资源 可以与运行状况检查关联（仅返回运行状况资源的值） 每个多值查询最多返回8条健康记录 多值不能代替ELB 12. Instantiating Applications quickly EC2 Instances: 使用Golden AMI：提前安装应用程序、操作系统依赖项等，并从Golden AMI启动EC2实例 使用用户数据引导：对于动态配置，请使用用户数据脚本 混合：混合Golden AMI和用户数据（Elastic Beanstalk） RDS Databases: 从快照恢复：数据库将准备好架构和数据！ EBS Volumes: 从快照还原：磁盘将已格式化并具有数据！ 13. Elastic Beanstalk – Overview Elastic Beanstalk是在AWS上部署应用程序的以开发人员为中心的视图 它使用了我们以前见过的所有组件：EC2、ASG、ELB、RDS… 托管服务 自动处理容量调配、负载平衡、扩展、应用程序运行状况监视、实例配置… 只是应用程序代码是开发人员的责任 我们仍然可以完全控制配置 Beanstalk是免费的，但您需要为底层实例付费 14. Elastic Beanstalk – Components 应用程序：Elastic Beanstalk组件的集合（环境、版本、配置…） 应用程序版本：应用程序代码的迭代 环境 运行应用程序版本的AWS资源集合（一次只能运行一个应用程序版本） 层：Web服务器环境层和工作环境层 您可以创建多个环境（开发、测试、生产…） Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/cloudfront_etc.html":{"url":"cloud/cloudfront_etc.html","title":"CloudFront, SQS, SNS & Kinesis","keywords":"","body":"1. CloudFront vs S3 Cross Region Replication CloudFront: 全球边缘网络 文件缓存一个TTL（可能一天） 非常适合必须随处可用的静态内容 S3 Cross Region Replication: 必须为要进行复制的每个区域设置 文件几乎实时更新 只读 非常适合在少数地区以低延迟提供的动态内容 2. CloudFront Signed URL Diagram 3. CloudFront Signed URL vs S3 Pre-Signed URL CloudFront Signed URL: 允许访问路径，无论其来源如何 帐户范围的密钥对，只有root用户才能管理 可以按IP、路径、日期、过期日期进行筛选 可以利用缓存功能 S3 Pre-Signed URL: 以预签名URL的身份发出请求 使用签名IAM主体的IAM密钥 寿命有限 4. Unicast IP vs Anycast IP 单播IP：一台服务器拥有一个IP地址 Anycast IP：所有服务器都拥有相同的IP地址，客户端路由到最近的一个 5. AWS Global Accelerator vs CloudFront 他们都使用AWS全球网络及其在世界各地的边缘位置 这两项服务都与AWS Shield集成以提供DDoS保护。 CloudFront 提高了可缓存内容（如图像和视频）的性能 动态内容（如API加速和动态网站交付） 边缘提供内容 Global Accelerator 通过TCP或UDP提高各种应用程序的性能 将边缘的数据包代理到一个或多个AWS区域中运行的应用程序。 非常适合非HTTP用例，如游戏（UDP）、物联网（MQTT）或IP语音 适用于需要静态IP地址的HTTP用例 适用于需要确定性、快速区域故障切换的HTTP用例 使用SQS:队列模型 使用SNS:发布/子模型 使用Kinesis：实时流媒体模型 6. Amazon SQS – Standard Queue 最老的产品（超过10年） 完全管理的服务，用于解耦应用程序 属性： 吞吐量不受限制，队列中的消息数量不受限制 邮件的默认保留期：4天，最长14天 低延迟（发布和接收时＜10毫秒） 发送的每条消息限制为256KB 可能有重复的邮件（至少一次传递，偶尔） 可能会出现无序消息（尽力订购） 7. Kinesis Kinesis数据流：捕获、处理和存储数据流 Kinesis Data Firehose：将数据流加载到AWS数据存储中以可靠方式将实时数据串流加载到数据湖、数据库和分析服务中 Kinesis数据分析：使用SQL或Apache Flink分析数据流 Kinesis视频流：捕获、处理和存储视频流 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/db_basic.html":{"url":"cloud/db_basic.html","title":"云数据库","keywords":"","body":"1. OLTP（Online Transaction Processing）和 OLAP（Online Analytical Processing） OLTP 和 OLAP 是两种常见的数据处理技术，用于处理不同类型的数据和满足不同的业务需求。 OLTP 是一种用于处理事务性数据的技术。它主要用于日常的业务操作，例如订单处理、交易记录、库存管理等。OLTP系统通常针对大量的短期事务，需要高并发性和快速的数据插入、更新和查询能力。这些系统通常强调数据的一致性和准确性，并追求低延迟的响应时间。OLTP 数据库通常采用关系型数据库管理系统（RDBMS），如 Oracle、MySQL、Microsoft SQL Server 等。 OLAP 则是一种用于处理分析性数据的技术。它主要用于数据分析、决策支持和业务智能等领域。OLAP系统通常面对复杂的查询和多维数据分析，需要支持大规模数据的聚合、切片、切块和透视等操作。这些系统通常强调数据的可读性和多维度的分析能力，并追求高性能的数据查询和计算。OLAP 数据库通常采用特定的数据存储和处理技术，如多维数据库（例如 OLAP 立方体）或列式数据库。 2. BASE（Basically Available, Soft state, Eventually consistent） BASE 是非关系型数据库系统设计的一种原则，与ACID（原子性、一致性、隔离性、持久性）相对应。它强调在大规模分布式系统中的可用性和性能，而放松了强一致性的要求。 下面是BASE原则的解释： Basically Available（基本可用）：系统保持基本的可用性，即使在面临部分故障或分区的情况下也能继续提供服务。它意味着系统可以接受部分故障，但仍然能够返回响应或执行部分功能。 Soft state（软状态）：系统在没有输入时可以处于不确定的状态，这是允许的。与ACID中的强一致性要求不同，软状态允许系统的状态在时间上存在一定的延迟和不一致性，而这些不一致性可以通过后续的操作或时间来纠正。 Eventually consistent（最终一致性）：系统的状态最终将达到一致状态，尽管在分布式环境中不同节点之间的状态可能存在一段时间的不一致。最终一致性并不要求实时或即时的一致性，而是通过后续的同步和协调机制使系统最终达到一致状态。 BASE原则在分布式系统和大规模数据处理中更为适用，特别是在面对高可用性、可扩展性和性能需求的场景下。相对于强一致性的ACID，BASE提供了更灵活的数据一致性模型，可以在牺牲一部分一致性的前提下获得更高的可用性和性能。 需要注意的是，BASE原则并非一种具体的技术实现或标准，而是一种设计思想和原则的概念。具体的非关系型数据库系统可以根据BASE原则进行设计和实现，以满足不同的可用性、一致性和性能需求。常见的非关系型数据库系统包括MongoDB、Cassandra、Redis等。 3. 关系型数据库和非关系型数据库的对比 关系型数据库 非关系型数据库 数据存储在表中，通过外键建立关系 数据用集合或键值对存储 严格的ACID原则 BASE原则 结构化数据 半结构化或非结构化数据 垂直扩展（换成更大的instance） 水平扩展（添加新node） 使用SQL 使用基于对象的APIs 适配OLTP和OLAP 适配OLTP（网页、移动应用） 4. 概述 支持的引擎：PostgreSQL，MySQL，MariaDB，Oracle，Microsoft SQL Server，Aurora 在一个VPC内启动：常使用私有子网，用 security groups 控制权限（如使用 Lambda 时） 使用 EBS 储存：gp2 或 io1，可自动扩展容量 备份：自动根据时间点恢复，备份过期 监控：使用 CloudWatch RDS Events：SNS 事件提醒 1 credit = CPU 核心利用率100%一分钟 5. 参数组（Parameter groups） 动态参数（修改立即生效，可能会导致停机）和静态参数（需要手动重启，状态从 in-sync -> pending-reboot -> in-sync） 6. 选项组（Option groups） 数据库引擎功能选项（默认为空），修改则需要创建新的选项组 7. Security - Network 数据库创建（通常是在子网中 ）后 VPC 就无法修改了 8. Security - IAM 身份验证 您不需要密码，只需通过 IAM 和 RDS API 调用获得身份验证令牌 身份验证令牌的生存期为15分钟 优点： 进出流量必须使用SSL 取代数据库，IAM 集中化管理用户 9. 如何使用 IAM DB 身份认证？ 在数据库群集上启用 IAM 数据库身份验证 创建数据库用户（不带密码） 附加 IAM 策略以将 DB 用户映射到 IAM 角色 将 IAM 角色附加到 IAM 用户（或 EC2 实例） 现在您可以通过 SSL 使用 IAM 令牌连接到 DB MySQL 实现方式： -- 创建用户 CREATE USER {db_username} IDENTIFIED WITH AWSAuthenticationPlugin as 'RDS'; -- 授予SSL权限 GRANT USAGE ON *.* TO '{db_username}'@'REQUIRE SSL; -- 下载SSL密钥 wget \"https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\" -- 使用CLI生成token TOKEN=\"$(aws rds generate-db-auth-token --hostname {db_or_cluster_endpoint} --port 3306 --username {db_username})\" -- 连接数据库 mysql --host={db_or_cluster_endpoint} --port=3306 --ssl-ca=/home/ec2-user/rds-combined-ca-bundle.pem --enable-cleartext-plugin --user={db_username} --password=$TOKEN PostgreSQL 实现方式： 不需要修改配置文件pg_hba.conf CREATE USER {db_username}; GRANT rds_iam to {db_username}; wget \"https://s3.amazonaws.com/rds-downloads/rds-ca-2019-root.pem\" export PGPASSWORD=\"$(aws rds generate-db-auth-token --hostname={db_endpoint} --port=5432 --username={db_username} --region us-west-2)\" psql -h {db_endpoint} -p 5432 \"dbname={db_name} user={db_username} password=$PGPASSWORD sslrootcert=/home/ec2-user/rds-combined-ca-bundle.pem sslmode=verify-ca\" 10. 轮换 RDS DB 证书 使用 AWS Security Manager 集中、安全地存储凭证，支持审计 支持 secrets 的自动轮换 Secrets Manager 提供了一个 Lambda 轮换函数，并自动用 ARN（Amazon Resource Name）填充 secret 与 RDS 中的 MySQL、PostgreSQL 和 Aurora 集成 11. RDS SQL Server 的 Windows 身份认证 创建 AWS 托管的 Microsoft AD 目录，并在您的公司 AD 和 AWS 托管的AD之间建立信任关系（称为 forest trust 域信任） 如果您有超过5000个用户，并且需要与您的前置目录建立信任关系，则 AWS 托管 AD 是最佳选择 或者，您也可以使用 AD 连接器（用于现有的本地目录）或简单 AD（如果您的用户少于5000） 在 AD 中设置用户和组 在 RDS 实例中启用 SQL Server Windows 身份验证以将目录映射到 DB 实例（自动创建适当的lAM角色） 使用主用户凭据登录到数据库，并为 AD 用户创建 SQL Server Windows 登录 12. 强制使用 SSL SQL Server 或 PostgreSQL：在参数组中设置参数rds.force ssl=1（静态参数，需要手动重新启动） MySQL 或 MariaDB：ALTER USER 'mysqluser'@'%' REQUIRE SSL Oracle：将 SSL 选项添加到 DB 实例选项组 13. SSL连接选项 # PostgreSQL sslrootcert=rds-cert.pem sslmode=[verify-ca | verify-full] # MySQL --ssl-ca=rds-cert.pem --ssl-mode=VERIFY_IDENTITY (MySQL 5.7+) # MariaDB --ss1-ca=rds-cert.pem --ssl-mode=REQUIRED (MaraDB 10.2+) # MySOL/MariaDB (older versions) --ss1-ca=rds-cert.pem --ssl-verify-server-cert 14. RDS 数据加密 RDS 支持 AES-256 加密算法 通过 KMS 管理的密钥 可以同时加密主副本和读取副本必须在启动时定义加密 RDS 还支持 TDE（透明数据加密） For SQL Server（仅限 Enterprise Edition）和 Oracle DB 实例 通过设置选项组启用 对 Oracle 启用 TDE 选项 对 SQL Server 启用 TRANSPARENT_DATA_ENCRYPTION 选项 如果同时使用 TDE 和 静态加密，可能会略微影响数据库性能 15. 复制和分享加密的 snapshots 使用默认RDSencryption密钥加密的快照不能直接共享 使用自定义加密密钥复制快照，然后共享[密钥+快照] 无法共享具有某些自定义选项组的快照（例如TDE） Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/storages.html":{"url":"cloud/storages.html","title":"云存储选择","keywords":"","body":"1. RDS 备份 RDS 支持自动备份 实时捕获事务日志 默认情况下启用，保留期为7天（0-35天保留期，0=禁用自动备份） 您可以提供备份窗口时间和备份保留天数 第一个备份是完整备份，后续备份是增量备份 数据存储在 S3 存储桶中（由 RDS 服务拥有和管理，您不会在 S3 控制台中看到它们） 建议使用 Multi-AZ 选项来避免备份运行时的性能问题 与 AWS Backup 服务集成以实现集中管理 支持 PITR，而快照则不支持 复制备份会变为快照，可以跨越账号、region Point-In-Time Recovery 时间间隔为 5 分钟 2. 从快照还原 只能恢复到新实例 一个实例可以有一个或多个数据库，所有这些数据库都将被恢复 要保留相同的名称，请先删除或重命名现有实例 无法直接从共享和加密的快照中恢复（先复制，然后从副本中恢复） 无法直接从另一个区域恢复（先复制，然后从副本恢复） 可以从 VPC 外的数据库实例快照恢复到 VPC 内（但相反则不行） 默认情况下，还原的集群使用： 新建安全组 默认参数组 与快照关联的选项组 从快照恢复时，请确保 选择正确的安全组以确保恢复的数据库的连接 为还原的数据库选择正确的参数组 建议保留快照的参数组，以帮助使用正确的参数组进行恢复 3. 导出快照到 S3 可以导出所有类型的备份（自动/手动或使用 AWS 备份服务创建的备份） 如何出口？ 设置具有适当 IAM 权限的 S3 存储桶，并为 SSE 创建KMS密钥 使用控制台（Actions -> Export to Amazon S3）或使用 start Export task CLI 命令导出快照 导出在后台运行 不会影响数据库性能 以 Apache Parquet 格式导出的数据（压缩、一致） 允许您使用 Athena 或 Redshift Spectrum 分析数据库数据 4. 比较 RDS DR 策略 RTORecovery Time Objective RPORecovery Point Object Cost Scope Automated backups Good Better Low Single Region Manual snapshots Better Good Medium Cross-Region Read replicas Best Best High Cross-Region 5. 如何解决复制错误的建议 调整副本的大小以匹配源数据库（存储大小和数据库实例类） 对源数据库和副本使用兼容的数据库参数组设置 例如，读取副本允许的最大数据包必须与源数据库实例的数据包相同 监视副本实例的 Replication State 字段 如果 Replication State = Error，然后查看 Replication Error 字段中的错误详细信息 使用 RDS 事件通知获取有关此类副本问题的警报 写入读取复制副本上的表 将只读设置为0以使读取副本可写 仅用于维护任务（如仅在复制副本上创建索引） 如果您在读取副本上写入表，可能会使其与源数据库不兼容并破坏复制 因此，在完成维护任务后立即设置read-only=1 只有像lnnoDB这样的事务存储引擎才支持复制，使用MylSAM这样的引擎会导致复制错误 使用不安全的非确定性查询（如SYSDATE）（可能会破坏复制） 您可以跳过复制错误（如果不是主要错误），也可以删除并重新创建复制副本 对于MySQL： 错误或数据不一致b/w源实例和replica 可能是由于 binlog 事件或 lnnoDB 重做日志在 replica 或源实例失败期间未刷新而发生的 必须手动删除并重新创建复制 预防性建议： sync_binlog=1 innodb_flush_log_at_trx_commit=1 innodb_support_xa=1 这些设置可能会降低性能（因此在转到生产前进行测试） 6. Aurora Replicas vs MySQL Replicas Feature Amazon Aurora Replicas MySQL Replicas Number of replicas Up to 15 Up to 5 Replication type Asynchronous (milliseconds) Asynchronous (seconds) Performance impact on primary Low High Replica location In-region Cross-region Act as failover target Yes (no data loss) Yes (potentially minutes of data loss) Automated failover Yes No Support for user-defined replication delay No Yes Support for different data or schema vs. primary No Yes 7. Comparison of RDS Deployments Read replicas Multi-AZ deployments (Single-Region) Multi-Region deployments Main purpose Scalability HA DR and performance Replication method Asynchronous (eventual consistency) Synchronous Asynchronous (eventual consistency) Asynchronous (Aurora) Accessibility All replicas can be used for read scaling Active-Passive (standby not accessible) All regions can be used for reads Automated backups No backups configured by default Taken from standby Can be taken in each region Taken from shared storage layer (Aurora) Instance placement Within-AZ, Cross-AZ, or Cross-Region At least two AZs within region Each region can have a Multi-AZ deployment Upgrades Independent from source instance On primary Independent in each region All instances together (Aurora) DR (Disaster Recovery) Can be manually promoted to a standalone instance Automatic failover to standby Aurora allows promotion of a secondary region to be the master Can be promoted to primary (Aurora) Automatic failover to read replica (Aurora) 8. DynamoDB Terminology Compared to SQL SQL / RDBMS DynamoDB Tables Tables Rows Items Columns Attributes Primary Keys - Multicolumn Primary Keys - Mandatory, minimum one and maximum two attributes Indexes Local Secondary Indexes Views Global Secondary Indexes 9. AWS 数据库之间的比较 Database Type of data Workload Size Performance Operational overhead RDS DBs Structured Relational / OLTP / simple OLAP Low TB range Mid-to-high throughtput, low latency Moderate Aurora Mysql Structured Relational / OLTP / simple OLAP Mid TB range High throughtput, low latency Low-to-moderate Aurora PostgrSQL Structured Relational / OLTP / simple OLAP Mid TB range High throughtput, low latency Low-to-moderate Redshift Structured / Semi-structured Relational / OLAP / DW PB range Mid-to-high latency Moderate DynamoDB Semi-structured Non-relational / Key-Value / OLTP / Document store High TB range Ultra-high throughtput, low latency, ultra-low latency with DAX Low ElastiCache Semi-structured / Unstructured Non-relational / In-memory caching / Key-Value Low TB range High throughtput, ultra-low latency Low Neptune Graph data Highly connected graph datasets Mid TB range High throughtput, ultra-low latency Low Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/networking.html":{"url":"cloud/networking.html","title":"Networking","keywords":"","body":"1. VPC 和子网 VPC：部署资源的专用网络（region 层面） 子网：允许您在VPC内分区网络（AZ 层面） 公共子网：是可从互联网访问的子网 专用子网：是不能从互联网访问的子网 为了定义对互联网和子网之间的访问，我们使用路由表 2. Internet Gateway & NAT Gateways 互联网网关帮助我们的 VPC 实例与互联网连接 公共子网有通往互联网网关的路由 NAT 网关（AWS管理）和 NAT 实例（自我管理）允许您的私有子网中的实例在保持私有状态的同时访问互联网 3. Network ACL & Security Groups 网络访问控制列表 控制子网和子网之间流量的防火墙 可以具有 ALLOW 和 DENY 规则 附加在子网级别 规则仅包括 IP 地址 安全组 控制往返 ENl / EC2 实例的流量的防火墙 只能有 ALLOW 规则 规则包括 IP 地址和其他安全组 4. 网络访问控制列表和安全组对比 Security Group Network ACL 运行在实例层面 运行在子网层面 只支持allow规则 支持allow和deny规则 无状态：返回的流量自动允许 有状态：返回的流量必须经规则允许 比对所有规则 按顺序比对规则 仅在启动实例时指定了安全组，或在之后与实例关联时，才适用于实例 自动应用于与之关联的子网中的所有实例（因此，您不必依赖用户来指定安全组） 5. VPC Flow Logs 捕获有关进入接口的 IP 流量的信息 VPC 流日志 子网流日志 弹性网络接口流日志 帮助监控和定位连接问题。比如： 子网到互联网 子网到子网 因特网到子网 还从 AWS 托管接口捕获网络信息：弹性负载均衡器，ElastiCache，RDS，Aurora等... VPC Flow日志数据可以转到 S3、CloudWatch Logs 和 Kinesis Data Firehose 6. VPC Peering 连接两个VPC，单独使用AWS的网络 使它们的行为就像在同一个网终中一样 不能有重叠的CIDR（IP地址范围） VPC 对等连接不具有可传递性（必须为需要彼此通信的每个 VPC 建立对等连接） 7. VPC Endpoints 端点允许您使用专用网络而不是公共万维网网络连接到AWS服务 这增强了安全性并降低了访问AWS服务的延迟 VPC端点网关：S3和DynamoDB VPC端点接口：其余部分 只在你的VPC中使用 8. 堡垒机 我们可以使用 Bastion 主机访问我们的私人 RDS 数据库，ElastiCache 集群，等等... 堡垒在公共子网中，然后连接到所有其他私有子网 必须加强堡垒主机安全组 确保数据库安全组允许 bastion 安全组 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "},"cloud/dbs.html":{"url":"cloud/dbs.html","title":"DBS考试经验分享","keywords":"","body":"已通过 2023.07.24 记 推荐 Udemy 课程 Ultimate AWS Certified Database Specialty 2023 刷题题库不需要在淘宝上买，ExamTopics 上都有，后面一半的题会显示需要购买，只要在 google 上搜关键字“Exam AWS Certified Database - Specialty topic 1 question xxx”就可以一题一题地看 我选的英文的监考官，试题只有英文的，没有语言切换 之前的 AWS 考试证书在有效期内，就可以使用 50% 打折券 公司支持每种证和到期 renew 的考试费用报销 Copyright ©Bota5ky all right reserved，powered by GitbookLast Updated: 2023-11-13 09:41:56 "}}